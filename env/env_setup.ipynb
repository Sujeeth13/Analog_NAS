{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from VQVAE_environment import VQVAE_Env\n",
    "from stable_baselines3.common.env_checker import check_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy surrogate model, decoder, and codebook to test the environment\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class MockSurrogateModel:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def evaluate(self, decoded_state):\n",
    "        # Return a dummy accuracy value\n",
    "        return np.random.random()\n",
    "\n",
    "class MockDecoder:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def decode(self, state):\n",
    "        # Return a dummy decoded state\n",
    "        return state\n",
    "\n",
    "# Create a dummy codebook as a numpy array\n",
    "# Assuming the embed_dim is 10 and you have 100 embeddings plus 1 for the stop action\n",
    "mock_codebook = np.random.rand(101, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize your environment with the mock components\n",
    "env = VQVAE_Env(embed_dim=10, num_embeddings=101, max_allowed_actions=200,\n",
    "                surrogate_model=MockSurrogateModel(), decoder=MockDecoder(), codebook=mock_codebook,\n",
    "                num_previous_actions=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using check_env from stable baselines 3 to check if the environment is compatible with stable baselines\n",
    "check_env(env, warn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Observation: ({'latent_vector': array([ 1.3413056 , -0.6393575 , -0.79040223, -1.2632823 ,  1.256437  ,\n",
      "        0.5965506 , -0.72908646, -1.217553  , -0.60058206,  1.2916957 ],\n",
      "      dtype=float32), 'action_history': array([-1, -1, -1, -1], dtype=int32)}, {})\n",
      "Taking action: 655\n",
      "New Observation: {'latent_vector': array([ 1.3413056 , -0.6393575 , -0.79040223, -1.2632823 ,  1.256437  ,\n",
      "        0.43826824, -0.72908646, -1.217553  , -0.60058206,  1.2916957 ],\n",
      "      dtype=float32), 'action_history': array([ -1,  -1,  -1, 655], dtype=int32)}\n",
      "Reward: 0.9952473670909366\n",
      "Done: False\n",
      "Truncate: False\n",
      "Info: {}\n",
      "---\n",
      "Taking action: 126\n",
      "New Observation: {'latent_vector': array([ 1.3413056 , -0.6393575 , -0.79040223, -1.2632823 ,  1.256437  ,\n",
      "        0.43826824,  0.6787315 , -1.217553  , -0.60058206,  1.2916957 ],\n",
      "      dtype=float32), 'action_history': array([ -1,  -1, 655, 126], dtype=int32)}\n",
      "Reward: -0.28470189742054386\n",
      "Done: False\n",
      "Truncate: False\n",
      "Info: {}\n",
      "---\n",
      "Taking action: 586\n",
      "New Observation: {'latent_vector': array([ 1.3413056 , -0.6393575 , -0.79040223, -1.2632823 ,  1.256437  ,\n",
      "        0.43826824,  0.79158306, -1.217553  , -0.60058206,  1.2916957 ],\n",
      "      dtype=float32), 'action_history': array([ -1, 655, 126, 586], dtype=int32)}\n",
      "Reward: -0.6308133268580692\n",
      "Done: False\n",
      "Truncate: False\n",
      "Info: {}\n",
      "---\n",
      "Taking action: 285\n",
      "New Observation: {'latent_vector': array([ 1.3413056 , -0.6393575 , -0.79040223, -1.2632823 ,  1.256437  ,\n",
      "        0.41381475,  0.79158306, -1.217553  , -0.60058206,  1.2916957 ],\n",
      "      dtype=float32), 'action_history': array([655, 126, 586, 285], dtype=int32)}\n",
      "Reward: 0.48386007938424647\n",
      "Done: False\n",
      "Truncate: False\n",
      "Info: {}\n",
      "---\n",
      "Taking action: 369\n",
      "New Observation: {'latent_vector': array([ 1.3413056 , -0.6393575 , -0.79040223, -1.2632823 ,  1.256437  ,\n",
      "        0.41381475,  0.79158306, -1.217553  , -0.60058206,  0.760751  ],\n",
      "      dtype=float32), 'action_history': array([126, 586, 285, 369], dtype=int32)}\n",
      "Reward: -0.4095873669953487\n",
      "Done: False\n",
      "Truncate: False\n",
      "Info: {}\n",
      "---\n",
      "Taking action: 387\n",
      "New Observation: {'latent_vector': array([ 1.3413056 , -0.6393575 , -0.79040223, -1.2632823 ,  1.256437  ,\n",
      "        0.41381475,  0.79158306,  0.8484989 , -0.60058206,  0.760751  ],\n",
      "      dtype=float32), 'action_history': array([586, 285, 369, 387], dtype=int32)}\n",
      "Reward: 0.6972934767093396\n",
      "Done: False\n",
      "Truncate: False\n",
      "Info: {}\n",
      "---\n",
      "Taking action: 983\n",
      "New Observation: {'latent_vector': array([ 1.3413056 , -0.6393575 , -0.79040223,  0.86883   ,  1.256437  ,\n",
      "        0.41381475,  0.79158306,  0.8484989 , -0.60058206,  0.760751  ],\n",
      "      dtype=float32), 'action_history': array([285, 369, 387, 983], dtype=int32)}\n",
      "Reward: -0.15988147922214024\n",
      "Done: False\n",
      "Truncate: False\n",
      "Info: {}\n",
      "---\n",
      "Taking action: 147\n",
      "New Observation: {'latent_vector': array([ 1.3413056 , -0.6393575 , -0.79040223,  0.86883   ,  1.256437  ,\n",
      "        0.41381475,  0.79158306,  0.27186072, -0.60058206,  0.760751  ],\n",
      "      dtype=float32), 'action_history': array([369, 387, 983, 147], dtype=int32)}\n",
      "Reward: -0.02293539753168239\n",
      "Done: False\n",
      "Truncate: False\n",
      "Info: {}\n",
      "---\n",
      "Taking action: 993\n",
      "New Observation: {'latent_vector': array([ 1.3413056 , -0.6393575 , -0.79040223,  0.7206345 ,  1.256437  ,\n",
      "        0.41381475,  0.79158306,  0.27186072, -0.60058206,  0.760751  ],\n",
      "      dtype=float32), 'action_history': array([387, 983, 147, 993], dtype=int32)}\n",
      "Reward: -0.12417283343154795\n",
      "Done: False\n",
      "Truncate: False\n",
      "Info: {}\n",
      "---\n",
      "Taking action: 354\n",
      "New Observation: {'latent_vector': array([ 1.3413056 , -0.6393575 , -0.79040223,  0.7206345 ,  0.6788344 ,\n",
      "        0.41381475,  0.79158306,  0.27186072, -0.60058206,  0.760751  ],\n",
      "      dtype=float32), 'action_history': array([983, 147, 993, 354], dtype=int32)}\n",
      "Reward: -0.022454449190732362\n",
      "Done: False\n",
      "Truncate: False\n",
      "Info: {}\n",
      "---\n",
      "Taking action: 742\n",
      "New Observation: {'latent_vector': array([ 1.3413056 , -0.6393575 ,  0.73191696,  0.7206345 ,  0.6788344 ,\n",
      "        0.41381475,  0.79158306,  0.27186072, -0.60058206,  0.760751  ],\n",
      "      dtype=float32), 'action_history': array([147, 993, 354, 742], dtype=int32)}\n",
      "Reward: -0.14970767346430702\n",
      "Done: False\n",
      "Truncate: False\n",
      "Info: {}\n",
      "---\n",
      "Taking action: 896\n",
      "New Observation: {'latent_vector': array([ 1.3413056 , -0.6393575 ,  0.73191696,  0.7206345 ,  0.6788344 ,\n",
      "        0.41381475,  0.18609811,  0.27186072, -0.60058206,  0.760751  ],\n",
      "      dtype=float32), 'action_history': array([993, 354, 742, 896], dtype=int32)}\n",
      "Reward: -0.35873625398450615\n",
      "Done: False\n",
      "Truncate: False\n",
      "Info: {}\n",
      "---\n",
      "Taking action: 927\n",
      "New Observation: {'latent_vector': array([ 1.3413056 , -0.6393575 ,  0.73191696,  0.7206345 ,  0.6788344 ,\n",
      "        0.41381475,  0.18609811,  0.7071485 , -0.60058206,  0.760751  ],\n",
      "      dtype=float32), 'action_history': array([354, 742, 896, 927], dtype=int32)}\n",
      "Reward: 0.29589784469529945\n",
      "Done: False\n",
      "Truncate: False\n",
      "Info: {}\n",
      "---\n",
      "Taking action: 31\n",
      "New Observation: {'latent_vector': array([ 1.3413056 ,  0.81787026,  0.73191696,  0.7206345 ,  0.6788344 ,\n",
      "        0.41381475,  0.18609811,  0.7071485 , -0.60058206,  0.760751  ],\n",
      "      dtype=float32), 'action_history': array([742, 896, 927,  31], dtype=int32)}\n",
      "Reward: -0.2642841344721225\n",
      "Done: False\n",
      "Truncate: False\n",
      "Info: {}\n",
      "---\n",
      "Taking action: 196\n",
      "New Observation: {'latent_vector': array([ 1.3413056 ,  0.81787026,  0.73191696,  0.7206345 ,  0.6788344 ,\n",
      "        0.41381475,  0.8201132 ,  0.7071485 , -0.60058206,  0.760751  ],\n",
      "      dtype=float32), 'action_history': array([896, 927,  31, 196], dtype=int32)}\n",
      "Reward: 0.7558144641809257\n",
      "Done: False\n",
      "Truncate: False\n",
      "Info: {}\n",
      "---\n",
      "Taking action: 247\n",
      "New Observation: {'latent_vector': array([ 1.3413056 ,  0.81787026,  0.73191696,  0.7206345 ,  0.6788344 ,\n",
      "        0.41381475,  0.8201132 ,  0.10863385, -0.60058206,  0.760751  ],\n",
      "      dtype=float32), 'action_history': array([927,  31, 196, 247], dtype=int32)}\n",
      "Reward: 0.1929651849508326\n",
      "Done: False\n",
      "Truncate: False\n",
      "Info: {}\n",
      "---\n",
      "Taking action: 111\n",
      "New Observation: {'latent_vector': array([ 1.3413056 ,  0.6905221 ,  0.73191696,  0.7206345 ,  0.6788344 ,\n",
      "        0.41381475,  0.8201132 ,  0.10863385, -0.60058206,  0.760751  ],\n",
      "      dtype=float32), 'action_history': array([ 31, 196, 247, 111], dtype=int32)}\n",
      "Reward: -0.8941918406065076\n",
      "Done: False\n",
      "Truncate: False\n",
      "Info: {}\n",
      "---\n",
      "Taking action: 171\n",
      "New Observation: {'latent_vector': array([ 1.3413056 ,  0.94753325,  0.73191696,  0.7206345 ,  0.6788344 ,\n",
      "        0.41381475,  0.8201132 ,  0.10863385, -0.60058206,  0.760751  ],\n",
      "      dtype=float32), 'action_history': array([196, 247, 111, 171], dtype=int32)}\n",
      "Reward: 0.28500033692737003\n",
      "Done: False\n",
      "Truncate: False\n",
      "Info: {}\n",
      "---\n",
      "Taking action: 629\n",
      "New Observation: {'latent_vector': array([ 1.3413056 ,  0.94753325,  0.73191696,  0.7206345 ,  0.6788344 ,\n",
      "        0.41381475,  0.8201132 ,  0.10863385, -0.60058206,  0.43017542],\n",
      "      dtype=float32), 'action_history': array([247, 111, 171, 629], dtype=int32)}\n",
      "Reward: -0.11206078647489826\n",
      "Done: False\n",
      "Truncate: False\n",
      "Info: {}\n",
      "---\n",
      "Taking action: 732\n",
      "New Observation: {'latent_vector': array([ 1.3413056 ,  0.94753325,  0.22656456,  0.7206345 ,  0.6788344 ,\n",
      "        0.41381475,  0.8201132 ,  0.10863385, -0.60058206,  0.43017542],\n",
      "      dtype=float32), 'action_history': array([111, 171, 629, 732], dtype=int32)}\n",
      "Reward: 0.11865963636950239\n",
      "Done: True\n",
      "Truncate: True\n",
      "Info: {}\n",
      "---\n",
      "Episode finished after 20 timesteps.\n"
     ]
    }
   ],
   "source": [
    "# Manual testing of the environment\n",
    "\n",
    "# Create an instance of the environment with dummy parameters\n",
    "env = VQVAE_Env(\n",
    "    embed_dim=10,\n",
    "    num_embeddings=101,\n",
    "    max_allowed_actions=20,\n",
    "    surrogate_model=MockSurrogateModel(),  # Dummy surrogate model\n",
    "    decoder=MockDecoder(),  # Dummy decoder\n",
    "    codebook=mock_codebook  \n",
    ")\n",
    "\n",
    "# Reset the environment to start a new episode\n",
    "observation = env.reset()\n",
    "print(\"Initial Observation:\", observation)\n",
    "\n",
    "# Take actions in a loop until the episode ends\n",
    "done = False\n",
    "while not done:\n",
    "    # Sample a random action\n",
    "    action = env.sample_action()\n",
    "    print(\"Taking action:\", action)\n",
    "\n",
    "    # Perform the action in the environment\n",
    "    observation, reward, done, truncate, info = env.step(action)\n",
    "    print(\"New Observation:\", observation)\n",
    "    print(\"Reward:\", reward)\n",
    "    print(\"Done:\", done)\n",
    "    print(\"Truncate:\", truncate)\n",
    "    print(\"Info:\", info)\n",
    "    print(\"---\")\n",
    "\n",
    "    if done:\n",
    "        print(\"Episode finished after {} timesteps.\".format(env.step_count))\n",
    "        break\n",
    "\n",
    "# Close the environment\n",
    "env.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
