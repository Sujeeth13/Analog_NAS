{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from VQVAE_environment import VQVAE_Env\n",
    "from stable_baselines3.common.env_checker import check_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy surrogate model, decoder, and codebook to test the environment\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class MockSurrogateModel:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def evaluate(self, decoded_state):\n",
    "        # Return a dummy accuracy value\n",
    "        return np.random.random()\n",
    "\n",
    "class MockDecoder:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def decode(self, state):\n",
    "        # Return a dummy decoded state\n",
    "        return state\n",
    "\n",
    "# Create a dummy codebook as a numpy array\n",
    "# Assuming the embed_dim is 10 and you have 100 embeddings plus 1 for the stop action\n",
    "mock_codebook = np.random.rand(100, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize your environment with the mock components\n",
    "env = VQVAE_Env(embed_dim=10, num_embeddings=100, max_allowed_actions=200,\n",
    "                surrogate_model=MockSurrogateModel(), decoder=MockDecoder(), codebook=mock_codebook,\n",
    "                num_previous_actions=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using check_env from stable baselines 3 to check if the environment is compatible with stable baselines\n",
    "check_env(env, warn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Observation: ({'latent_vector': array([ 0.33280307,  0.14973612,  1.3733011 , -2.159962  , -0.41633907,\n",
      "       -0.22256349, -1.1731398 , -0.13592942, -1.1384417 , -0.46192485],\n",
      "      dtype=float32), 'action_history': array([-1, -1, -1, -1], dtype=int32)}, {})\n",
      "Taking action: 394\n",
      "New Observation: {'latent_vector': array([ 0.33280307,  0.14973612,  1.3733011 , -2.159962  ,  0.39785   ,\n",
      "       -0.22256349, -1.1731398 , -0.13592942, -1.1384417 , -0.46192485],\n",
      "      dtype=float32), 'action_history': array([ -1,  -1,  -1, 394], dtype=int32)}\n",
      "Reward: 0.7042221596755148\n",
      "Done: False\n",
      "Truncate: False\n",
      "Info: {}\n",
      "---\n",
      "Taking action: 800\n",
      "New Observation: {'latent_vector': array([ 0.77780163,  0.14973612,  1.3733011 , -2.159962  ,  0.39785   ,\n",
      "       -0.22256349, -1.1731398 , -0.13592942, -1.1384417 , -0.46192485],\n",
      "      dtype=float32), 'action_history': array([ -1,  -1, 394, 800], dtype=int32)}\n",
      "Reward: -0.5505472755989522\n",
      "Done: False\n",
      "Truncate: False\n",
      "Info: {}\n",
      "---\n",
      "Taking action: 60\n",
      "New Observation: {'latent_vector': array([ 0.33934712,  0.14973612,  1.3733011 , -2.159962  ,  0.39785   ,\n",
      "       -0.22256349, -1.1731398 , -0.13592942, -1.1384417 , -0.46192485],\n",
      "      dtype=float32), 'action_history': array([ -1, 394, 800,  60], dtype=int32)}\n",
      "Reward: 0.3887219327222037\n",
      "Done: False\n",
      "Truncate: False\n",
      "Info: {}\n",
      "---\n",
      "Taking action: 631\n",
      "New Observation: {'latent_vector': array([ 0.33934712,  0.84975725,  1.3733011 , -2.159962  ,  0.39785   ,\n",
      "       -0.22256349, -1.1731398 , -0.13592942, -1.1384417 , -0.46192485],\n",
      "      dtype=float32), 'action_history': array([394, 800,  60, 631], dtype=int32)}\n",
      "Reward: -0.09510503834712836\n",
      "Done: False\n",
      "Truncate: False\n",
      "Info: {}\n",
      "---\n",
      "Taking action: 641\n",
      "New Observation: {'latent_vector': array([ 0.33934712,  0.32304353,  1.3733011 , -2.159962  ,  0.39785   ,\n",
      "       -0.22256349, -1.1731398 , -0.13592942, -1.1384417 , -0.46192485],\n",
      "      dtype=float32), 'action_history': array([800,  60, 631, 641], dtype=int32)}\n",
      "Reward: 0.015259545730151869\n",
      "Done: False\n",
      "Truncate: False\n",
      "Info: {}\n",
      "---\n",
      "Taking action: 163\n",
      "New Observation: {'latent_vector': array([ 0.33934712,  0.32304353,  1.3733011 ,  0.05960894,  0.39785   ,\n",
      "       -0.22256349, -1.1731398 , -0.13592942, -1.1384417 , -0.46192485],\n",
      "      dtype=float32), 'action_history': array([ 60, 631, 641, 163], dtype=int32)}\n",
      "Reward: 0.225244119737956\n",
      "Done: False\n",
      "Truncate: False\n",
      "Info: {}\n",
      "---\n",
      "Taking action: 67\n",
      "New Observation: {'latent_vector': array([ 0.33934712,  0.32304353,  1.3733011 ,  0.05960894,  0.39785   ,\n",
      "       -0.22256349, -1.1731398 ,  0.5552526 , -1.1384417 , -0.46192485],\n",
      "      dtype=float32), 'action_history': array([631, 641, 163,  67], dtype=int32)}\n",
      "Reward: 0.17579138178449538\n",
      "Done: False\n",
      "Truncate: False\n",
      "Info: {}\n",
      "---\n",
      "Taking action: 622\n",
      "New Observation: {'latent_vector': array([ 0.33934712,  0.32304353,  0.43712682,  0.05960894,  0.39785   ,\n",
      "       -0.22256349, -1.1731398 ,  0.5552526 , -1.1384417 , -0.46192485],\n",
      "      dtype=float32), 'action_history': array([641, 163,  67, 622], dtype=int32)}\n",
      "Reward: -0.02488027857378794\n",
      "Done: False\n",
      "Truncate: False\n",
      "Info: {}\n",
      "---\n",
      "Taking action: 223\n",
      "New Observation: {'latent_vector': array([ 0.33934712,  0.32304353,  0.43712682,  0.50790703,  0.39785   ,\n",
      "       -0.22256349, -1.1731398 ,  0.5552526 , -1.1384417 , -0.46192485],\n",
      "      dtype=float32), 'action_history': array([163,  67, 622, 223], dtype=int32)}\n",
      "Reward: -0.38875710843129896\n",
      "Done: False\n",
      "Truncate: False\n",
      "Info: {}\n",
      "---\n",
      "Taking action: 657\n",
      "New Observation: {'latent_vector': array([ 0.33934712,  0.32304353,  0.43712682,  0.50790703,  0.39785   ,\n",
      "       -0.22256349, -1.1731398 ,  0.50324726, -1.1384417 , -0.46192485],\n",
      "      dtype=float32), 'action_history': array([ 67, 622, 223, 657], dtype=int32)}\n",
      "Reward: 0.11007437668933384\n",
      "Done: False\n",
      "Truncate: False\n",
      "Info: {}\n",
      "---\n",
      "Taking action: 327\n",
      "New Observation: {'latent_vector': array([ 0.33934712,  0.32304353,  0.43712682,  0.50790703,  0.39785   ,\n",
      "       -0.22256349, -1.1731398 ,  0.90391785, -1.1384417 , -0.46192485],\n",
      "      dtype=float32), 'action_history': array([622, 223, 657, 327], dtype=int32)}\n",
      "Reward: -0.5049447787843273\n",
      "Done: False\n",
      "Truncate: False\n",
      "Info: {}\n",
      "---\n",
      "Taking action: 625\n",
      "New Observation: {'latent_vector': array([ 0.33934712,  0.32304353,  0.43712682,  0.50790703,  0.39785   ,\n",
      "        0.934539  , -1.1731398 ,  0.90391785, -1.1384417 , -0.46192485],\n",
      "      dtype=float32), 'action_history': array([223, 657, 327, 625], dtype=int32)}\n",
      "Reward: 0.27217047375480374\n",
      "Done: False\n",
      "Truncate: False\n",
      "Info: {}\n",
      "---\n",
      "Taking action: 782\n",
      "New Observation: {'latent_vector': array([ 0.33934712,  0.32304353,  0.62539166,  0.50790703,  0.39785   ,\n",
      "        0.934539  , -1.1731398 ,  0.90391785, -1.1384417 , -0.46192485],\n",
      "      dtype=float32), 'action_history': array([657, 327, 625, 782], dtype=int32)}\n",
      "Reward: -0.06595068671258986\n",
      "Done: False\n",
      "Truncate: False\n",
      "Info: {}\n",
      "---\n",
      "Taking action: 987\n",
      "New Observation: {'latent_vector': array([ 0.33934712,  0.32304353,  0.62539166,  0.50790703,  0.39785   ,\n",
      "        0.934539  , -1.1731398 ,  0.85775405, -1.1384417 , -0.46192485],\n",
      "      dtype=float32), 'action_history': array([327, 625, 782, 987], dtype=int32)}\n",
      "Reward: -0.13541831485649714\n",
      "Done: False\n",
      "Truncate: False\n",
      "Info: {}\n",
      "---\n",
      "Taking action: 629\n",
      "New Observation: {'latent_vector': array([ 0.33934712,  0.32304353,  0.62539166,  0.50790703,  0.39785   ,\n",
      "        0.934539  , -1.1731398 ,  0.85775405, -1.1384417 ,  0.33470857],\n",
      "      dtype=float32), 'action_history': array([625, 782, 987, 629], dtype=int32)}\n",
      "Reward: 0.3354170530586903\n",
      "Done: False\n",
      "Truncate: False\n",
      "Info: {}\n",
      "---\n",
      "Taking action: 554\n",
      "New Observation: {'latent_vector': array([ 0.33934712,  0.32304353,  0.62539166,  0.50790703,  0.78888357,\n",
      "        0.934539  , -1.1731398 ,  0.85775405, -1.1384417 ,  0.33470857],\n",
      "      dtype=float32), 'action_history': array([782, 987, 629, 554], dtype=int32)}\n",
      "Reward: 0.0036971920921151646\n",
      "Done: False\n",
      "Truncate: False\n",
      "Info: {}\n",
      "---\n",
      "Taking action: 18\n",
      "New Observation: {'latent_vector': array([ 0.33934712,  0.32304353,  0.62539166,  0.50790703,  0.78888357,\n",
      "        0.934539  , -1.1731398 ,  0.85775405,  0.54948086,  0.33470857],\n",
      "      dtype=float32), 'action_history': array([987, 629, 554,  18], dtype=int32)}\n",
      "Reward: -0.13882309977261542\n",
      "Done: False\n",
      "Truncate: False\n",
      "Info: {}\n",
      "---\n",
      "Taking action: 912\n",
      "New Observation: {'latent_vector': array([ 0.33934712,  0.32304353,  0.1898073 ,  0.50790703,  0.78888357,\n",
      "        0.934539  , -1.1731398 ,  0.85775405,  0.54948086,  0.33470857],\n",
      "      dtype=float32), 'action_history': array([629, 554,  18, 912], dtype=int32)}\n",
      "Reward: 0.29513651030018906\n",
      "Done: False\n",
      "Truncate: False\n",
      "Info: {}\n",
      "---\n",
      "Taking action: 266\n",
      "New Observation: {'latent_vector': array([0.33934712, 0.32304353, 0.1898073 , 0.50790703, 0.78888357,\n",
      "       0.934539  , 0.64209217, 0.85775405, 0.54948086, 0.33470857],\n",
      "      dtype=float32), 'action_history': array([554,  18, 912, 266], dtype=int32)}\n",
      "Reward: -0.4640324967109698\n",
      "Done: False\n",
      "Truncate: False\n",
      "Info: {}\n",
      "---\n",
      "Taking action: 186\n",
      "New Observation: {'latent_vector': array([0.33934712, 0.32304353, 0.1898073 , 0.50790703, 0.78888357,\n",
      "       0.934539  , 0.19623391, 0.85775405, 0.54948086, 0.33470857],\n",
      "      dtype=float32), 'action_history': array([ 18, 912, 266, 186], dtype=int32)}\n",
      "Reward: -0.022592559836541293\n",
      "Done: True\n",
      "Truncate: True\n",
      "Info: {}\n",
      "---\n",
      "Episode finished after 20 timesteps.\n"
     ]
    }
   ],
   "source": [
    "# Manual testing of the environment\n",
    "\n",
    "# Create an instance of the environment with dummy parameters\n",
    "env = VQVAE_Env(\n",
    "    embed_dim=10,\n",
    "    num_embeddings=100,\n",
    "    max_allowed_actions=20,\n",
    "    surrogate_model=MockSurrogateModel(),  # Dummy surrogate model\n",
    "    decoder=MockDecoder(),  # Dummy decoder\n",
    "    codebook=mock_codebook  \n",
    ")\n",
    "\n",
    "# Reset the environment to start a new episode\n",
    "observation = env.reset()\n",
    "print(\"Initial Observation:\", observation)\n",
    "\n",
    "# Take actions in a loop until the episode ends\n",
    "done = False\n",
    "while not done:\n",
    "    # Sample a random action\n",
    "    action = env.sample_action()\n",
    "    print(\"Taking action:\", action)\n",
    "\n",
    "    # Perform the action in the environment\n",
    "    observation, reward, done, truncate, info = env.step(action)\n",
    "    print(\"New Observation:\", observation)\n",
    "    print(\"Reward:\", reward)\n",
    "    print(\"Done:\", done)\n",
    "    print(\"Truncate:\", truncate)\n",
    "    print(\"Info:\", info)\n",
    "    print(\"---\")\n",
    "\n",
    "    if done:\n",
    "        print(\"Episode finished after {} timesteps.\".format(env.step_count))\n",
    "        break\n",
    "\n",
    "# Close the environment\n",
    "env.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
