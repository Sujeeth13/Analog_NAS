{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import os\n",
    "from VQVAE_environment import VQVAE_Env\n",
    "from stable_baselines3.common.env_checker import check_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy surrogate model, decoder, and codebook to test the environment\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class MockSurrogateModel:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def evaluate(self, decoded_state):\n",
    "        # Return a dummy accuracy value\n",
    "        return np.random.random()\n",
    "\n",
    "class MockDecoder:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def decode(self, state):\n",
    "        # Return a dummy decoded state\n",
    "        return state\n",
    "\n",
    "# Create a dummy codebook as a numpy array\n",
    "# Assuming the embed_dim is 10 and you have 100 embeddings plus 1 for the stop action\n",
    "mock_codebook = np.random.rand(100, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize your environment with the mock components\n",
    "env = VQVAE_Env(embed_dim=10, num_embeddings=100, max_allowed_actions=200,\n",
    "                surrogate_model=MockSurrogateModel(), decoder=MockDecoder(), codebook=mock_codebook,\n",
    "                num_previous_actions=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using check_env from stable baselines 3 to check if the environment is compatible with stable baselines\n",
    "check_env(env, warn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Observation: ({'latent_vector': array([ 0.9183627 , -0.4344771 ,  1.2901202 ,  0.08363848, -0.44587874,\n",
      "        2.771025  , -1.1156787 ,  1.4472796 , -0.6398812 , -0.24225442],\n",
      "      dtype=float32), 'action_history': array([-1, -1, -1, -1], dtype=int32)}, {})\n",
      "Taking action: 153\n",
      "New Observation: {'latent_vector': array([ 0.9183627 , -0.4344771 ,  1.2901202 ,  0.8510552 , -0.44587874,\n",
      "        2.771025  , -1.1156787 ,  1.4472796 , -0.6398812 , -0.24225442],\n",
      "      dtype=float32), 'action_history': array([ -1,  -1,  -1, 153], dtype=int32)}\n",
      "Reward: 0.07863918608774867\n",
      "Done: False\n",
      "Truncate: False\n",
      "Info: {}\n",
      "---\n",
      "Taking action: 48\n",
      "New Observation: {'latent_vector': array([ 0.9183627 , -0.4344771 ,  1.2901202 ,  0.8510552 , -0.44587874,\n",
      "        2.771025  , -1.1156787 ,  1.4472796 ,  0.8455125 , -0.24225442],\n",
      "      dtype=float32), 'action_history': array([ -1,  -1, 153,  48], dtype=int32)}\n",
      "Reward: 0.24013735096384758\n",
      "Done: False\n",
      "Truncate: False\n",
      "Info: {}\n",
      "---\n",
      "Taking action: 901\n",
      "New Observation: {'latent_vector': array([ 0.9183627 ,  0.9318859 ,  1.2901202 ,  0.8510552 , -0.44587874,\n",
      "        2.771025  , -1.1156787 ,  1.4472796 ,  0.8455125 , -0.24225442],\n",
      "      dtype=float32), 'action_history': array([ -1, 153,  48, 901], dtype=int32)}\n",
      "Reward: 0.652623354945416\n",
      "Done: False\n",
      "Truncate: False\n",
      "Info: {}\n",
      "---\n",
      "Taking action: 829\n",
      "New Observation: {'latent_vector': array([ 0.9183627 ,  0.9318859 ,  1.2901202 ,  0.8510552 , -0.44587874,\n",
      "        2.771025  , -1.1156787 ,  1.4472796 ,  0.8455125 ,  0.38783485],\n",
      "      dtype=float32), 'action_history': array([153,  48, 901, 829], dtype=int32)}\n",
      "Reward: -0.48055166830724494\n",
      "Done: False\n",
      "Truncate: False\n",
      "Info: {}\n",
      "---\n",
      "Taking action: 481\n",
      "New Observation: {'latent_vector': array([ 0.9183627 ,  0.41396183,  1.2901202 ,  0.8510552 , -0.44587874,\n",
      "        2.771025  , -1.1156787 ,  1.4472796 ,  0.8455125 ,  0.38783485],\n",
      "      dtype=float32), 'action_history': array([ 48, 901, 829, 481], dtype=int32)}\n",
      "Reward: -0.20804687620383955\n",
      "Done: False\n",
      "Truncate: False\n",
      "Info: {}\n",
      "---\n",
      "Taking action: 474\n",
      "New Observation: {'latent_vector': array([ 0.9183627 ,  0.41396183,  1.2901202 ,  0.8510552 ,  0.15709157,\n",
      "        2.771025  , -1.1156787 ,  1.4472796 ,  0.8455125 ,  0.38783485],\n",
      "      dtype=float32), 'action_history': array([901, 829, 481, 474], dtype=int32)}\n",
      "Reward: -0.25049737343487843\n",
      "Done: False\n",
      "Truncate: False\n",
      "Info: {}\n",
      "---\n",
      "Taking action: 886\n",
      "New Observation: {'latent_vector': array([0.9183627 , 0.41396183, 1.2901202 , 0.8510552 , 0.15709157,\n",
      "       2.771025  , 0.68341744, 1.4472796 , 0.8455125 , 0.38783485],\n",
      "      dtype=float32), 'action_history': array([829, 481, 474, 886], dtype=int32)}\n",
      "Reward: 0.09312799360173585\n",
      "Done: False\n",
      "Truncate: False\n",
      "Info: {}\n",
      "---\n",
      "Taking action: 948\n",
      "New Observation: {'latent_vector': array([0.9183627 , 0.41396183, 1.2901202 , 0.8510552 , 0.15709157,\n",
      "       2.771025  , 0.68341744, 1.4472796 , 0.10974561, 0.38783485],\n",
      "      dtype=float32), 'action_history': array([481, 474, 886, 948], dtype=int32)}\n",
      "Reward: 0.5796814115278454\n",
      "Done: False\n",
      "Truncate: False\n",
      "Info: {}\n",
      "---\n",
      "Taking action: 12\n",
      "New Observation: {'latent_vector': array([0.9183627 , 0.41396183, 0.525087  , 0.8510552 , 0.15709157,\n",
      "       2.771025  , 0.68341744, 1.4472796 , 0.10974561, 0.38783485],\n",
      "      dtype=float32), 'action_history': array([474, 886, 948,  12], dtype=int32)}\n",
      "Reward: -0.00845475623587566\n",
      "Done: False\n",
      "Truncate: False\n",
      "Info: {}\n",
      "---\n",
      "Taking action: 617\n",
      "New Observation: {'latent_vector': array([0.9183627 , 0.41396183, 0.525087  , 0.8510552 , 0.15709157,\n",
      "       2.771025  , 0.68341744, 0.3457701 , 0.10974561, 0.38783485],\n",
      "      dtype=float32), 'action_history': array([886, 948,  12, 617], dtype=int32)}\n",
      "Reward: 0.23377958389852282\n",
      "Done: False\n",
      "Truncate: False\n",
      "Info: {}\n",
      "---\n",
      "Taking action: 92\n",
      "New Observation: {'latent_vector': array([0.9183627 , 0.41396183, 0.25898954, 0.8510552 , 0.15709157,\n",
      "       2.771025  , 0.68341744, 0.3457701 , 0.10974561, 0.38783485],\n",
      "      dtype=float32), 'action_history': array([948,  12, 617,  92], dtype=int32)}\n",
      "Reward: -0.34500133447768644\n",
      "Done: False\n",
      "Truncate: False\n",
      "Info: {}\n",
      "---\n",
      "Taking action: 560\n",
      "New Observation: {'latent_vector': array([0.19295795, 0.41396183, 0.25898954, 0.8510552 , 0.15709157,\n",
      "       2.771025  , 0.68341744, 0.3457701 , 0.10974561, 0.38783485],\n",
      "      dtype=float32), 'action_history': array([ 12, 617,  92, 560], dtype=int32)}\n",
      "Reward: 0.08819906205392503\n",
      "Done: False\n",
      "Truncate: False\n",
      "Info: {}\n",
      "---\n",
      "Taking action: 177\n",
      "New Observation: {'latent_vector': array([0.19295795, 0.41396183, 0.25898954, 0.8510552 , 0.15709157,\n",
      "       2.771025  , 0.68341744, 0.9892451 , 0.10974561, 0.38783485],\n",
      "      dtype=float32), 'action_history': array([617,  92, 560, 177], dtype=int32)}\n",
      "Reward: -0.06466441598861106\n",
      "Done: False\n",
      "Truncate: False\n",
      "Info: {}\n",
      "---\n",
      "Taking action: 387\n",
      "New Observation: {'latent_vector': array([0.19295795, 0.41396183, 0.25898954, 0.8510552 , 0.15709157,\n",
      "       2.771025  , 0.68341744, 0.9558507 , 0.10974561, 0.38783485],\n",
      "      dtype=float32), 'action_history': array([ 92, 560, 177, 387], dtype=int32)}\n",
      "Reward: -0.07043783841006701\n",
      "Done: False\n",
      "Truncate: False\n",
      "Info: {}\n",
      "---\n",
      "Taking action: 496\n",
      "New Observation: {'latent_vector': array([0.19295795, 0.41396183, 0.25898954, 0.8510552 , 0.15709157,\n",
      "       2.771025  , 0.07608311, 0.9558507 , 0.10974561, 0.38783485],\n",
      "      dtype=float32), 'action_history': array([560, 177, 387, 496], dtype=int32)}\n",
      "Reward: -0.48213567658454615\n",
      "Done: False\n",
      "Truncate: False\n",
      "Info: {}\n",
      "---\n",
      "Taking action: 843\n",
      "New Observation: {'latent_vector': array([0.19295795, 0.41396183, 0.25898954, 0.45332372, 0.15709157,\n",
      "       2.771025  , 0.07608311, 0.9558507 , 0.10974561, 0.38783485],\n",
      "      dtype=float32), 'action_history': array([177, 387, 496, 843], dtype=int32)}\n",
      "Reward: 0.8682422087561429\n",
      "Done: False\n",
      "Truncate: False\n",
      "Info: {}\n",
      "---\n",
      "Taking action: 581\n",
      "New Observation: {'latent_vector': array([0.19295795, 0.54694647, 0.25898954, 0.45332372, 0.15709157,\n",
      "       2.771025  , 0.07608311, 0.9558507 , 0.10974561, 0.38783485],\n",
      "      dtype=float32), 'action_history': array([387, 496, 843, 581], dtype=int32)}\n",
      "Reward: -0.3200117959428186\n",
      "Done: False\n",
      "Truncate: False\n",
      "Info: {}\n",
      "---\n",
      "Taking action: 32\n",
      "New Observation: {'latent_vector': array([0.19295795, 0.54694647, 0.7766801 , 0.45332372, 0.15709157,\n",
      "       2.771025  , 0.07608311, 0.9558507 , 0.10974561, 0.38783485],\n",
      "      dtype=float32), 'action_history': array([496, 843, 581,  32], dtype=int32)}\n",
      "Reward: -0.22168088648302875\n",
      "Done: False\n",
      "Truncate: False\n",
      "Info: {}\n",
      "---\n",
      "Taking action: 871\n",
      "New Observation: {'latent_vector': array([0.19295795, 0.04501911, 0.7766801 , 0.45332372, 0.15709157,\n",
      "       2.771025  , 0.07608311, 0.9558507 , 0.10974561, 0.38783485],\n",
      "      dtype=float32), 'action_history': array([843, 581,  32, 871], dtype=int32)}\n",
      "Reward: 0.47669902779573614\n",
      "Done: False\n",
      "Truncate: False\n",
      "Info: {}\n",
      "---\n",
      "Taking action: 968\n",
      "New Observation: {'latent_vector': array([0.19295795, 0.04501911, 0.7766801 , 0.45332372, 0.15709157,\n",
      "       2.771025  , 0.07608311, 0.9558507 , 0.00803622, 0.38783485],\n",
      "      dtype=float32), 'action_history': array([581,  32, 871, 968], dtype=int32)}\n",
      "Reward: -0.7380720907945796\n",
      "Done: True\n",
      "Truncate: True\n",
      "Info: {}\n",
      "---\n",
      "Episode finished after 20 timesteps.\n"
     ]
    }
   ],
   "source": [
    "# Manual testing of the environment\n",
    "\n",
    "# Create an instance of the environment with dummy parameters\n",
    "env = VQVAE_Env(\n",
    "    embed_dim=10,\n",
    "    num_embeddings=100,\n",
    "    max_allowed_actions=20,\n",
    "    surrogate_model=MockSurrogateModel(),  # Dummy surrogate model\n",
    "    decoder=MockDecoder(),  # Dummy decoder\n",
    "    codebook=mock_codebook  \n",
    ")\n",
    "\n",
    "# Reset the environment to start a new episode\n",
    "observation = env.reset()\n",
    "print(\"Initial Observation:\", observation)\n",
    "\n",
    "# Take actions in a loop until the episode ends\n",
    "done = False\n",
    "while not done:\n",
    "    # Sample a random action\n",
    "    action = env.sample_action()\n",
    "    print(\"Taking action:\", action)\n",
    "\n",
    "    # Perform the action in the environment\n",
    "    observation, reward, done, truncate, info = env.step(action)\n",
    "    print(\"New Observation:\", observation)\n",
    "    print(\"Reward:\", reward)\n",
    "    print(\"Done:\", done)\n",
    "    print(\"Truncate:\", truncate)\n",
    "    print(\"Info:\", info)\n",
    "    print(\"---\")\n",
    "\n",
    "    if done:\n",
    "        print(\"Episode finished after {} timesteps.\".format(env.step_count))\n",
    "        break\n",
    "\n",
    "# Close the environment\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stable Baseline Training Script (with dummy Surrogate & Decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO, A2C, DQN\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "import wandb\n",
    "from wandb.integration.sb3 import WandbCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = 'models'\n",
    "log_dir = 'logs'\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "os.makedirs(log_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the env\n",
    "vec_env = make_vec_env(VQVAE_Env, n_envs=1, env_kwargs=dict(embed_dim=10,\n",
    "    num_embeddings=100,\n",
    "    max_allowed_actions=20,\n",
    "    surrogate_model=MockSurrogateModel(),  # Dummy surrogate model\n",
    "    decoder=MockDecoder(),  # Dummy decoder\n",
    "    codebook=mock_codebook ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('action_history', array([[-1, -1, -1, -1]], dtype=int32)),\n",
       "             ('latent_vector',\n",
       "              array([[ 0.10479282, -1.0372716 ,  1.3703396 ,  0.408466  , -0.2843564 ,\n",
       "                      -1.0075978 ,  0.5536992 , -2.2233102 , -0.07724699, -1.0645074 ]],\n",
       "                    dtype=float32))])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/tawab/Desktop/columbia/Courses/Spring2024/HPML/Project/Analog_NAS/env/wandb/run-20240416_192251-udd0b8vn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/trex-ai/Test/runs/udd0b8vn' target=\"_blank\">confused-gorge-2</a></strong> to <a href='https://wandb.ai/trex-ai/Test' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/trex-ai/Test' target=\"_blank\">https://wandb.ai/trex-ai/Test</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/trex-ai/Test/runs/udd0b8vn' target=\"_blank\">https://wandb.ai/trex-ai/Test/runs/udd0b8vn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config = {\n",
    "    \"policy\": 'MultiInputPolicy',\n",
    "    \"total_timesteps\": 25000\n",
    "}\n",
    "\n",
    "run = wandb.init(\n",
    "    config=config,\n",
    "    sync_tensorboard=True,  # automatically upload SB3's tensorboard metrics to W&B\n",
    "    project=\"Test\",\n",
    "    #monitor_gym=True,       # automatically upload gym environements' videos\n",
    "    save_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m When using several event log directories, please call `wandb.tensorboard.patch(root_logdir=\"...\")` before `wandb.init`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Logging to logs/PPO_2\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 19.8     |\n",
      "|    ep_rew_mean     | 0.464    |\n",
      "| time/              |          |\n",
      "|    fps             | 4440     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 19.9        |\n",
      "|    ep_rew_mean          | 0.518       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2075        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 1           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027024012 |\n",
      "|    clip_fraction        | 0.438       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -6.9        |\n",
      "|    explained_variance   | -1.51       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.084      |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.105      |\n",
      "|    value_loss           | 0.147       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 20         |\n",
      "|    ep_rew_mean          | 0.517      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 1791       |\n",
      "|    iterations           | 3          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 6144       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03122745 |\n",
      "|    clip_fraction        | 0.503      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -6.89      |\n",
      "|    explained_variance   | -0.0211    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.111     |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | -0.118     |\n",
      "|    value_loss           | 0.114      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 19.6       |\n",
      "|    ep_rew_mean          | 0.506      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 1665       |\n",
      "|    iterations           | 4          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 8192       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03222262 |\n",
      "|    clip_fraction        | 0.507      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -6.88      |\n",
      "|    explained_variance   | -0.00144   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0906    |\n",
      "|    n_updates            | 30         |\n",
      "|    policy_gradient_loss | -0.12      |\n",
      "|    value_loss           | 0.119      |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 19.8      |\n",
      "|    ep_rew_mean          | 0.528     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 1609      |\n",
      "|    iterations           | 5         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 10240     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0330295 |\n",
      "|    clip_fraction        | 0.523     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -6.87     |\n",
      "|    explained_variance   | 0.0256    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.101    |\n",
      "|    n_updates            | 40        |\n",
      "|    policy_gradient_loss | -0.122    |\n",
      "|    value_loss           | 0.11      |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 20         |\n",
      "|    ep_rew_mean          | 0.47       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 1567       |\n",
      "|    iterations           | 6          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 12288      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03413942 |\n",
      "|    clip_fraction        | 0.509      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -6.86      |\n",
      "|    explained_variance   | 0.0582     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.105     |\n",
      "|    n_updates            | 50         |\n",
      "|    policy_gradient_loss | -0.121     |\n",
      "|    value_loss           | 0.113      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 19.8        |\n",
      "|    ep_rew_mean          | 0.501       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1543        |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034111816 |\n",
      "|    clip_fraction        | 0.494       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -6.85       |\n",
      "|    explained_variance   | 0.0259      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.101      |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.12       |\n",
      "|    value_loss           | 0.118       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 19.9        |\n",
      "|    ep_rew_mean          | 0.464       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1518        |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034134276 |\n",
      "|    clip_fraction        | 0.499       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -6.84       |\n",
      "|    explained_variance   | 0.0327      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.108      |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.119      |\n",
      "|    value_loss           | 0.111       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 19.5        |\n",
      "|    ep_rew_mean          | 0.51        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1502        |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034304783 |\n",
      "|    clip_fraction        | 0.494       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -6.83       |\n",
      "|    explained_variance   | 0.0577      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0989     |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.119      |\n",
      "|    value_loss           | 0.108       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 19.9        |\n",
      "|    ep_rew_mean          | 0.487       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1486        |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033356376 |\n",
      "|    clip_fraction        | 0.485       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -6.83       |\n",
      "|    explained_variance   | 0.0361      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.102      |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.118      |\n",
      "|    value_loss           | 0.117       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 19.9        |\n",
      "|    ep_rew_mean          | 0.48        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1469        |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033311673 |\n",
      "|    clip_fraction        | 0.466       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -6.82       |\n",
      "|    explained_variance   | 0.0603      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.101      |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.116      |\n",
      "|    value_loss           | 0.108       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 19.9       |\n",
      "|    ep_rew_mean          | 0.488      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 1462       |\n",
      "|    iterations           | 12         |\n",
      "|    time_elapsed         | 16         |\n",
      "|    total_timesteps      | 24576      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03463162 |\n",
      "|    clip_fraction        | 0.465      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -6.81      |\n",
      "|    explained_variance   | 0.0471     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.121     |\n",
      "|    n_updates            | 110        |\n",
      "|    policy_gradient_loss | -0.116     |\n",
      "|    value_loss           | 0.108      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 19.8       |\n",
      "|    ep_rew_mean          | 0.494      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 1456       |\n",
      "|    iterations           | 13         |\n",
      "|    time_elapsed         | 18         |\n",
      "|    total_timesteps      | 26624      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03399954 |\n",
      "|    clip_fraction        | 0.461      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -6.8       |\n",
      "|    explained_variance   | 0.0404     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0966    |\n",
      "|    n_updates            | 120        |\n",
      "|    policy_gradient_loss | -0.115     |\n",
      "|    value_loss           | 0.112      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>global_step</td><td>▁▂▂▃▃▄▅▅▆▆▇▇█</td></tr><tr><td>rollout/ep_len_mean</td><td>▆▇█▃▅█▅▇▁▇▆▇▅</td></tr><tr><td>rollout/ep_rew_mean</td><td>▁▇▇▆█▂▅▁▆▄▃▄▄</td></tr><tr><td>time/fps</td><td>█▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/approx_kl</td><td>▁▅▆▇████▇▇█▇</td></tr><tr><td>train/clip_fraction</td><td>▁▆▇█▇▆▆▆▅▃▃▃</td></tr><tr><td>train/clip_range</td><td>▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/entropy_loss</td><td>▁▂▃▃▄▅▅▆▆▇▇█</td></tr><tr><td>train/explained_variance</td><td>▁███████████</td></tr><tr><td>train/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/loss</td><td>█▃▇▅▄▅▃▅▅▅▁▆</td></tr><tr><td>train/policy_gradient_loss</td><td>█▃▂▁▂▂▂▂▃▃▄▄</td></tr><tr><td>train/value_loss</td><td>█▂▃▁▂▃▂▁▃▁▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>global_step</td><td>26624</td></tr><tr><td>rollout/ep_len_mean</td><td>19.83</td></tr><tr><td>rollout/ep_rew_mean</td><td>0.49422</td></tr><tr><td>time/fps</td><td>1456.0</td></tr><tr><td>train/approx_kl</td><td>0.034</td></tr><tr><td>train/clip_fraction</td><td>0.46099</td></tr><tr><td>train/clip_range</td><td>0.2</td></tr><tr><td>train/entropy_loss</td><td>-6.80211</td></tr><tr><td>train/explained_variance</td><td>0.0404</td></tr><tr><td>train/learning_rate</td><td>0.0003</td></tr><tr><td>train/loss</td><td>-0.09659</td></tr><tr><td>train/policy_gradient_loss</td><td>-0.11477</td></tr><tr><td>train/value_loss</td><td>0.11164</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">confused-gorge-2</strong> at: <a href='https://wandb.ai/trex-ai/Test/runs/udd0b8vn' target=\"_blank\">https://wandb.ai/trex-ai/Test/runs/udd0b8vn</a><br/> View project at: <a href='https://wandb.ai/trex-ai/Test' target=\"_blank\">https://wandb.ai/trex-ai/Test</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240416_192251-udd0b8vn/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train the agent\n",
    "model = PPO(config['policy'], vec_env, verbose=1, tensorboard_log=log_dir)\n",
    "model.learn(\n",
    "    total_timesteps=config[\"total_timesteps\"],\n",
    "    callback=WandbCallback(\n",
    "        model_save_path=f\"models/{run.id}\",\n",
    "        verbose=2,\n",
    "    ),\n",
    ")\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1\n",
      "Action:  [955]\n",
      "obs= OrderedDict([('action_history', array([[ -1,  -1,  -1, 955]], dtype=int32)), ('latent_vector', array([[ 0.05283138, -0.09423001,  0.31997174, -0.37998945,  0.15682319,\n",
      "         0.20595308,  0.30689126,  0.27883932, -1.5004084 ,  0.84475845]],\n",
      "      dtype=float32))]) reward= [0.5419346] done= [False]\n",
      "Step 2\n",
      "Action:  [921]\n",
      "obs= OrderedDict([('action_history', array([[ -1,  -1, 955, 921]], dtype=int32)), ('latent_vector', array([[ 0.05283138,  0.6752847 ,  0.31997174, -0.37998945,  0.15682319,\n",
      "         0.20595308,  0.30689126,  0.27883932, -1.5004084 ,  0.84475845]],\n",
      "      dtype=float32))]) reward= [0.4386297] done= [False]\n",
      "Step 3\n",
      "Action:  [921]\n",
      "obs= OrderedDict([('action_history', array([[ -1, 955, 921, 921]], dtype=int32)), ('latent_vector', array([[ 0.05283138,  0.6752847 ,  0.31997174, -0.37998945,  0.15682319,\n",
      "         0.20595308,  0.30689126,  0.27883932, -1.5004084 ,  0.84475845]],\n",
      "      dtype=float32))]) reward= [-0.22091103] done= [False]\n",
      "Step 4\n",
      "Action:  [921]\n",
      "obs= OrderedDict([('action_history', array([[955, 921, 921, 921]], dtype=int32)), ('latent_vector', array([[ 0.05283138,  0.6752847 ,  0.31997174, -0.37998945,  0.15682319,\n",
      "         0.20595308,  0.30689126,  0.27883932, -1.5004084 ,  0.84475845]],\n",
      "      dtype=float32))]) reward= [-0.08159108] done= [False]\n",
      "Step 5\n",
      "Action:  [921]\n",
      "obs= OrderedDict([('action_history', array([[921, 921, 921, 921]], dtype=int32)), ('latent_vector', array([[ 0.05283138,  0.6752847 ,  0.31997174, -0.37998945,  0.15682319,\n",
      "         0.20595308,  0.30689126,  0.27883932, -1.5004084 ,  0.84475845]],\n",
      "      dtype=float32))]) reward= [0.24929601] done= [False]\n",
      "Step 6\n",
      "Action:  [921]\n",
      "obs= OrderedDict([('action_history', array([[921, 921, 921, 921]], dtype=int32)), ('latent_vector', array([[ 0.05283138,  0.6752847 ,  0.31997174, -0.37998945,  0.15682319,\n",
      "         0.20595308,  0.30689126,  0.27883932, -1.5004084 ,  0.84475845]],\n",
      "      dtype=float32))]) reward= [-0.25281563] done= [False]\n",
      "Step 7\n",
      "Action:  [921]\n",
      "obs= OrderedDict([('action_history', array([[921, 921, 921, 921]], dtype=int32)), ('latent_vector', array([[ 0.05283138,  0.6752847 ,  0.31997174, -0.37998945,  0.15682319,\n",
      "         0.20595308,  0.30689126,  0.27883932, -1.5004084 ,  0.84475845]],\n",
      "      dtype=float32))]) reward= [0.20796894] done= [False]\n",
      "Step 8\n",
      "Action:  [921]\n",
      "obs= OrderedDict([('action_history', array([[921, 921, 921, 921]], dtype=int32)), ('latent_vector', array([[ 0.05283138,  0.6752847 ,  0.31997174, -0.37998945,  0.15682319,\n",
      "         0.20595308,  0.30689126,  0.27883932, -1.5004084 ,  0.84475845]],\n",
      "      dtype=float32))]) reward= [-0.10844392] done= [False]\n",
      "Step 9\n",
      "Action:  [921]\n",
      "obs= OrderedDict([('action_history', array([[921, 921, 921, 921]], dtype=int32)), ('latent_vector', array([[ 0.05283138,  0.6752847 ,  0.31997174, -0.37998945,  0.15682319,\n",
      "         0.20595308,  0.30689126,  0.27883932, -1.5004084 ,  0.84475845]],\n",
      "      dtype=float32))]) reward= [-0.6976873] done= [False]\n",
      "Step 10\n",
      "Action:  [921]\n",
      "obs= OrderedDict([('action_history', array([[921, 921, 921, 921]], dtype=int32)), ('latent_vector', array([[ 0.05283138,  0.6752847 ,  0.31997174, -0.37998945,  0.15682319,\n",
      "         0.20595308,  0.30689126,  0.27883932, -1.5004084 ,  0.84475845]],\n",
      "      dtype=float32))]) reward= [0.08249594] done= [False]\n",
      "Step 11\n",
      "Action:  [921]\n",
      "obs= OrderedDict([('action_history', array([[921, 921, 921, 921]], dtype=int32)), ('latent_vector', array([[ 0.05283138,  0.6752847 ,  0.31997174, -0.37998945,  0.15682319,\n",
      "         0.20595308,  0.30689126,  0.27883932, -1.5004084 ,  0.84475845]],\n",
      "      dtype=float32))]) reward= [0.51485085] done= [False]\n",
      "Step 12\n",
      "Action:  [921]\n",
      "obs= OrderedDict([('action_history', array([[921, 921, 921, 921]], dtype=int32)), ('latent_vector', array([[ 0.05283138,  0.6752847 ,  0.31997174, -0.37998945,  0.15682319,\n",
      "         0.20595308,  0.30689126,  0.27883932, -1.5004084 ,  0.84475845]],\n",
      "      dtype=float32))]) reward= [-0.37892142] done= [False]\n",
      "Step 13\n",
      "Action:  [921]\n",
      "obs= OrderedDict([('action_history', array([[921, 921, 921, 921]], dtype=int32)), ('latent_vector', array([[ 0.05283138,  0.6752847 ,  0.31997174, -0.37998945,  0.15682319,\n",
      "         0.20595308,  0.30689126,  0.27883932, -1.5004084 ,  0.84475845]],\n",
      "      dtype=float32))]) reward= [0.25185964] done= [False]\n",
      "Step 14\n",
      "Action:  [921]\n",
      "obs= OrderedDict([('action_history', array([[921, 921, 921, 921]], dtype=int32)), ('latent_vector', array([[ 0.05283138,  0.6752847 ,  0.31997174, -0.37998945,  0.15682319,\n",
      "         0.20595308,  0.30689126,  0.27883932, -1.5004084 ,  0.84475845]],\n",
      "      dtype=float32))]) reward= [-0.51519626] done= [False]\n",
      "Step 15\n",
      "Action:  [921]\n",
      "obs= OrderedDict([('action_history', array([[921, 921, 921, 921]], dtype=int32)), ('latent_vector', array([[ 0.05283138,  0.6752847 ,  0.31997174, -0.37998945,  0.15682319,\n",
      "         0.20595308,  0.30689126,  0.27883932, -1.5004084 ,  0.84475845]],\n",
      "      dtype=float32))]) reward= [0.08700968] done= [False]\n",
      "Step 16\n",
      "Action:  [921]\n",
      "obs= OrderedDict([('action_history', array([[921, 921, 921, 921]], dtype=int32)), ('latent_vector', array([[ 0.05283138,  0.6752847 ,  0.31997174, -0.37998945,  0.15682319,\n",
      "         0.20595308,  0.30689126,  0.27883932, -1.5004084 ,  0.84475845]],\n",
      "      dtype=float32))]) reward= [0.1362022] done= [False]\n",
      "Step 17\n",
      "Action:  [921]\n",
      "obs= OrderedDict([('action_history', array([[921, 921, 921, 921]], dtype=int32)), ('latent_vector', array([[ 0.05283138,  0.6752847 ,  0.31997174, -0.37998945,  0.15682319,\n",
      "         0.20595308,  0.30689126,  0.27883932, -1.5004084 ,  0.84475845]],\n",
      "      dtype=float32))]) reward= [0.37352192] done= [False]\n",
      "Step 18\n",
      "Action:  [921]\n",
      "obs= OrderedDict([('action_history', array([[921, 921, 921, 921]], dtype=int32)), ('latent_vector', array([[ 0.05283138,  0.6752847 ,  0.31997174, -0.37998945,  0.15682319,\n",
      "         0.20595308,  0.30689126,  0.27883932, -1.5004084 ,  0.84475845]],\n",
      "      dtype=float32))]) reward= [-0.31032613] done= [False]\n",
      "Step 19\n",
      "Action:  [921]\n",
      "obs= OrderedDict([('action_history', array([[921, 921, 921, 921]], dtype=int32)), ('latent_vector', array([[ 0.05283138,  0.6752847 ,  0.31997174, -0.37998945,  0.15682319,\n",
      "         0.20595308,  0.30689126,  0.27883932, -1.5004084 ,  0.84475845]],\n",
      "      dtype=float32))]) reward= [0.1585781] done= [False]\n",
      "Step 20\n",
      "Action:  [921]\n",
      "obs= OrderedDict([('action_history', array([[-1, -1, -1, -1]], dtype=int32)), ('latent_vector', array([[-0.5116609 , -0.57361513, -0.65562785,  0.25484824, -0.07792725,\n",
      "        -1.4990935 ,  1.0203893 ,  1.5525414 , -0.8613806 , -1.427906  ]],\n",
      "      dtype=float32))]) reward= [-0.47524548] done= [ True]\n",
      "Goal reached! reward= [-0.47524548]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tawab/miniconda3/envs/DL/lib/python3.11/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:243: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.\n",
      "  warnings.warn(\"You tried to call render() but no `render_mode` was passed to the env constructor.\")\n"
     ]
    }
   ],
   "source": [
    "# Test the trained agent\n",
    "# using the vecenv\n",
    "obs = vec_env.reset()\n",
    "n_steps = 20\n",
    "for step in range(n_steps):\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    print(f\"Step {step + 1}\")\n",
    "    print(\"Action: \", action)\n",
    "    obs, reward, done, info = vec_env.step(action)\n",
    "    print(\"obs=\", obs, \"reward=\", reward, \"done=\", done)\n",
    "    vec_env.render()\n",
    "    if done:\n",
    "        # Note that the VecEnv resets automatically\n",
    "        # when a done signal is encountered\n",
    "        print(\"Goal reached!\", \"reward=\", reward)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33masaficontact\u001b[0m (\u001b[33mtrex-ai\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/tawab/Desktop/columbia/Courses/Spring2024/HPML/Project/Analog_NAS/env/wandb/run-20240416_183840-vgt8hlk5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/trex-ai/my-awesome-project/runs/vgt8hlk5' target=\"_blank\">ruby-tree-1</a></strong> to <a href='https://wandb.ai/trex-ai/my-awesome-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/trex-ai/my-awesome-project' target=\"_blank\">https://wandb.ai/trex-ai/my-awesome-project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/trex-ai/my-awesome-project/runs/vgt8hlk5' target=\"_blank\">https://wandb.ai/trex-ai/my-awesome-project/runs/vgt8hlk5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>▁▄▇▇█▇██</td></tr><tr><td>loss</td><td>█▅▃▃▁▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>0.7922</td></tr><tr><td>loss</td><td>0.20247</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ruby-tree-1</strong> at: <a href='https://wandb.ai/trex-ai/my-awesome-project/runs/vgt8hlk5' target=\"_blank\">https://wandb.ai/trex-ai/my-awesome-project/runs/vgt8hlk5</a><br/> View project at: <a href='https://wandb.ai/trex-ai/my-awesome-project' target=\"_blank\">https://wandb.ai/trex-ai/my-awesome-project</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240416_183840-vgt8hlk5/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reference code for later!!\n",
    "\n",
    "\n",
    "import wandb\n",
    "import random\n",
    "\n",
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"my-awesome-project\",\n",
    "\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": 0.02,\n",
    "    \"architecture\": \"CNN\",\n",
    "    \"dataset\": \"CIFAR-100\",\n",
    "    \"epochs\": 10,\n",
    "    }\n",
    ")\n",
    "\n",
    "# simulate training\n",
    "epochs = 10\n",
    "offset = random.random() / 5\n",
    "for epoch in range(2, epochs):\n",
    "    acc = 1 - 2 ** -epoch - random.random() / epoch - offset\n",
    "    loss = 2 ** -epoch + random.random() / epoch + offset\n",
    "\n",
    "    # log metrics to wandb\n",
    "    wandb.log({\"acc\": acc, \"loss\": loss})\n",
    "\n",
    "# [optional] finish the wandb run, necessary in notebooks\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
