{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%matplotlib notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from env.surrogate.SurrogateModel import SurrogateModel\n",
    "from env.vqvae.decoder import Decoder\n",
    "from env.environment import VQVAE_Env, RenderCallback\n",
    "from agent.RLTrainer import Trainer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the trained environment weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "surrogate_model = 'env/models/surrogate_model.json'\n",
    "codebook = 'env/models/codebook.pth'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_config = {\n",
    "    \"out_dim\": 22,           # Output dimension\n",
    "    \"embed_dim\": 8,          # Embedding dimension\n",
    "    \"h_nodes\": 512,          # Number of hidden nodes\n",
    "    \"dropout\": 0.2,          # Dropout rate\n",
    "    \"scale\": 2,              # Scale factor\n",
    "    \"num_layers\": 5,         # Number of layers\n",
    "    \"load_path\": 'env/models/decoder_model.pth', # Path to load model weights\n",
    "}\n",
    "\n",
    "env_config = {\n",
    "    \"embed_dim\": decoder_config['embed_dim'],    # Embedding dimension\n",
    "    \"num_embeddings\": 14,           # Number of embeddings\n",
    "    \"max_allowed_actions\": 200,      # Maximum allowed actions\n",
    "    \"consider_previous_actions\": False, # Consider previous actions\n",
    "    \"num_previous_actions\": 6,       # Number of previous actions to consider  \n",
    "    \"render_mode\": 'human',          # Render mode\n",
    "    \"render_data\": 'env/render/architectures_trained_on.npy',  # Data for rendering\n",
    "    \"render_labels\": 'env/render/labels.npy',   # Labels for rendering\n",
    "    \"render_log_dir\": 'trainingLogs',                  # Directory for logging data\n",
    "    \"consider_max_params\": True,   # Consider maximum parameters\n",
    "    \"max_params\": 1e9,             # Maximum parameters\n",
    "    \"min_params\" : 1e8,                # Minimum parameters\n",
    "}\n",
    "\n",
    "model_config = {                #TODO: Consider adding entropy coefficient as parameter and policy & value function structure parameters\n",
    "    \"model\": \"PPO\",                # Model type ('PPO', 'A2C', 'DQN', etc.)\n",
    "    \"policy\": 'MultiInputPolicy',          # Policy type\n",
    "    \"total_timesteps\": 1000000,       # Total number of timesteps\n",
    "    \"verbose\": 0,                  # Verbosity level\n",
    "    \"tensorboard_log\": env_config['render_log_dir'],  # Tensorboard log directory\n",
    "    \"n_steps\": 2048,               # Number of steps to run for each environment per update\n",
    "    \"progress_bar\": False,          # Whether to display a progress bar\n",
    "    \"n_epochs\": 12,                # Number of epochs\n",
    "    \"batch_size\": 32,              # Batch size\n",
    "}\n",
    "\n",
    "log_config = {\n",
    "    \"project\": 'Test',                          # Project name in wandb\n",
    "    #\"entity\": 'trex-ai',                            # Entity name in wandb\n",
    "    \"sync_tensorboard\": True,                           # Whether to sync TensorBoard\n",
    "    \"save_code\": True,                                  # Whether to save code in wandb\n",
    "    \"model_save_path\": env_config['render_log_dir'],    # Path to save the model\n",
    "    \"gradient_save_freq\": 100,                          # Frequency to save gradients\n",
    "    \"verbose\": 2,                                       # Verbosity level\n",
    "}\n",
    "\n",
    "custom_callback_function = RenderCallback()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init the trainer object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surrogate model loaded from:  env/models/surrogate_model.json\n",
      "Codebook loaded from:  env/models/codebook.pth\n",
      "Decoder model loaded from:  env/models/decoder_model.pth\n",
      "Environment check passed\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(surrogate_path=surrogate_model, \n",
    "                  codebook_path=codebook, \n",
    "                  decoder_config=decoder_config, \n",
    "                  env_config=env_config, \n",
    "                  model_config=model_config, \n",
    "                  log_config=log_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the below cell to train the PPO agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing WanderDB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33masaficontact\u001b[0m (\u001b[33mtrex-ai\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/tawab/Desktop/columbia/Courses/Spring2024/HPML/Project/Analog_NAS/wandb/run-20240724_232219-tu4ysuzf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/trex-ai/Test/runs/tu4ysuzf' target=\"_blank\">stilted-bush-17</a></strong> to <a href='https://wandb.ai/trex-ai/Test' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/trex-ai/Test' target=\"_blank\">https://wandb.ai/trex-ai/Test</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/trex-ai/Test/runs/tu4ysuzf' target=\"_blank\">https://wandb.ai/trex-ai/Test/runs/tu4ysuzf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up Model\n",
      "Model Config: {'model': 'PPO', 'policy': 'MultiInputPolicy', 'total_timesteps': 1000000, 'verbose': 0, 'tensorboard_log': 'trainingLogs', 'n_steps': 2048, 'progress_bar': False, 'n_epochs': 12, 'batch_size': 32}\n",
      "Resetting Environment\n",
      "Training Model\n",
      "Training Started\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55366dea5ed74c85bc72159ddb74bbc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.571 MB of 0.571 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>rollout/ep_len_mean</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>rollout/ep_rew_mean</td><td>▁█▇█▇▇███▇▇██████▇▇██▇████▇▇▇█▇▇▇▇█▇█▇▇▇</td></tr><tr><td>time/fps</td><td>▁▂▄▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇██████████</td></tr><tr><td>train/approx_kl</td><td>▅▂▃█▆▅▂▆▆▂▄▄▅▅▄▃▆▅▂▄▅▅▄▄▃▃▃▃▆▅▁▂▂▁▄▂▂▂▄▆</td></tr><tr><td>train/clip_fraction</td><td>█▃▄▇▄▄▂▄▅▂▅▃▂▄▆▃▄▄▃▂▃▆▇▄▂▂▂▂▄▄▂▂▂▁▄▂▃▃▅█</td></tr><tr><td>train/clip_range</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/entropy_loss</td><td>▁▆▆▅▆▇▇▆▇▇▇█▇▇▇▇▇▇▇█▇████▇▇▇█▇█████▇▇▇▇▇</td></tr><tr><td>train/explained_variance</td><td>▁▅▁▅▁▅▅▅▅▁▁▅▅▅█▁▅▅▅▁▅▅▅▅▅▅▅▅▅▅▅▅▅▅▆▅▆▅█▅</td></tr><tr><td>train/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/loss</td><td>▃▃▁▂▁▃▂▂▃▂▃▂▃█▂▅▁▃▄▇▄▂▂▅▂▄▂▃▄▃▂▂▂▂▂▅▃▃▇▃</td></tr><tr><td>train/policy_gradient_loss</td><td>▁▄▂▁▂▃▆▅▂▇▅▆▇▆▅▄▃▄▅▇▅▄▆▅▆▇▆▇▅▅▇▆▇█▄▆▅▆▄▂</td></tr><tr><td>train/value_loss</td><td>█▁▁▂▁▃▂▂▂▂▄▂▃▅▃▃▁▂▂▂▂▅▄▂▂▂▂▃▃▁▃▃▂▂▂▂▂▃▄▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>global_step</td><td>1001472</td></tr><tr><td>rollout/ep_len_mean</td><td>2.21</td></tr><tr><td>rollout/ep_rew_mean</td><td>0.33496</td></tr><tr><td>time/fps</td><td>366.0</td></tr><tr><td>train/approx_kl</td><td>0.01996</td></tr><tr><td>train/clip_fraction</td><td>0.37272</td></tr><tr><td>train/clip_range</td><td>0.2</td></tr><tr><td>train/entropy_loss</td><td>-2.44525</td></tr><tr><td>train/explained_variance</td><td>0.0</td></tr><tr><td>train/learning_rate</td><td>0.0003</td></tr><tr><td>train/loss</td><td>0.23932</td></tr><tr><td>train/policy_gradient_loss</td><td>-0.02709</td></tr><tr><td>train/value_loss</td><td>1.08414</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">stilted-bush-17</strong> at: <a href='https://wandb.ai/trex-ai/Test/runs/tu4ysuzf' target=\"_blank\">https://wandb.ai/trex-ai/Test/runs/tu4ysuzf</a><br/> View project at: <a href='https://wandb.ai/trex-ai/Test' target=\"_blank\">https://wandb.ai/trex-ai/Test</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 3 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240724_232219-tu4ysuzf/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"models/test\"\n",
    "trainer.save_model(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the below cell to evaluate the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "save_path = \"models/test\"\n",
    "trainer.load_model(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0\n",
      "Episode 0,200: cum reward: 0.9141677618026733, max reward: 0.9141677618026733, action: [2], last action: [2]\n",
      "Episode 0: Episode Accuracy: 0.9141677618026733, Max Accuracy till Episode: 0.9141677618026733\n",
      "Episode 1\n",
      "Episode 1,200: cum reward: 0.7513466477394104, max reward: 0.7513466477394104, action: [26], last action: [26]\n",
      "Episode 1: Episode Accuracy: 0.7513466477394104, Max Accuracy till Episode: 0.9141677618026733\n",
      "Episode 2\n",
      "Episode 2,200: cum reward: 0.7723342180252075, max reward: 0.7725493907928467, action: [11], last action: [77]\n",
      "Episode 2: Episode Accuracy: 0.7725493907928467, Max Accuracy till Episode: 0.9141677618026733\n",
      "Episode 3\n",
      "Episode 3,200: cum reward: 0.7889947295188904, max reward: 0.7889947295188904, action: [98], last action: [98]\n",
      "Episode 3: Episode Accuracy: 0.7889947295188904, Max Accuracy till Episode: 0.9141677618026733\n",
      "Episode 4\n",
      "Episode 4,200: cum reward: 0.7760769128799438, max reward: 0.7760769128799438, action: [33], last action: [33]\n",
      "Episode 4: Episode Accuracy: 0.7760769128799438, Max Accuracy till Episode: 0.9141677618026733\n",
      "Episode 5\n",
      "Episode 5,200: cum reward: 0.7644068598747253, max reward: 0.7644068598747253, action: [16], last action: [104]\n",
      "Episode 5: Episode Accuracy: 0.7644068598747253, Max Accuracy till Episode: 0.9141677618026733\n",
      "Episode 6\n",
      "Episode 6,200: cum reward: 0.928982675075531, max reward: 0.928982675075531, action: [13], last action: [77]\n",
      "Episode 6: Episode Accuracy: 0.928982675075531, Max Accuracy till Episode: 0.928982675075531\n",
      "Episode 7\n",
      "Episode 7,200: cum reward: 0.9141677618026733, max reward: 0.9201174378395081, action: [52], last action: [52]\n",
      "Episode 7: Episode Accuracy: 0.9201174378395081, Max Accuracy till Episode: 0.928982675075531\n",
      "Episode 8\n",
      "Episode 8,200: cum reward: 0.7584961652755737, max reward: 0.7584961652755737, action: [52], last action: [52]\n",
      "Episode 8: Episode Accuracy: 0.7584961652755737, Max Accuracy till Episode: 0.928982675075531\n",
      "Episode 9\n",
      "Episode 9,200: cum reward: 0.9238914251327515, max reward: 0.9238914251327515, action: [70], last action: [77]\n",
      "Episode 9: Episode Accuracy: 0.9238914251327515, Max Accuracy till Episode: 0.928982675075531\n",
      "Episode 10\n",
      "Episode 10,200: cum reward: 0.9238914251327515, max reward: 0.9238914251327515, action: [57], last action: [15]\n",
      "Episode 10: Episode Accuracy: 0.9238914251327515, Max Accuracy till Episode: 0.928982675075531\n",
      "Episode 11\n",
      "Episode 11,200: cum reward: 0.7686992287635803, max reward: 0.7686992287635803, action: [60], last action: [60]\n",
      "Episode 11: Episode Accuracy: 0.7686992287635803, Max Accuracy till Episode: 0.928982675075531\n",
      "Episode 12\n",
      "Episode 12,200: cum reward: 0.9238914251327515, max reward: 0.9238914251327515, action: [19], last action: [87]\n",
      "Episode 12: Episode Accuracy: 0.9238914251327515, Max Accuracy till Episode: 0.928982675075531\n",
      "Episode 13\n",
      "Episode 13,200: cum reward: 0.7686756253242493, max reward: 0.7686756253242493, action: [59], last action: [59]\n",
      "Episode 13: Episode Accuracy: 0.7686756253242493, Max Accuracy till Episode: 0.928982675075531\n",
      "Episode 14\n",
      "Episode 14,200: cum reward: 0.9190993905067444, max reward: 0.9281994104385376, action: [42], last action: [97]\n",
      "Episode 14: Episode Accuracy: 0.9281994104385376, Max Accuracy till Episode: 0.928982675075531\n",
      "Episode 15\n",
      "Episode 15,200: cum reward: 0.770458459854126, max reward: 0.770458459854126, action: [31], last action: [31]\n",
      "Episode 15: Episode Accuracy: 0.770458459854126, Max Accuracy till Episode: 0.928982675075531\n",
      "Episode 16\n",
      "Episode 16,200: cum reward: 0.9243929982185364, max reward: 0.9243929982185364, action: [9], last action: [33]\n",
      "Episode 16: Episode Accuracy: 0.9243929982185364, Max Accuracy till Episode: 0.928982675075531\n",
      "Episode 17\n",
      "Episode 17,200: cum reward: 0.7372422218322754, max reward: 0.7372422218322754, action: [31], last action: [31]\n",
      "Episode 17: Episode Accuracy: 0.7372422218322754, Max Accuracy till Episode: 0.928982675075531\n",
      "Episode 18\n",
      "Episode 18,200: cum reward: 0.923737645149231, max reward: 0.923737645149231, action: [77], last action: [77]\n",
      "Episode 18: Episode Accuracy: 0.923737645149231, Max Accuracy till Episode: 0.928982675075531\n",
      "Episode 19\n",
      "Episode 19,200: cum reward: 0.7575494050979614, max reward: 0.8088371157646179, action: [23], last action: [16]\n",
      "Episode 19: Episode Accuracy: 0.8088371157646179, Max Accuracy till Episode: 0.928982675075531\n",
      "Episode 20\n",
      "Episode 20,200: cum reward: 0.7839134335517883, max reward: 0.7889947295188904, action: [60], last action: [102]\n",
      "Episode 20: Episode Accuracy: 0.7889947295188904, Max Accuracy till Episode: 0.928982675075531\n",
      "Episode 21\n",
      "Episode 21,200: cum reward: 0.770458459854126, max reward: 0.770458459854126, action: [27], last action: [27]\n",
      "Episode 21: Episode Accuracy: 0.770458459854126, Max Accuracy till Episode: 0.928982675075531\n",
      "Episode 22\n",
      "Episode 22,200: cum reward: 0.9238914251327515, max reward: 0.9238914251327515, action: [87], last action: [7]\n",
      "Episode 22: Episode Accuracy: 0.9238914251327515, Max Accuracy till Episode: 0.928982675075531\n",
      "Episode 23\n",
      "Episode 23,200: cum reward: 0.7733031511306763, max reward: 0.7889947295188904, action: [78], last action: [104]\n",
      "Episode 23: Episode Accuracy: 0.7889947295188904, Max Accuracy till Episode: 0.928982675075531\n",
      "Episode 24\n",
      "Episode 24,4: cum reward: 0.925092875957489, max reward: 0.925092875957489, action: [23], last action: [112]\n",
      "Episode 24: Episode Accuracy: 0.925092875957489, Max Accuracy till Episode: 0.928982675075531\n",
      "Episode 25\n",
      "Episode 25,200: cum reward: 0.9238914251327515, max reward: 0.9238914251327515, action: [55], last action: [55]\n",
      "Episode 25: Episode Accuracy: 0.9238914251327515, Max Accuracy till Episode: 0.928982675075531\n",
      "Episode 26\n",
      "Episode 26,200: cum reward: 0.7404413819313049, max reward: 0.7889947295188904, action: [93], last action: [98]\n",
      "Episode 26: Episode Accuracy: 0.7889947295188904, Max Accuracy till Episode: 0.928982675075531\n",
      "Episode 27\n",
      "Episode 27,200: cum reward: 0.9276752471923828, max reward: 0.9276752471923828, action: [4], last action: [15]\n",
      "Episode 27: Episode Accuracy: 0.9276752471923828, Max Accuracy till Episode: 0.928982675075531\n",
      "Episode 28\n",
      "Episode 28,200: cum reward: 0.9238914251327515, max reward: 0.9238914251327515, action: [63], last action: [63]\n",
      "Episode 28: Episode Accuracy: 0.9238914251327515, Max Accuracy till Episode: 0.928982675075531\n",
      "Episode 29\n",
      "Episode 29,200: cum reward: 0.944547712802887, max reward: 0.944547712802887, action: [105], last action: [101]\n",
      "Episode 29: Episode Accuracy: 0.944547712802887, Max Accuracy till Episode: 0.944547712802887\n",
      "Episode 30\n",
      "Episode 30,200: cum reward: 0.7889947295188904, max reward: 0.7889947295188904, action: [47], last action: [47]\n",
      "Episode 30: Episode Accuracy: 0.7889947295188904, Max Accuracy till Episode: 0.944547712802887\n",
      "Episode 31\n",
      "Episode 31,200: cum reward: 0.7889947295188904, max reward: 0.7889947295188904, action: [14], last action: [14]\n",
      "Episode 31: Episode Accuracy: 0.7889947295188904, Max Accuracy till Episode: 0.944547712802887\n",
      "Episode 32\n",
      "Episode 32,200: cum reward: 0.9243929982185364, max reward: 0.9243929982185364, action: [55], last action: [55]\n",
      "Episode 32: Episode Accuracy: 0.9243929982185364, Max Accuracy till Episode: 0.944547712802887\n",
      "Episode 33\n",
      "Episode 33,200: cum reward: 0.7375838756561279, max reward: 0.7375838756561279, action: [44], last action: [44]\n",
      "Episode 33: Episode Accuracy: 0.7375838756561279, Max Accuracy till Episode: 0.944547712802887\n",
      "Episode 34\n",
      "Episode 34,200: cum reward: 0.7723342180252075, max reward: 0.7723342180252075, action: [98], last action: [98]\n",
      "Episode 34: Episode Accuracy: 0.7723342180252075, Max Accuracy till Episode: 0.944547712802887\n",
      "Episode 35\n",
      "Episode 35,200: cum reward: 0.7644068598747253, max reward: 0.7723342180252075, action: [13], last action: [77]\n",
      "Episode 35: Episode Accuracy: 0.7723342180252075, Max Accuracy till Episode: 0.944547712802887\n",
      "Episode 36\n",
      "Episode 36,200: cum reward: 0.7348170280456543, max reward: 0.7348170280456543, action: [16], last action: [16]\n",
      "Episode 36: Episode Accuracy: 0.7348170280456543, Max Accuracy till Episode: 0.944547712802887\n",
      "Episode 37\n",
      "Episode 37,200: cum reward: 0.7375838756561279, max reward: 0.819848895072937, action: [64], last action: [81]\n",
      "Episode 37: Episode Accuracy: 0.819848895072937, Max Accuracy till Episode: 0.944547712802887\n",
      "Episode 38\n",
      "Episode 38,200: cum reward: 0.759606122970581, max reward: 0.759606122970581, action: [92], last action: [22]\n",
      "Episode 38: Episode Accuracy: 0.759606122970581, Max Accuracy till Episode: 0.944547712802887\n",
      "Episode 39\n",
      "Episode 39,200: cum reward: 0.7584961652755737, max reward: 0.7584961652755737, action: [98], last action: [98]\n",
      "Episode 39: Episode Accuracy: 0.7584961652755737, Max Accuracy till Episode: 0.944547712802887\n",
      "Episode 40\n",
      "Episode 40,200: cum reward: 0.923686146736145, max reward: 0.9238914251327515, action: [85], last action: [44]\n",
      "Episode 40: Episode Accuracy: 0.9238914251327515, Max Accuracy till Episode: 0.944547712802887\n",
      "Episode 41\n",
      "Episode 41,200: cum reward: 0.7349642515182495, max reward: 0.7349642515182495, action: [74], last action: [74]\n",
      "Episode 41: Episode Accuracy: 0.7349642515182495, Max Accuracy till Episode: 0.944547712802887\n",
      "Episode 42\n",
      "Episode 42,200: cum reward: 0.9471520185470581, max reward: 0.9471520185470581, action: [95], last action: [95]\n",
      "Episode 42: Episode Accuracy: 0.9471520185470581, Max Accuracy till Episode: 0.9471520185470581\n",
      "Episode 43\n",
      "Episode 43,200: cum reward: 0.759606122970581, max reward: 0.759606122970581, action: [12], last action: [20]\n",
      "Episode 43: Episode Accuracy: 0.759606122970581, Max Accuracy till Episode: 0.9471520185470581\n",
      "Episode 44\n",
      "Episode 44,200: cum reward: 0.7889947295188904, max reward: 0.7889947295188904, action: [74], last action: [72]\n",
      "Episode 44: Episode Accuracy: 0.7889947295188904, Max Accuracy till Episode: 0.9471520185470581\n",
      "Episode 45\n",
      "Episode 45,200: cum reward: 0.9141677618026733, max reward: 0.9141677618026733, action: [31], last action: [29]\n",
      "Episode 45: Episode Accuracy: 0.9141677618026733, Max Accuracy till Episode: 0.9471520185470581\n",
      "Episode 46\n",
      "Episode 46,200: cum reward: 0.9238914251327515, max reward: 0.9238914251327515, action: [85], last action: [98]\n",
      "Episode 46: Episode Accuracy: 0.9238914251327515, Max Accuracy till Episode: 0.9471520185470581\n",
      "Episode 47\n",
      "Episode 47,200: cum reward: 0.9342565536499023, max reward: 0.9373398423194885, action: [63], last action: [63]\n",
      "Episode 47: Episode Accuracy: 0.9373398423194885, Max Accuracy till Episode: 0.9471520185470581\n",
      "Episode 48\n",
      "Episode 48,200: cum reward: 0.9285473227500916, max reward: 0.9285473227500916, action: [23], last action: [23]\n",
      "Episode 48: Episode Accuracy: 0.9285473227500916, Max Accuracy till Episode: 0.9471520185470581\n",
      "Episode 49\n",
      "Episode 49,200: cum reward: 0.7348170280456543, max reward: 0.7349642515182495, action: [3], last action: [87]\n",
      "Episode 49: Episode Accuracy: 0.7349642515182495, Max Accuracy till Episode: 0.9471520185470581\n",
      "Episode 50\n",
      "Episode 50,200: cum reward: 0.7733031511306763, max reward: 0.7733031511306763, action: [104], last action: [104]\n",
      "Episode 50: Episode Accuracy: 0.7733031511306763, Max Accuracy till Episode: 0.9471520185470581\n",
      "Episode 51\n",
      "Episode 51,200: cum reward: 0.7889947295188904, max reward: 0.7889947295188904, action: [87], last action: [87]\n",
      "Episode 51: Episode Accuracy: 0.7889947295188904, Max Accuracy till Episode: 0.9471520185470581\n",
      "Episode 52\n",
      "Episode 52,200: cum reward: 0.7562592625617981, max reward: 0.7562592625617981, action: [47], last action: [47]\n",
      "Episode 52: Episode Accuracy: 0.7562592625617981, Max Accuracy till Episode: 0.9471520185470581\n",
      "Episode 53\n",
      "Episode 53,200: cum reward: 0.9212459325790405, max reward: 0.9252868294715881, action: [13], last action: [13]\n",
      "Episode 53: Episode Accuracy: 0.9252868294715881, Max Accuracy till Episode: 0.9471520185470581\n",
      "Episode 54\n",
      "Episode 54,200: cum reward: 0.9281994104385376, max reward: 0.9281994104385376, action: [16], last action: [55]\n",
      "Episode 54: Episode Accuracy: 0.9281994104385376, Max Accuracy till Episode: 0.9471520185470581\n",
      "Episode 55\n",
      "Episode 55,200: cum reward: 0.7575494050979614, max reward: 0.7744229435920715, action: [51], last action: [27]\n",
      "Episode 55: Episode Accuracy: 0.7744229435920715, Max Accuracy till Episode: 0.9471520185470581\n",
      "Episode 56\n",
      "Episode 56,200: cum reward: 0.7889947295188904, max reward: 0.7889947295188904, action: [111], last action: [111]\n",
      "Episode 56: Episode Accuracy: 0.7889947295188904, Max Accuracy till Episode: 0.9471520185470581\n",
      "Episode 57\n",
      "Episode 57,200: cum reward: 0.9441887140274048, max reward: 0.9441887140274048, action: [95], last action: [95]\n",
      "Episode 57: Episode Accuracy: 0.9441887140274048, Max Accuracy till Episode: 0.9471520185470581\n",
      "Episode 58\n",
      "Episode 58,200: cum reward: 0.9331715703010559, max reward: 0.9331715703010559, action: [44], last action: [44]\n",
      "Episode 58: Episode Accuracy: 0.9331715703010559, Max Accuracy till Episode: 0.9471520185470581\n",
      "Episode 59\n",
      "Episode 59,200: cum reward: 0.759606122970581, max reward: 0.7889947295188904, action: [81], last action: [86]\n",
      "Episode 59: Episode Accuracy: 0.7889947295188904, Max Accuracy till Episode: 0.9471520185470581\n",
      "Episode 60\n",
      "Episode 60,200: cum reward: 0.9326378107070923, max reward: 0.944547712802887, action: [74], last action: [95]\n",
      "Episode 60: Episode Accuracy: 0.944547712802887, Max Accuracy till Episode: 0.9471520185470581\n",
      "Episode 61\n",
      "Episode 61,200: cum reward: 0.7733031511306763, max reward: 0.7733031511306763, action: [85], last action: [44]\n",
      "Episode 61: Episode Accuracy: 0.7733031511306763, Max Accuracy till Episode: 0.9471520185470581\n",
      "Episode 62\n",
      "Episode 62,200: cum reward: 0.9281994104385376, max reward: 0.9281994104385376, action: [23], last action: [23]\n",
      "Episode 62: Episode Accuracy: 0.9281994104385376, Max Accuracy till Episode: 0.9471520185470581\n",
      "Episode 63\n",
      "Episode 63,200: cum reward: 0.9325972199440002, max reward: 0.9325972199440002, action: [16], last action: [55]\n",
      "Episode 63: Episode Accuracy: 0.9325972199440002, Max Accuracy till Episode: 0.9471520185470581\n",
      "Episode 64\n",
      "Episode 64,200: cum reward: 0.770458459854126, max reward: 0.770458459854126, action: [29], last action: [29]\n",
      "Episode 64: Episode Accuracy: 0.770458459854126, Max Accuracy till Episode: 0.9471520185470581\n",
      "Episode 65\n",
      "Episode 65,200: cum reward: 0.7734160423278809, max reward: 0.7734160423278809, action: [4], last action: [4]\n",
      "Episode 65: Episode Accuracy: 0.7734160423278809, Max Accuracy till Episode: 0.9471520185470581\n",
      "Episode 66\n",
      "Episode 66,200: cum reward: 0.9252868294715881, max reward: 0.9281994104385376, action: [100], last action: [15]\n",
      "Episode 66: Episode Accuracy: 0.9281994104385376, Max Accuracy till Episode: 0.9471520185470581\n",
      "Episode 67\n",
      "Episode 67,200: cum reward: 0.9238914251327515, max reward: 0.9238914251327515, action: [66], last action: [29]\n",
      "Episode 67: Episode Accuracy: 0.9238914251327515, Max Accuracy till Episode: 0.9471520185470581\n",
      "Episode 68\n",
      "Episode 68,200: cum reward: 0.7372422218322754, max reward: 0.7372422218322754, action: [85], last action: [85]\n",
      "Episode 68: Episode Accuracy: 0.7372422218322754, Max Accuracy till Episode: 0.9471520185470581\n",
      "Episode 69\n",
      "Episode 69,200: cum reward: 0.7686756253242493, max reward: 0.7744229435920715, action: [82], last action: [87]\n",
      "Episode 69: Episode Accuracy: 0.7744229435920715, Max Accuracy till Episode: 0.9471520185470581\n",
      "Episode 70\n",
      "Episode 70,200: cum reward: 0.9438520669937134, max reward: 0.9438520669937134, action: [33], last action: [33]\n",
      "Episode 70: Episode Accuracy: 0.9438520669937134, Max Accuracy till Episode: 0.9471520185470581\n",
      "Episode 71\n",
      "Episode 71,200: cum reward: 0.9202268123626709, max reward: 0.9202268123626709, action: [55], last action: [85]\n",
      "Episode 71: Episode Accuracy: 0.9202268123626709, Max Accuracy till Episode: 0.9471520185470581\n",
      "Episode 72\n",
      "Episode 72,200: cum reward: 0.7686992287635803, max reward: 0.7839134335517883, action: [98], last action: [27]\n",
      "Episode 72: Episode Accuracy: 0.7839134335517883, Max Accuracy till Episode: 0.9471520185470581\n",
      "Episode 73\n",
      "Episode 73,200: cum reward: 0.9281994104385376, max reward: 0.9285393953323364, action: [79], last action: [15]\n",
      "Episode 73: Episode Accuracy: 0.9285393953323364, Max Accuracy till Episode: 0.9471520185470581\n",
      "Episode 74\n",
      "Episode 74,200: cum reward: 0.7889947295188904, max reward: 0.7889947295188904, action: [60], last action: [60]\n",
      "Episode 74: Episode Accuracy: 0.7889947295188904, Max Accuracy till Episode: 0.9471520185470581\n",
      "Episode 75\n",
      "Episode 75,200: cum reward: 0.759606122970581, max reward: 0.7889947295188904, action: [64], last action: [14]\n",
      "Episode 75: Episode Accuracy: 0.7889947295188904, Max Accuracy till Episode: 0.9471520185470581\n",
      "Episode 76\n",
      "Episode 76,200: cum reward: 0.9190993905067444, max reward: 0.9190993905067444, action: [44], last action: [44]\n",
      "Episode 76: Episode Accuracy: 0.9190993905067444, Max Accuracy till Episode: 0.9471520185470581\n",
      "Episode 77\n",
      "Episode 77,200: cum reward: 0.7771551012992859, max reward: 0.7771551012992859, action: [16], last action: [16]\n",
      "Episode 77: Episode Accuracy: 0.7771551012992859, Max Accuracy till Episode: 0.9471520185470581\n",
      "Episode 78\n",
      "Episode 78,200: cum reward: 0.9243929982185364, max reward: 0.9243929982185364, action: [66], last action: [68]\n",
      "Episode 78: Episode Accuracy: 0.9243929982185364, Max Accuracy till Episode: 0.9471520185470581\n",
      "Episode 79\n",
      "Episode 79,200: cum reward: 0.9202268123626709, max reward: 0.9202268123626709, action: [33], last action: [33]\n",
      "Episode 79: Episode Accuracy: 0.9202268123626709, Max Accuracy till Episode: 0.9471520185470581\n",
      "Episode 80\n",
      "Episode 80,200: cum reward: 0.9238914251327515, max reward: 0.9281994104385376, action: [13], last action: [92]\n",
      "Episode 80: Episode Accuracy: 0.9281994104385376, Max Accuracy till Episode: 0.9471520185470581\n",
      "Episode 81\n",
      "Episode 81,200: cum reward: 0.9319303035736084, max reward: 0.9319303035736084, action: [25], last action: [25]\n",
      "Episode 81: Episode Accuracy: 0.9319303035736084, Max Accuracy till Episode: 0.9471520185470581\n",
      "Episode 82\n",
      "Episode 82,200: cum reward: 0.7562592625617981, max reward: 0.7584961652755737, action: [66], last action: [29]\n",
      "Episode 82: Episode Accuracy: 0.7584961652755737, Max Accuracy till Episode: 0.9471520185470581\n",
      "Episode 83\n",
      "Episode 83,200: cum reward: 0.7889947295188904, max reward: 0.7889947295188904, action: [65], last action: [65]\n",
      "Episode 83: Episode Accuracy: 0.7889947295188904, Max Accuracy till Episode: 0.9471520185470581\n",
      "Episode 84\n",
      "Episode 84,200: cum reward: 0.9471520185470581, max reward: 0.9471520185470581, action: [7], last action: [7]\n",
      "Episode 84: Episode Accuracy: 0.9471520185470581, Max Accuracy till Episode: 0.9471520185470581\n",
      "Episode 85\n",
      "Episode 85,200: cum reward: 0.7889947295188904, max reward: 0.7889947295188904, action: [81], last action: [81]\n",
      "Episode 85: Episode Accuracy: 0.7889947295188904, Max Accuracy till Episode: 0.9471520185470581\n",
      "Episode 86\n",
      "Episode 86,200: cum reward: 0.9429131746292114, max reward: 0.944547712802887, action: [98], last action: [95]\n",
      "Episode 86: Episode Accuracy: 0.944547712802887, Max Accuracy till Episode: 0.9471520185470581\n",
      "Episode 87\n",
      "Episode 87,200: cum reward: 0.7644068598747253, max reward: 0.9141677618026733, action: [41], last action: [98]\n",
      "Episode 87: Episode Accuracy: 0.9141677618026733, Max Accuracy till Episode: 0.9471520185470581\n",
      "Episode 88\n",
      "Episode 88,200: cum reward: 0.7349642515182495, max reward: 0.7725493907928467, action: [41], last action: [39]\n",
      "Episode 88: Episode Accuracy: 0.7725493907928467, Max Accuracy till Episode: 0.9471520185470581\n",
      "Episode 89\n",
      "Episode 89,200: cum reward: 0.7348170280456543, max reward: 0.7348170280456543, action: [36], last action: [92]\n",
      "Episode 89: Episode Accuracy: 0.7348170280456543, Max Accuracy till Episode: 0.9471520185470581\n",
      "Episode 90\n",
      "Episode 90,200: cum reward: 0.9243929982185364, max reward: 0.9243929982185364, action: [16], last action: [28]\n",
      "Episode 90: Episode Accuracy: 0.9243929982185364, Max Accuracy till Episode: 0.9471520185470581\n",
      "Episode 91\n",
      "Episode 91,200: cum reward: 0.7372422218322754, max reward: 0.7372422218322754, action: [22], last action: [22]\n",
      "Episode 91: Episode Accuracy: 0.7372422218322754, Max Accuracy till Episode: 0.9471520185470581\n",
      "Episode 92\n",
      "Episode 92,200: cum reward: 0.7348170280456543, max reward: 0.7889947295188904, action: [93], last action: [39]\n",
      "Episode 92: Episode Accuracy: 0.7889947295188904, Max Accuracy till Episode: 0.9471520185470581\n",
      "Episode 93\n",
      "Episode 93,200: cum reward: 0.7889947295188904, max reward: 0.7889947295188904, action: [60], last action: [60]\n",
      "Episode 93: Episode Accuracy: 0.7889947295188904, Max Accuracy till Episode: 0.9471520185470581\n",
      "Episode 94\n",
      "Episode 94,4: cum reward: 0.9331715703010559, max reward: 0.944547712802887, action: [64], last action: [112]\n",
      "Episode 94: Episode Accuracy: 0.944547712802887, Max Accuracy till Episode: 0.9471520185470581\n",
      "Episode 95\n",
      "Episode 95,200: cum reward: 0.9319303035736084, max reward: 0.9369305968284607, action: [98], last action: [98]\n",
      "Episode 95: Episode Accuracy: 0.9369305968284607, Max Accuracy till Episode: 0.9471520185470581\n",
      "Episode 96\n",
      "Episode 96,200: cum reward: 0.9227997064590454, max reward: 0.9227997064590454, action: [55], last action: [16]\n",
      "Episode 96: Episode Accuracy: 0.9227997064590454, Max Accuracy till Episode: 0.9471520185470581\n",
      "Episode 97\n",
      "Episode 97,200: cum reward: 0.7889947295188904, max reward: 0.7889947295188904, action: [60], last action: [60]\n",
      "Episode 97: Episode Accuracy: 0.7889947295188904, Max Accuracy till Episode: 0.9471520185470581\n",
      "Episode 98\n",
      "Episode 98,200: cum reward: 0.7744229435920715, max reward: 0.7744229435920715, action: [27], last action: [12]\n",
      "Episode 98: Episode Accuracy: 0.7744229435920715, Max Accuracy till Episode: 0.9471520185470581\n",
      "Episode 99\n",
      "Episode 99,200: cum reward: 0.7551933526992798, max reward: 0.7575494050979614, action: [13], last action: [9]\n",
      "Episode 99: Episode Accuracy: 0.7575494050979614, Max Accuracy till Episode: 0.9471520185470581\n",
      "Average Accuracy: 0.843926191329956\n"
     ]
    }
   ],
   "source": [
    "x = trainer.evaluate_accuracy(num_episodes=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[108.9913,   6.4508,  12.8702,  11.9179,  10.2512,   6.4805,   3.7706,\n",
       "           3.9087,   4.6221,  10.6889,   4.0760,   3.8912,   9.1347,   3.6351,\n",
       "           3.5720,   7.1175,   2.9104,   2.9669,   5.9397,   0.0000,   0.0000,\n",
       "           3.2940]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.947152"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.calculate_accuracy_for_decoded_state(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if the recommended model exists in the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_values = torch.tensor(\n",
    "            [8, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0],\n",
    "            dtype=torch.float32,\n",
    "        )\n",
    "max_values = torch.tensor(\n",
    "    [\n",
    "        128,\n",
    "        5,\n",
    "        16,\n",
    "        16,\n",
    "        16,\n",
    "        16,\n",
    "        16,\n",
    "        4,\n",
    "        4,\n",
    "        12,\n",
    "        4,\n",
    "        4,\n",
    "        12,\n",
    "        4,\n",
    "        4,\n",
    "        12,\n",
    "        4,\n",
    "        4,\n",
    "        12,\n",
    "        4,\n",
    "        4,\n",
    "        12,\n",
    "    ],\n",
    "    dtype=torch.float32,\n",
    ")\n",
    "\n",
    "def clip_values(X):\n",
    "    # Shape of X = (batch_size, 22)\n",
    "\n",
    "    # Round all values\n",
    "    rounded_data = torch.round(X)\n",
    "\n",
    "    # Now clamp each column individually\n",
    "    clamped_data = torch.empty_like(rounded_data)\n",
    "    for i in range(X.shape[1]):\n",
    "        clamped_data[:, i] = torch.clamp(\n",
    "            rounded_data[:, i], min_values[i], max_values[i]\n",
    "        )\n",
    "\n",
    "    return clamped_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k0/y02lb3y56ys_56xwpxd8xg2r0000gn/T/ipykernel_41181/3894422672.py:7: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[column] = df[column].replace(map).astype('float32')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([117.,   1.,   9.,   0.,   0.,   0.,   0.,   2.,   4.,  11.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "df = pd.read_csv(\"data/dataset_cifar10_v1.csv\") # loading the dataset to pandas df\n",
    "map = {\"A\":1.0,\"B\":2.0,\"C\":3.0,\"D\":4.0} # mapping the conv block type to numerical values\n",
    "for column, dtype in df.dtypes.items(): # applying the mapping to the column and also converting to float32\n",
    "    if dtype == 'object':\n",
    "        df[column] = df[column].replace(map).astype('float32')\n",
    "df = df.astype({col: 'float32' for col in df.select_dtypes('int64').columns})\n",
    "df = df.iloc[:,:-3]\n",
    "df.head()\n",
    "df_tensor = torch.tensor(df[df.columns].values,dtype=torch.float32)\n",
    "df_tensor[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = clip_values(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[109.,   5.,  13.,  12.,  10.,   6.,   4.,   4.,   4.,  11.,   4.,   4.,\n",
       "           9.,   4.,   4.,   7.,   3.,   3.,   6.,   1.,   0.,   3.]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in df_tensor: \n",
    "    if torch.allclose(t, x, rtol=1e-05, atol=1e-08):\n",
    "        print(\"True\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in df_tensor: \n",
    "    if torch.all(t.eq(x)):\n",
    "        print(\"True\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_config = {\n",
    "    \"out_dim\": 22,           # Output dimension\n",
    "    \"embed_dim\": 8,          # Embedding dimension\n",
    "    \"h_nodes\": 512,          # Number of hidden nodes\n",
    "    \"dropout\": 0.2,          # Dropout rate\n",
    "    \"scale\": 2,              # Scale factor\n",
    "    \"num_layers\": 5,         # Number of layers\n",
    "    \"load_path\": 'env/models/decoder_model.pth', # Path to load model weights\n",
    "}\n",
    "\n",
    "env_config = {\n",
    "    \"embed_dim\": decoder_config['embed_dim'],    # Embedding dimension\n",
    "    \"num_embeddings\": 14,           # Number of embeddings\n",
    "    \"max_allowed_actions\": 200,      # Maximum allowed actions\n",
    "    \"consider_previous_actions\": False, # Consider previous actions\n",
    "    \"num_previous_actions\": 6,       # Number of previous actions to consider  \n",
    "    \"render_mode\": 'human',          # Render mode\n",
    "    \"render_data\": 'env/render/architectures_trained_on.npy',  # Data for rendering\n",
    "    \"render_labels\": 'env/render/labels.npy',   # Labels for rendering\n",
    "    \"render_log_dir\": 'trainingLogs',                  # Directory for logging data\n",
    "}\n",
    "\n",
    "model_config = {\n",
    "    \"model\": \"PPO\",                # Model type ('PPO', 'A2C', 'DQN', etc.)\n",
    "    \"policy\": 'MlpPolicy',  # Policy type\n",
    "    \"total_timesteps\": 512,       # Total number of timesteps\n",
    "    \"verbose\": 0,                  # Verbosity level\n",
    "    \"tensorboard_log\": env_config['render_log_dir'],  # Tensorboard log directory\n",
    "    \"n_steps\": 512,               # Number of steps to run for each environment per update\n",
    "    \"progress_bar\": False,          # Whether to display a progress bar\n",
    "    \"n_epochs\": 12,                # Number of epochs\n",
    "    \"batch_size\": 32,              # Batch size\n",
    "}\n",
    "\n",
    "log_config = {\n",
    "    \"project\": 'PPO Training',                          # Project name in wandb\n",
    "    #\"entity\": 'trex-ai',                            # Entity name in wandb\n",
    "    \"sync_tensorboard\": True,                           # Whether to sync TensorBoard\n",
    "    \"save_code\": True,                                  # Whether to save code in wandb\n",
    "    \"model_save_path\": env_config['render_log_dir'],    # Path to save the model\n",
    "    \"gradient_save_freq\": 100,                          # Frequency to save gradients\n",
    "    \"verbose\": 2,                                       # Verbosity level\n",
    "}\n",
    "\n",
    "custom_callback_function = RenderCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surrogate model loaded from:  env/models/surrogate_model.json\n",
      "Codebook loaded from:  env/models/codebook.pth\n",
      "Decoder model loaded from:  env/models/decoder_model.pth\n",
      "Environment check passed\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(surrogate_path=surrogate_model, \n",
    "                  codebook_path=codebook, \n",
    "                  decoder_config=decoder_config, \n",
    "                  env_config=env_config, \n",
    "                  model_config=model_config, \n",
    "                  log_config=log_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33masaficontact\u001b[0m (\u001b[33mtrex-ai\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/tawab/Desktop/columbia/Courses/Spring2024/HPML/Project/Analog_NAS/wandb/run-20240509_085324-dhdp5day</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/trex-ai/PPO%20Training/runs/dhdp5day' target=\"_blank\">olive-puddle-46</a></strong> to <a href='https://wandb.ai/trex-ai/PPO%20Training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/trex-ai/PPO%20Training' target=\"_blank\">https://wandb.ai/trex-ai/PPO%20Training</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/trex-ai/PPO%20Training/runs/dhdp5day' target=\"_blank\">https://wandb.ai/trex-ai/PPO%20Training/runs/dhdp5day</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode lengths:  [87, 59, 199, 106]\n",
      "(512, 1, 8)\n",
      "states.shape, ep_len_states.shape (512, 8) (4,)\n",
      "Rendering the environment...\n",
      "(1200, 8) (14, 8) (512, 8)\n"
     ]
    },
    {
     "data": {
      "application/javascript": "/* Put everything inside the global mpl namespace */\n/* global mpl */\nwindow.mpl = {};\n\nmpl.get_websocket_type = function () {\n    if (typeof WebSocket !== 'undefined') {\n        return WebSocket;\n    } else if (typeof MozWebSocket !== 'undefined') {\n        return MozWebSocket;\n    } else {\n        alert(\n            'Your browser does not have WebSocket support. ' +\n                'Please try Chrome, Safari or Firefox ≥ 6. ' +\n                'Firefox 4 and 5 are also supported but you ' +\n                'have to enable WebSockets in about:config.'\n        );\n    }\n};\n\nmpl.figure = function (figure_id, websocket, ondownload, parent_element) {\n    this.id = figure_id;\n\n    this.ws = websocket;\n\n    this.supports_binary = this.ws.binaryType !== undefined;\n\n    if (!this.supports_binary) {\n        var warnings = document.getElementById('mpl-warnings');\n        if (warnings) {\n            warnings.style.display = 'block';\n            warnings.textContent =\n                'This browser does not support binary websocket messages. ' +\n                'Performance may be slow.';\n        }\n    }\n\n    this.imageObj = new Image();\n\n    this.context = undefined;\n    this.message = undefined;\n    this.canvas = undefined;\n    this.rubberband_canvas = undefined;\n    this.rubberband_context = undefined;\n    this.format_dropdown = undefined;\n\n    this.image_mode = 'full';\n\n    this.root = document.createElement('div');\n    this.root.setAttribute('style', 'display: inline-block');\n    this._root_extra_style(this.root);\n\n    parent_element.appendChild(this.root);\n\n    this._init_header(this);\n    this._init_canvas(this);\n    this._init_toolbar(this);\n\n    var fig = this;\n\n    this.waiting = false;\n\n    this.ws.onopen = function () {\n        fig.send_message('supports_binary', { value: fig.supports_binary });\n        fig.send_message('send_image_mode', {});\n        if (fig.ratio !== 1) {\n            fig.send_message('set_device_pixel_ratio', {\n                device_pixel_ratio: fig.ratio,\n            });\n        }\n        fig.send_message('refresh', {});\n    };\n\n    this.imageObj.onload = function () {\n        if (fig.image_mode === 'full') {\n            // Full images could contain transparency (where diff images\n            // almost always do), so we need to clear the canvas so that\n            // there is no ghosting.\n            fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n        }\n        fig.context.drawImage(fig.imageObj, 0, 0);\n    };\n\n    this.imageObj.onunload = function () {\n        fig.ws.close();\n    };\n\n    this.ws.onmessage = this._make_on_message_function(this);\n\n    this.ondownload = ondownload;\n};\n\nmpl.figure.prototype._init_header = function () {\n    var titlebar = document.createElement('div');\n    titlebar.classList =\n        'ui-dialog-titlebar ui-widget-header ui-corner-all ui-helper-clearfix';\n    var titletext = document.createElement('div');\n    titletext.classList = 'ui-dialog-title';\n    titletext.setAttribute(\n        'style',\n        'width: 100%; text-align: center; padding: 3px;'\n    );\n    titlebar.appendChild(titletext);\n    this.root.appendChild(titlebar);\n    this.header = titletext;\n};\n\nmpl.figure.prototype._canvas_extra_style = function (_canvas_div) {};\n\nmpl.figure.prototype._root_extra_style = function (_canvas_div) {};\n\nmpl.figure.prototype._init_canvas = function () {\n    var fig = this;\n\n    var canvas_div = (this.canvas_div = document.createElement('div'));\n    canvas_div.setAttribute('tabindex', '0');\n    canvas_div.setAttribute(\n        'style',\n        'border: 1px solid #ddd;' +\n            'box-sizing: content-box;' +\n            'clear: both;' +\n            'min-height: 1px;' +\n            'min-width: 1px;' +\n            'outline: 0;' +\n            'overflow: hidden;' +\n            'position: relative;' +\n            'resize: both;' +\n            'z-index: 2;'\n    );\n\n    function on_keyboard_event_closure(name) {\n        return function (event) {\n            return fig.key_event(event, name);\n        };\n    }\n\n    canvas_div.addEventListener(\n        'keydown',\n        on_keyboard_event_closure('key_press')\n    );\n    canvas_div.addEventListener(\n        'keyup',\n        on_keyboard_event_closure('key_release')\n    );\n\n    this._canvas_extra_style(canvas_div);\n    this.root.appendChild(canvas_div);\n\n    var canvas = (this.canvas = document.createElement('canvas'));\n    canvas.classList.add('mpl-canvas');\n    canvas.setAttribute(\n        'style',\n        'box-sizing: content-box;' +\n            'pointer-events: none;' +\n            'position: relative;' +\n            'z-index: 0;'\n    );\n\n    this.context = canvas.getContext('2d');\n\n    var backingStore =\n        this.context.backingStorePixelRatio ||\n        this.context.webkitBackingStorePixelRatio ||\n        this.context.mozBackingStorePixelRatio ||\n        this.context.msBackingStorePixelRatio ||\n        this.context.oBackingStorePixelRatio ||\n        this.context.backingStorePixelRatio ||\n        1;\n\n    this.ratio = (window.devicePixelRatio || 1) / backingStore;\n\n    var rubberband_canvas = (this.rubberband_canvas = document.createElement(\n        'canvas'\n    ));\n    rubberband_canvas.setAttribute(\n        'style',\n        'box-sizing: content-box;' +\n            'left: 0;' +\n            'pointer-events: none;' +\n            'position: absolute;' +\n            'top: 0;' +\n            'z-index: 1;'\n    );\n\n    // Apply a ponyfill if ResizeObserver is not implemented by browser.\n    if (this.ResizeObserver === undefined) {\n        if (window.ResizeObserver !== undefined) {\n            this.ResizeObserver = window.ResizeObserver;\n        } else {\n            var obs = _JSXTOOLS_RESIZE_OBSERVER({});\n            this.ResizeObserver = obs.ResizeObserver;\n        }\n    }\n\n    this.resizeObserverInstance = new this.ResizeObserver(function (entries) {\n        var nentries = entries.length;\n        for (var i = 0; i < nentries; i++) {\n            var entry = entries[i];\n            var width, height;\n            if (entry.contentBoxSize) {\n                if (entry.contentBoxSize instanceof Array) {\n                    // Chrome 84 implements new version of spec.\n                    width = entry.contentBoxSize[0].inlineSize;\n                    height = entry.contentBoxSize[0].blockSize;\n                } else {\n                    // Firefox implements old version of spec.\n                    width = entry.contentBoxSize.inlineSize;\n                    height = entry.contentBoxSize.blockSize;\n                }\n            } else {\n                // Chrome <84 implements even older version of spec.\n                width = entry.contentRect.width;\n                height = entry.contentRect.height;\n            }\n\n            // Keep the size of the canvas and rubber band canvas in sync with\n            // the canvas container.\n            if (entry.devicePixelContentBoxSize) {\n                // Chrome 84 implements new version of spec.\n                canvas.setAttribute(\n                    'width',\n                    entry.devicePixelContentBoxSize[0].inlineSize\n                );\n                canvas.setAttribute(\n                    'height',\n                    entry.devicePixelContentBoxSize[0].blockSize\n                );\n            } else {\n                canvas.setAttribute('width', width * fig.ratio);\n                canvas.setAttribute('height', height * fig.ratio);\n            }\n            /* This rescales the canvas back to display pixels, so that it\n             * appears correct on HiDPI screens. */\n            canvas.style.width = width + 'px';\n            canvas.style.height = height + 'px';\n\n            rubberband_canvas.setAttribute('width', width);\n            rubberband_canvas.setAttribute('height', height);\n\n            // And update the size in Python. We ignore the initial 0/0 size\n            // that occurs as the element is placed into the DOM, which should\n            // otherwise not happen due to the minimum size styling.\n            if (fig.ws.readyState == 1 && width != 0 && height != 0) {\n                fig.request_resize(width, height);\n            }\n        }\n    });\n    this.resizeObserverInstance.observe(canvas_div);\n\n    function on_mouse_event_closure(name) {\n        /* User Agent sniffing is bad, but WebKit is busted:\n         * https://bugs.webkit.org/show_bug.cgi?id=144526\n         * https://bugs.webkit.org/show_bug.cgi?id=181818\n         * The worst that happens here is that they get an extra browser\n         * selection when dragging, if this check fails to catch them.\n         */\n        var UA = navigator.userAgent;\n        var isWebKit = /AppleWebKit/.test(UA) && !/Chrome/.test(UA);\n        if(isWebKit) {\n            return function (event) {\n                /* This prevents the web browser from automatically changing to\n                 * the text insertion cursor when the button is pressed. We\n                 * want to control all of the cursor setting manually through\n                 * the 'cursor' event from matplotlib */\n                event.preventDefault()\n                return fig.mouse_event(event, name);\n            };\n        } else {\n            return function (event) {\n                return fig.mouse_event(event, name);\n            };\n        }\n    }\n\n    canvas_div.addEventListener(\n        'mousedown',\n        on_mouse_event_closure('button_press')\n    );\n    canvas_div.addEventListener(\n        'mouseup',\n        on_mouse_event_closure('button_release')\n    );\n    canvas_div.addEventListener(\n        'dblclick',\n        on_mouse_event_closure('dblclick')\n    );\n    // Throttle sequential mouse events to 1 every 20ms.\n    canvas_div.addEventListener(\n        'mousemove',\n        on_mouse_event_closure('motion_notify')\n    );\n\n    canvas_div.addEventListener(\n        'mouseenter',\n        on_mouse_event_closure('figure_enter')\n    );\n    canvas_div.addEventListener(\n        'mouseleave',\n        on_mouse_event_closure('figure_leave')\n    );\n\n    canvas_div.addEventListener('wheel', function (event) {\n        if (event.deltaY < 0) {\n            event.step = 1;\n        } else {\n            event.step = -1;\n        }\n        on_mouse_event_closure('scroll')(event);\n    });\n\n    canvas_div.appendChild(canvas);\n    canvas_div.appendChild(rubberband_canvas);\n\n    this.rubberband_context = rubberband_canvas.getContext('2d');\n    this.rubberband_context.strokeStyle = '#000000';\n\n    this._resize_canvas = function (width, height, forward) {\n        if (forward) {\n            canvas_div.style.width = width + 'px';\n            canvas_div.style.height = height + 'px';\n        }\n    };\n\n    // Disable right mouse context menu.\n    canvas_div.addEventListener('contextmenu', function (_e) {\n        event.preventDefault();\n        return false;\n    });\n\n    function set_focus() {\n        canvas.focus();\n        canvas_div.focus();\n    }\n\n    window.setTimeout(set_focus, 100);\n};\n\nmpl.figure.prototype._init_toolbar = function () {\n    var fig = this;\n\n    var toolbar = document.createElement('div');\n    toolbar.classList = 'mpl-toolbar';\n    this.root.appendChild(toolbar);\n\n    function on_click_closure(name) {\n        return function (_event) {\n            return fig.toolbar_button_onclick(name);\n        };\n    }\n\n    function on_mouseover_closure(tooltip) {\n        return function (event) {\n            if (!event.currentTarget.disabled) {\n                return fig.toolbar_button_onmouseover(tooltip);\n            }\n        };\n    }\n\n    fig.buttons = {};\n    var buttonGroup = document.createElement('div');\n    buttonGroup.classList = 'mpl-button-group';\n    for (var toolbar_ind in mpl.toolbar_items) {\n        var name = mpl.toolbar_items[toolbar_ind][0];\n        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n        var image = mpl.toolbar_items[toolbar_ind][2];\n        var method_name = mpl.toolbar_items[toolbar_ind][3];\n\n        if (!name) {\n            /* Instead of a spacer, we start a new button group. */\n            if (buttonGroup.hasChildNodes()) {\n                toolbar.appendChild(buttonGroup);\n            }\n            buttonGroup = document.createElement('div');\n            buttonGroup.classList = 'mpl-button-group';\n            continue;\n        }\n\n        var button = (fig.buttons[name] = document.createElement('button'));\n        button.classList = 'mpl-widget';\n        button.setAttribute('role', 'button');\n        button.setAttribute('aria-disabled', 'false');\n        button.addEventListener('click', on_click_closure(method_name));\n        button.addEventListener('mouseover', on_mouseover_closure(tooltip));\n\n        var icon_img = document.createElement('img');\n        icon_img.src = '_images/' + image + '.png';\n        icon_img.srcset = '_images/' + image + '_large.png 2x';\n        icon_img.alt = tooltip;\n        button.appendChild(icon_img);\n\n        buttonGroup.appendChild(button);\n    }\n\n    if (buttonGroup.hasChildNodes()) {\n        toolbar.appendChild(buttonGroup);\n    }\n\n    var fmt_picker = document.createElement('select');\n    fmt_picker.classList = 'mpl-widget';\n    toolbar.appendChild(fmt_picker);\n    this.format_dropdown = fmt_picker;\n\n    for (var ind in mpl.extensions) {\n        var fmt = mpl.extensions[ind];\n        var option = document.createElement('option');\n        option.selected = fmt === mpl.default_extension;\n        option.innerHTML = fmt;\n        fmt_picker.appendChild(option);\n    }\n\n    var status_bar = document.createElement('span');\n    status_bar.classList = 'mpl-message';\n    toolbar.appendChild(status_bar);\n    this.message = status_bar;\n};\n\nmpl.figure.prototype.request_resize = function (x_pixels, y_pixels) {\n    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n    // which will in turn request a refresh of the image.\n    this.send_message('resize', { width: x_pixels, height: y_pixels });\n};\n\nmpl.figure.prototype.send_message = function (type, properties) {\n    properties['type'] = type;\n    properties['figure_id'] = this.id;\n    this.ws.send(JSON.stringify(properties));\n};\n\nmpl.figure.prototype.send_draw_message = function () {\n    if (!this.waiting) {\n        this.waiting = true;\n        this.ws.send(JSON.stringify({ type: 'draw', figure_id: this.id }));\n    }\n};\n\nmpl.figure.prototype.handle_save = function (fig, _msg) {\n    var format_dropdown = fig.format_dropdown;\n    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n    fig.ondownload(fig, format);\n};\n\nmpl.figure.prototype.handle_resize = function (fig, msg) {\n    var size = msg['size'];\n    if (size[0] !== fig.canvas.width || size[1] !== fig.canvas.height) {\n        fig._resize_canvas(size[0], size[1], msg['forward']);\n        fig.send_message('refresh', {});\n    }\n};\n\nmpl.figure.prototype.handle_rubberband = function (fig, msg) {\n    var x0 = msg['x0'] / fig.ratio;\n    var y0 = (fig.canvas.height - msg['y0']) / fig.ratio;\n    var x1 = msg['x1'] / fig.ratio;\n    var y1 = (fig.canvas.height - msg['y1']) / fig.ratio;\n    x0 = Math.floor(x0) + 0.5;\n    y0 = Math.floor(y0) + 0.5;\n    x1 = Math.floor(x1) + 0.5;\n    y1 = Math.floor(y1) + 0.5;\n    var min_x = Math.min(x0, x1);\n    var min_y = Math.min(y0, y1);\n    var width = Math.abs(x1 - x0);\n    var height = Math.abs(y1 - y0);\n\n    fig.rubberband_context.clearRect(\n        0,\n        0,\n        fig.canvas.width / fig.ratio,\n        fig.canvas.height / fig.ratio\n    );\n\n    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n};\n\nmpl.figure.prototype.handle_figure_label = function (fig, msg) {\n    // Updates the figure title.\n    fig.header.textContent = msg['label'];\n};\n\nmpl.figure.prototype.handle_cursor = function (fig, msg) {\n    fig.canvas_div.style.cursor = msg['cursor'];\n};\n\nmpl.figure.prototype.handle_message = function (fig, msg) {\n    fig.message.textContent = msg['message'];\n};\n\nmpl.figure.prototype.handle_draw = function (fig, _msg) {\n    // Request the server to send over a new figure.\n    fig.send_draw_message();\n};\n\nmpl.figure.prototype.handle_image_mode = function (fig, msg) {\n    fig.image_mode = msg['mode'];\n};\n\nmpl.figure.prototype.handle_history_buttons = function (fig, msg) {\n    for (var key in msg) {\n        if (!(key in fig.buttons)) {\n            continue;\n        }\n        fig.buttons[key].disabled = !msg[key];\n        fig.buttons[key].setAttribute('aria-disabled', !msg[key]);\n    }\n};\n\nmpl.figure.prototype.handle_navigate_mode = function (fig, msg) {\n    if (msg['mode'] === 'PAN') {\n        fig.buttons['Pan'].classList.add('active');\n        fig.buttons['Zoom'].classList.remove('active');\n    } else if (msg['mode'] === 'ZOOM') {\n        fig.buttons['Pan'].classList.remove('active');\n        fig.buttons['Zoom'].classList.add('active');\n    } else {\n        fig.buttons['Pan'].classList.remove('active');\n        fig.buttons['Zoom'].classList.remove('active');\n    }\n};\n\nmpl.figure.prototype.updated_canvas_event = function () {\n    // Called whenever the canvas gets updated.\n    this.send_message('ack', {});\n};\n\n// A function to construct a web socket function for onmessage handling.\n// Called in the figure constructor.\nmpl.figure.prototype._make_on_message_function = function (fig) {\n    return function socket_on_message(evt) {\n        if (evt.data instanceof Blob) {\n            var img = evt.data;\n            if (img.type !== 'image/png') {\n                /* FIXME: We get \"Resource interpreted as Image but\n                 * transferred with MIME type text/plain:\" errors on\n                 * Chrome.  But how to set the MIME type?  It doesn't seem\n                 * to be part of the websocket stream */\n                img.type = 'image/png';\n            }\n\n            /* Free the memory for the previous frames */\n            if (fig.imageObj.src) {\n                (window.URL || window.webkitURL).revokeObjectURL(\n                    fig.imageObj.src\n                );\n            }\n\n            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n                img\n            );\n            fig.updated_canvas_event();\n            fig.waiting = false;\n            return;\n        } else if (\n            typeof evt.data === 'string' &&\n            evt.data.slice(0, 21) === 'data:image/png;base64'\n        ) {\n            fig.imageObj.src = evt.data;\n            fig.updated_canvas_event();\n            fig.waiting = false;\n            return;\n        }\n\n        var msg = JSON.parse(evt.data);\n        var msg_type = msg['type'];\n\n        // Call the  \"handle_{type}\" callback, which takes\n        // the figure and JSON message as its only arguments.\n        try {\n            var callback = fig['handle_' + msg_type];\n        } catch (e) {\n            console.log(\n                \"No handler for the '\" + msg_type + \"' message type: \",\n                msg\n            );\n            return;\n        }\n\n        if (callback) {\n            try {\n                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n                callback(fig, msg);\n            } catch (e) {\n                console.log(\n                    \"Exception inside the 'handler_\" + msg_type + \"' callback:\",\n                    e,\n                    e.stack,\n                    msg\n                );\n            }\n        }\n    };\n};\n\nfunction getModifiers(event) {\n    var mods = [];\n    if (event.ctrlKey) {\n        mods.push('ctrl');\n    }\n    if (event.altKey) {\n        mods.push('alt');\n    }\n    if (event.shiftKey) {\n        mods.push('shift');\n    }\n    if (event.metaKey) {\n        mods.push('meta');\n    }\n    return mods;\n}\n\n/*\n * return a copy of an object with only non-object keys\n * we need this to avoid circular references\n * https://stackoverflow.com/a/24161582/3208463\n */\nfunction simpleKeys(original) {\n    return Object.keys(original).reduce(function (obj, key) {\n        if (typeof original[key] !== 'object') {\n            obj[key] = original[key];\n        }\n        return obj;\n    }, {});\n}\n\nmpl.figure.prototype.mouse_event = function (event, name) {\n    if (name === 'button_press') {\n        this.canvas.focus();\n        this.canvas_div.focus();\n    }\n\n    // from https://stackoverflow.com/q/1114465\n    var boundingRect = this.canvas.getBoundingClientRect();\n    var x = (event.clientX - boundingRect.left) * this.ratio;\n    var y = (event.clientY - boundingRect.top) * this.ratio;\n\n    this.send_message(name, {\n        x: x,\n        y: y,\n        button: event.button,\n        step: event.step,\n        modifiers: getModifiers(event),\n        guiEvent: simpleKeys(event),\n    });\n\n    return false;\n};\n\nmpl.figure.prototype._key_event_extra = function (_event, _name) {\n    // Handle any extra behaviour associated with a key event\n};\n\nmpl.figure.prototype.key_event = function (event, name) {\n    // Prevent repeat events\n    if (name === 'key_press') {\n        if (event.key === this._key) {\n            return;\n        } else {\n            this._key = event.key;\n        }\n    }\n    if (name === 'key_release') {\n        this._key = null;\n    }\n\n    var value = '';\n    if (event.ctrlKey && event.key !== 'Control') {\n        value += 'ctrl+';\n    }\n    else if (event.altKey && event.key !== 'Alt') {\n        value += 'alt+';\n    }\n    else if (event.shiftKey && event.key !== 'Shift') {\n        value += 'shift+';\n    }\n\n    value += 'k' + event.key;\n\n    this._key_event_extra(event, name);\n\n    this.send_message(name, { key: value, guiEvent: simpleKeys(event) });\n    return false;\n};\n\nmpl.figure.prototype.toolbar_button_onclick = function (name) {\n    if (name === 'download') {\n        this.handle_save(this, null);\n    } else {\n        this.send_message('toolbar_button', { name: name });\n    }\n};\n\nmpl.figure.prototype.toolbar_button_onmouseover = function (tooltip) {\n    this.message.textContent = tooltip;\n};\n\n///////////////// REMAINING CONTENT GENERATED BY embed_js.py /////////////////\n// prettier-ignore\nvar _JSXTOOLS_RESIZE_OBSERVER=function(A){var t,i=new WeakMap,n=new WeakMap,a=new WeakMap,r=new WeakMap,o=new Set;function s(e){if(!(this instanceof s))throw new TypeError(\"Constructor requires 'new' operator\");i.set(this,e)}function h(){throw new TypeError(\"Function is not a constructor\")}function c(e,t,i,n){e=0 in arguments?Number(arguments[0]):0,t=1 in arguments?Number(arguments[1]):0,i=2 in arguments?Number(arguments[2]):0,n=3 in arguments?Number(arguments[3]):0,this.right=(this.x=this.left=e)+(this.width=i),this.bottom=(this.y=this.top=t)+(this.height=n),Object.freeze(this)}function d(){t=requestAnimationFrame(d);var s=new WeakMap,p=new Set;o.forEach((function(t){r.get(t).forEach((function(i){var r=t instanceof window.SVGElement,o=a.get(t),d=r?0:parseFloat(o.paddingTop),f=r?0:parseFloat(o.paddingRight),l=r?0:parseFloat(o.paddingBottom),u=r?0:parseFloat(o.paddingLeft),g=r?0:parseFloat(o.borderTopWidth),m=r?0:parseFloat(o.borderRightWidth),w=r?0:parseFloat(o.borderBottomWidth),b=u+f,F=d+l,v=(r?0:parseFloat(o.borderLeftWidth))+m,W=g+w,y=r?0:t.offsetHeight-W-t.clientHeight,E=r?0:t.offsetWidth-v-t.clientWidth,R=b+v,z=F+W,M=r?t.width:parseFloat(o.width)-R-E,O=r?t.height:parseFloat(o.height)-z-y;if(n.has(t)){var k=n.get(t);if(k[0]===M&&k[1]===O)return}n.set(t,[M,O]);var S=Object.create(h.prototype);S.target=t,S.contentRect=new c(u,d,M,O),s.has(i)||(s.set(i,[]),p.add(i)),s.get(i).push(S)}))})),p.forEach((function(e){i.get(e).call(e,s.get(e),e)}))}return s.prototype.observe=function(i){if(i instanceof window.Element){r.has(i)||(r.set(i,new Set),o.add(i),a.set(i,window.getComputedStyle(i)));var n=r.get(i);n.has(this)||n.add(this),cancelAnimationFrame(t),t=requestAnimationFrame(d)}},s.prototype.unobserve=function(i){if(i instanceof window.Element&&r.has(i)){var n=r.get(i);n.has(this)&&(n.delete(this),n.size||(r.delete(i),o.delete(i))),n.size||r.delete(i),o.size||cancelAnimationFrame(t)}},A.DOMRectReadOnly=c,A.ResizeObserver=s,A.ResizeObserverEntry=h,A}; // eslint-disable-line\nmpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Left button pans, Right button zooms\\nx/y fixes axis, CTRL fixes aspect\", \"fa fa-arrows\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\\nx/y fixes axis\", \"fa fa-square-o\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o\", \"download\"]];\n\nmpl.extensions = [\"eps\", \"jpeg\", \"pgf\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\", \"webp\"];\n\nmpl.default_extension = \"png\";/* global mpl */\n\nvar comm_websocket_adapter = function (comm) {\n    // Create a \"websocket\"-like object which calls the given IPython comm\n    // object with the appropriate methods. Currently this is a non binary\n    // socket, so there is still some room for performance tuning.\n    var ws = {};\n\n    ws.binaryType = comm.kernel.ws.binaryType;\n    ws.readyState = comm.kernel.ws.readyState;\n    function updateReadyState(_event) {\n        if (comm.kernel.ws) {\n            ws.readyState = comm.kernel.ws.readyState;\n        } else {\n            ws.readyState = 3; // Closed state.\n        }\n    }\n    comm.kernel.ws.addEventListener('open', updateReadyState);\n    comm.kernel.ws.addEventListener('close', updateReadyState);\n    comm.kernel.ws.addEventListener('error', updateReadyState);\n\n    ws.close = function () {\n        comm.close();\n    };\n    ws.send = function (m) {\n        //console.log('sending', m);\n        comm.send(m);\n    };\n    // Register the callback with on_msg.\n    comm.on_msg(function (msg) {\n        //console.log('receiving', msg['content']['data'], msg);\n        var data = msg['content']['data'];\n        if (data['blob'] !== undefined) {\n            data = {\n                data: new Blob(msg['buffers'], { type: data['blob'] }),\n            };\n        }\n        // Pass the mpl event to the overridden (by mpl) onmessage function.\n        ws.onmessage(data);\n    });\n    return ws;\n};\n\nmpl.mpl_figure_comm = function (comm, msg) {\n    // This is the function which gets called when the mpl process\n    // starts-up an IPython Comm through the \"matplotlib\" channel.\n\n    var id = msg.content.data.id;\n    // Get hold of the div created by the display call when the Comm\n    // socket was opened in Python.\n    var element = document.getElementById(id);\n    var ws_proxy = comm_websocket_adapter(comm);\n\n    function ondownload(figure, _format) {\n        window.open(figure.canvas.toDataURL());\n    }\n\n    var fig = new mpl.figure(id, ws_proxy, ondownload, element);\n\n    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n    // web socket which is closed, not our websocket->open comm proxy.\n    ws_proxy.onopen();\n\n    fig.parent_element = element;\n    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n    if (!fig.cell_info) {\n        console.error('Failed to find cell for figure', id, fig);\n        return;\n    }\n    fig.cell_info[0].output_area.element.on(\n        'cleared',\n        { fig: fig },\n        fig._remove_fig_handler\n    );\n};\n\nmpl.figure.prototype.handle_close = function (fig, msg) {\n    var width = fig.canvas.width / fig.ratio;\n    fig.cell_info[0].output_area.element.off(\n        'cleared',\n        fig._remove_fig_handler\n    );\n    fig.resizeObserverInstance.unobserve(fig.canvas_div);\n\n    // Update the output cell to use the data from the current canvas.\n    fig.push_to_output();\n    var dataURL = fig.canvas.toDataURL();\n    // Re-enable the keyboard manager in IPython - without this line, in FF,\n    // the notebook keyboard shortcuts fail.\n    IPython.keyboard_manager.enable();\n    fig.parent_element.innerHTML =\n        '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n    fig.close_ws(fig, msg);\n};\n\nmpl.figure.prototype.close_ws = function (fig, msg) {\n    fig.send_message('closing', msg);\n    // fig.ws.close()\n};\n\nmpl.figure.prototype.push_to_output = function (_remove_interactive) {\n    // Turn the data on the canvas into data in the output cell.\n    var width = this.canvas.width / this.ratio;\n    var dataURL = this.canvas.toDataURL();\n    this.cell_info[1]['text/html'] =\n        '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n};\n\nmpl.figure.prototype.updated_canvas_event = function () {\n    // Tell IPython that the notebook contents must change.\n    IPython.notebook.set_dirty(true);\n    this.send_message('ack', {});\n    var fig = this;\n    // Wait a second, then push the new image to the DOM so\n    // that it is saved nicely (might be nice to debounce this).\n    setTimeout(function () {\n        fig.push_to_output();\n    }, 1000);\n};\n\nmpl.figure.prototype._init_toolbar = function () {\n    var fig = this;\n\n    var toolbar = document.createElement('div');\n    toolbar.classList = 'btn-toolbar';\n    this.root.appendChild(toolbar);\n\n    function on_click_closure(name) {\n        return function (_event) {\n            return fig.toolbar_button_onclick(name);\n        };\n    }\n\n    function on_mouseover_closure(tooltip) {\n        return function (event) {\n            if (!event.currentTarget.disabled) {\n                return fig.toolbar_button_onmouseover(tooltip);\n            }\n        };\n    }\n\n    fig.buttons = {};\n    var buttonGroup = document.createElement('div');\n    buttonGroup.classList = 'btn-group';\n    var button;\n    for (var toolbar_ind in mpl.toolbar_items) {\n        var name = mpl.toolbar_items[toolbar_ind][0];\n        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n        var image = mpl.toolbar_items[toolbar_ind][2];\n        var method_name = mpl.toolbar_items[toolbar_ind][3];\n\n        if (!name) {\n            /* Instead of a spacer, we start a new button group. */\n            if (buttonGroup.hasChildNodes()) {\n                toolbar.appendChild(buttonGroup);\n            }\n            buttonGroup = document.createElement('div');\n            buttonGroup.classList = 'btn-group';\n            continue;\n        }\n\n        button = fig.buttons[name] = document.createElement('button');\n        button.classList = 'btn btn-default';\n        button.href = '#';\n        button.title = name;\n        button.innerHTML = '<i class=\"fa ' + image + ' fa-lg\"></i>';\n        button.addEventListener('click', on_click_closure(method_name));\n        button.addEventListener('mouseover', on_mouseover_closure(tooltip));\n        buttonGroup.appendChild(button);\n    }\n\n    if (buttonGroup.hasChildNodes()) {\n        toolbar.appendChild(buttonGroup);\n    }\n\n    // Add the status bar.\n    var status_bar = document.createElement('span');\n    status_bar.classList = 'mpl-message pull-right';\n    toolbar.appendChild(status_bar);\n    this.message = status_bar;\n\n    // Add the close button to the window.\n    var buttongrp = document.createElement('div');\n    buttongrp.classList = 'btn-group inline pull-right';\n    button = document.createElement('button');\n    button.classList = 'btn btn-mini btn-primary';\n    button.href = '#';\n    button.title = 'Stop Interaction';\n    button.innerHTML = '<i class=\"fa fa-power-off icon-remove icon-large\"></i>';\n    button.addEventListener('click', function (_evt) {\n        fig.handle_close(fig, {});\n    });\n    button.addEventListener(\n        'mouseover',\n        on_mouseover_closure('Stop Interaction')\n    );\n    buttongrp.appendChild(button);\n    var titlebar = this.root.querySelector('.ui-dialog-titlebar');\n    titlebar.insertBefore(buttongrp, titlebar.firstChild);\n};\n\nmpl.figure.prototype._remove_fig_handler = function (event) {\n    var fig = event.data.fig;\n    if (event.target !== this) {\n        // Ignore bubbled events from children.\n        return;\n    }\n    fig.close_ws(fig, {});\n};\n\nmpl.figure.prototype._root_extra_style = function (el) {\n    el.style.boxSizing = 'content-box'; // override notebook setting of border-box.\n};\n\nmpl.figure.prototype._canvas_extra_style = function (el) {\n    // this is important to make the div 'focusable\n    el.setAttribute('tabindex', 0);\n    // reach out to IPython and tell the keyboard manager to turn it's self\n    // off when our div gets focus\n\n    // location in version 3\n    if (IPython.notebook.keyboard_manager) {\n        IPython.notebook.keyboard_manager.register_events(el);\n    } else {\n        // location in version 2\n        IPython.keyboard_manager.register_events(el);\n    }\n};\n\nmpl.figure.prototype._key_event_extra = function (event, _name) {\n    // Check for shift+enter\n    if (event.shiftKey && event.which === 13) {\n        this.canvas_div.blur();\n        // select the cell after this one\n        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n        IPython.notebook.select(index + 1);\n    }\n};\n\nmpl.figure.prototype.handle_save = function (fig, _msg) {\n    fig.ondownload(fig, null);\n};\n\nmpl.find_output_cell = function (html_output) {\n    // Return the cell and output element which can be found *uniquely* in the notebook.\n    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n    // IPython event is triggered only after the cells have been serialised, which for\n    // our purposes (turning an active figure into a static one), is too late.\n    var cells = IPython.notebook.get_cells();\n    var ncells = cells.length;\n    for (var i = 0; i < ncells; i++) {\n        var cell = cells[i];\n        if (cell.cell_type === 'code') {\n            for (var j = 0; j < cell.output_area.outputs.length; j++) {\n                var data = cell.output_area.outputs[j];\n                if (data.data) {\n                    // IPython >= 3 moved mimebundle to data attribute of output\n                    data = data.data;\n                }\n                if (data['text/html'] === html_output) {\n                    return [cell, data, j];\n                }\n            }\n        }\n    }\n};\n\n// Register the function which deals with the matplotlib target/channel.\n// The kernel may be null if the page has been refreshed.\nif (IPython.notebook.kernel !== null) {\n    IPython.notebook.kernel.comm_manager.register_target(\n        'matplotlib',\n        mpl.mpl_figure_comm\n    );\n}\n",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div id='db3a2200-f570-4ef1-9712-4f567e074f10'></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/tawab/Desktop/columbia/Courses/Spring2024/HPML/Project/Analog_NAS/trainingLogs/render/latent_space_20240509_085333.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e14a8cfefad4884b0e0b0f7714ac56f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.670 MB of 0.670 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>global_step</td><td>▁</td></tr><tr><td>rollout/ep_len_mean</td><td>▁</td></tr><tr><td>rollout/ep_rew_mean</td><td>▁</td></tr><tr><td>time/fps</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>global_step</td><td>512</td></tr><tr><td>rollout/ep_len_mean</td><td>113.5</td></tr><tr><td>rollout/ep_rew_mean</td><td>0.77255</td></tr><tr><td>time/fps</td><td>249.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">olive-puddle-46</strong> at: <a href='https://wandb.ai/trex-ai/PPO%20Training/runs/dhdp5day' target=\"_blank\">https://wandb.ai/trex-ai/PPO%20Training/runs/dhdp5day</a><br/> View project at: <a href='https://wandb.ai/trex-ai/PPO%20Training' target=\"_blank\">https://wandb.ai/trex-ai/PPO%20Training</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240509_085324-dhdp5day/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train(custom_callback=custom_callback_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "save_path = \"/Users/tawab/Desktop/columbia/Courses/Spring2024/HPML/Project/Analog_NAS/models/ppo_mlpPolicy_1500000.zip\"\n",
    "trainer.load_model(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0\n",
      "Episode 0,200: cum reward: 0.7686756253242493, max reward: 0.7686756253242493, action: [71], last action: [71]\n",
      "Episode 0: Accuracy: 0.7686756253242493 Max Accuracy till Episode: 0.7686756253242493\n",
      "Episode 1\n",
      "Episode 1,200: cum reward: 0.9201174378395081, max reward: 0.9238914251327515, action: [23], last action: [49]\n",
      "Episode 1: Accuracy: 0.9238914251327515 Max Accuracy till Episode: 0.9238914251327515\n",
      "Episode 2\n",
      "Episode 2,4: cum reward: 0.7723342180252075, max reward: 0.7723342180252075, action: [103], last action: [112]\n",
      "Episode 2: Accuracy: 0.7723342180252075 Max Accuracy till Episode: 0.9238914251327515\n",
      "Episode 3\n",
      "Episode 3,200: cum reward: 0.757368803024292, max reward: 0.7575494050979614, action: [8], last action: [96]\n",
      "Episode 3: Accuracy: 0.7575494050979614 Max Accuracy till Episode: 0.9238914251327515\n",
      "Episode 4\n",
      "Episode 4,200: cum reward: 0.7889947295188904, max reward: 0.7889947295188904, action: [68], last action: [68]\n",
      "Episode 4: Accuracy: 0.7889947295188904 Max Accuracy till Episode: 0.9238914251327515\n",
      "Episode 5\n",
      "Episode 5,200: cum reward: 0.7686992287635803, max reward: 0.7744229435920715, action: [71], last action: [75]\n",
      "Episode 5: Accuracy: 0.7744229435920715 Max Accuracy till Episode: 0.9238914251327515\n",
      "Episode 6\n",
      "Episode 6,200: cum reward: 0.9238914251327515, max reward: 0.9238914251327515, action: [22], last action: [109]\n",
      "Episode 6: Accuracy: 0.9238914251327515 Max Accuracy till Episode: 0.9238914251327515\n",
      "Episode 7\n",
      "Episode 7,200: cum reward: 0.7404413819313049, max reward: 0.7404413819313049, action: [109], last action: [109]\n",
      "Episode 7: Accuracy: 0.7404413819313049 Max Accuracy till Episode: 0.9238914251327515\n",
      "Episode 8\n",
      "Episode 8,200: cum reward: 0.770458459854126, max reward: 0.770458459854126, action: [40], last action: [40]\n",
      "Episode 8: Accuracy: 0.770458459854126 Max Accuracy till Episode: 0.9238914251327515\n",
      "Episode 9\n",
      "Episode 9,200: cum reward: 0.9304686188697815, max reward: 0.9318768382072449, action: [79], last action: [49]\n",
      "Episode 9: Accuracy: 0.9318768382072449 Max Accuracy till Episode: 0.9318768382072449\n",
      "Average Accuracy: 0.8152536153793335\n"
     ]
    }
   ],
   "source": [
    "x = trainer.evaluate_accuracy(num_episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93187684"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.calculate_accuracy_for_decoded_state(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
