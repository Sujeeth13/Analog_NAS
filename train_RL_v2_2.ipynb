{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%matplotlib notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from env.SurrogateModel import SurrogateModel\n",
    "from env.Decoder import Decoder\n",
    "from env.VQVAE_environment import VQVAE_Env, RenderCallback\n",
    "from env.RLTrainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "surrogate_model = '/Users/tawab/Desktop/columbia/Courses/Spring2024/HPML/Project/Analog_NAS/env/models/surrogate_model.json'\n",
    "codebook = '/Users/tawab/Desktop/columbia/Courses/Spring2024/HPML/Project/Analog_NAS/env/models/codebook.pth'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_config = {\n",
    "    \"out_dim\": 22,           # Output dimension\n",
    "    \"embed_dim\": 8,          # Embedding dimension\n",
    "    \"h_nodes\": 512,          # Number of hidden nodes\n",
    "    \"dropout\": 0.2,          # Dropout rate\n",
    "    \"scale\": 2,              # Scale factor\n",
    "    \"num_layers\": 5,         # Number of layers\n",
    "    \"load_path\": '/Users/tawab/Desktop/columbia/Courses/Spring2024/HPML/Project/Analog_NAS/env/models/decoder_model.pth', # Path to load model weights\n",
    "}\n",
    "\n",
    "env_config = {\n",
    "    \"embed_dim\": decoder_config['embed_dim'],    # Embedding dimension\n",
    "    \"num_embeddings\": 14,           # Number of embeddings\n",
    "    \"max_allowed_actions\": 200,      # Maximum allowed actions\n",
    "    \"consider_previous_actions\": False, # Consider previous actions\n",
    "    \"num_previous_actions\": 12,       # Number of previous actions to consider  \n",
    "    \"render_mode\": 'human',          # Render mode\n",
    "    \"render_data\": '/Users/tawab/Desktop/columbia/Courses/Spring2024/HPML/Project/Analog_NAS/env/render/architectures_trained_on.npy',  # Data for rendering\n",
    "    \"render_labels\": '/Users/tawab/Desktop/columbia/Courses/Spring2024/HPML/Project/Analog_NAS/env/render/labels.npy',   # Labels for rendering\n",
    "    \"render_log_dir\": 'trainingLogs',                  # Directory for logging data\n",
    "}\n",
    "\n",
    "model_config = {\n",
    "    \"model\": \"PPO\",                # Model type ('PPO', 'A2C', 'DQN', etc.)\n",
    "    \"policy\": 'MLPPolicy',  # Policy type\n",
    "    \"total_timesteps\": 1500000,       # Total number of timesteps\n",
    "    \"verbose\": 0,                  # Verbosity level\n",
    "    \"tensorboard_log\": env_config['render_log_dir'],  # Tensorboard log directory\n",
    "    \"n_steps\": 2048,               # Number of steps to run for each environment per update\n",
    "    \"progress_bar\": False,          # Whether to display a progress bar\n",
    "    \"n_epochs\": 10,                # Number of epochs\n",
    "    \"batch_size\": 64,              # Batch size\n",
    "}\n",
    "\n",
    "log_config = {\n",
    "    \"project\": 'PPO Training',                          # Project name in wandb\n",
    "    #\"entity\": 'trex-ai',                            # Entity name in wandb\n",
    "    \"sync_tensorboard\": True,                           # Whether to sync TensorBoard\n",
    "    \"save_code\": True,                                  # Whether to save code in wandb\n",
    "    \"model_save_path\": env_config['render_log_dir'],    # Path to save the model\n",
    "    \"gradient_save_freq\": 100,                          # Frequency to save gradients\n",
    "    \"verbose\": 2,                                       # Verbosity level\n",
    "}\n",
    "\n",
    "# Example of initializing the Trainer class with these configurations\n",
    "# trainer = Trainer(\n",
    "#     surrogate_path=\"path_to_surrogate_model.pt\",\n",
    "#     codebook_path=\"path_to_codebook.pt\",\n",
    "#     decoder_config=decoder_config,\n",
    "#     env_config=env_config,\n",
    "#     model_config=model_config,\n",
    "#     log_config=log_config\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surrogate model loaded from:  /Users/tawab/Desktop/columbia/Courses/Spring2024/HPML/Project/Analog_NAS/env/models/surrogate_model.json\n",
      "Codebook loaded from:  /Users/tawab/Desktop/columbia/Courses/Spring2024/HPML/Project/Analog_NAS/env/models/codebook.pth\n",
      "Decoder model loaded from:  /Users/tawab/Desktop/columbia/Courses/Spring2024/HPML/Project/Analog_NAS/env/models/decoder_model.pth\n",
      "Environment check passed\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(surrogate_path=surrogate_model, \n",
    "                  codebook_path=codebook, \n",
    "                  decoder_config=decoder_config, \n",
    "                  env_config=env_config, \n",
    "                  model_config=model_config, \n",
    "                  log_config=log_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.load_model('/Users/tawab/Desktop/columbia/Courses/Spring2024/HPML/Project/Analog_NAS/models/ppo_mlpPolicy_1500000.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7744229435920715 for episode 0\n",
      "Accuracy: 0.9243929982185364 for episode 1\n",
      "Accuracy: 0.9238914251327515 for episode 2\n",
      "Accuracy: 0.7723342180252075 for episode 3\n",
      "Accuracy: 0.7889947295188904 for episode 4\n",
      "Accuracy: 0.7889947295188904 for episode 5\n",
      "Accuracy: 0.7535715699195862 for episode 6\n",
      "Accuracy: 0.9272683262825012 for episode 7\n",
      "Accuracy: 0.7889947295188904 for episode 8\n",
      "Accuracy: 0.7348170280456543 for episode 9\n",
      "Accuracy: 0.9238914251327515 for episode 10\n",
      "Accuracy: 0.7513466477394104 for episode 11\n",
      "Accuracy: 0.9369305968284607 for episode 12\n",
      "Accuracy: 0.7348170280456543 for episode 13\n",
      "Accuracy: 0.7376570105552673 for episode 14\n",
      "Accuracy: 0.7348170280456543 for episode 15\n",
      "Accuracy: 0.925092875957489 for episode 16\n",
      "Accuracy: 0.770458459854126 for episode 17\n",
      "Accuracy: 0.7686992287635803 for episode 18\n",
      "Accuracy: 0.770458459854126 for episode 19\n",
      "Accuracy: 0.7348170280456543 for episode 20\n",
      "Accuracy: 0.7692904472351074 for episode 21\n",
      "Accuracy: 0.9369305968284607 for episode 22\n",
      "Accuracy: 0.7889947295188904 for episode 23\n",
      "Accuracy: 0.7889947295188904 for episode 24\n",
      "Accuracy: 0.9417205452919006 for episode 25\n",
      "Accuracy: 0.7535715699195862 for episode 26\n",
      "Accuracy: 0.7348170280456543 for episode 27\n",
      "Accuracy: 0.7889947295188904 for episode 28\n",
      "Accuracy: 0.7889947295188904 for episode 29\n",
      "Accuracy: 0.944547712802887 for episode 30\n",
      "Accuracy: 0.770458459854126 for episode 31\n",
      "Accuracy: 0.7686756253242493 for episode 32\n",
      "Accuracy: 0.9238914251327515 for episode 33\n",
      "Accuracy: 0.7889947295188904 for episode 34\n",
      "Accuracy: 0.9319303035736084 for episode 35\n",
      "Accuracy: 0.9281994104385376 for episode 36\n",
      "Accuracy: 0.7551933526992798 for episode 37\n",
      "Accuracy: 0.9353243112564087 for episode 38\n",
      "Accuracy: 0.7518733143806458 for episode 39\n",
      "Accuracy: 0.7889947295188904 for episode 40\n",
      "Accuracy: 0.7889947295188904 for episode 41\n",
      "Accuracy: 0.7889947295188904 for episode 42\n",
      "Accuracy: 0.9325972199440002 for episode 43\n",
      "Accuracy: 0.9243929982185364 for episode 44\n",
      "Accuracy: 0.7889947295188904 for episode 45\n",
      "Accuracy: 0.770458459854126 for episode 46\n",
      "Accuracy: 0.7889947295188904 for episode 47\n",
      "Accuracy: 0.9417205452919006 for episode 48\n",
      "Accuracy: 0.9441887140274048 for episode 49\n",
      "Accuracy: 0.928982675075531 for episode 50\n",
      "Accuracy: 0.7725493907928467 for episode 51\n",
      "Accuracy: 0.7725493907928467 for episode 52\n",
      "Accuracy: 0.9438520669937134 for episode 53\n",
      "Accuracy: 0.7889947295188904 for episode 54\n",
      "Accuracy: 0.770458459854126 for episode 55\n",
      "Accuracy: 0.7889947295188904 for episode 56\n",
      "Accuracy: 0.7725493907928467 for episode 57\n",
      "Accuracy: 0.7372422218322754 for episode 58\n",
      "Accuracy: 0.931036651134491 for episode 59\n",
      "Accuracy: 0.7889947295188904 for episode 60\n",
      "Accuracy: 0.7723342180252075 for episode 61\n",
      "Accuracy: 0.7889947295188904 for episode 62\n",
      "Accuracy: 0.7889947295188904 for episode 63\n",
      "Accuracy: 0.7889947295188904 for episode 64\n",
      "Accuracy: 0.770458459854126 for episode 65\n",
      "Accuracy: 0.7513466477394104 for episode 66\n",
      "Accuracy: 0.7644068598747253 for episode 67\n",
      "Accuracy: 0.925092875957489 for episode 68\n",
      "Accuracy: 0.9233819842338562 for episode 69\n",
      "Accuracy: 0.7839134335517883 for episode 70\n",
      "Accuracy: 0.9238914251327515 for episode 71\n",
      "Accuracy: 0.9281994104385376 for episode 72\n",
      "Accuracy: 0.7753796577453613 for episode 73\n",
      "Accuracy: 0.7889947295188904 for episode 74\n",
      "Accuracy: 0.7889947295188904 for episode 75\n",
      "Accuracy: 0.9285473227500916 for episode 76\n",
      "Accuracy: 0.7889947295188904 for episode 77\n",
      "Accuracy: 0.939794659614563 for episode 78\n",
      "Accuracy: 0.759606122970581 for episode 79\n",
      "Accuracy: 0.9218910336494446 for episode 80\n",
      "Accuracy: 0.7889947295188904 for episode 81\n",
      "Accuracy: 0.7575494050979614 for episode 82\n",
      "Accuracy: 0.7889947295188904 for episode 83\n",
      "Accuracy: 0.7725493907928467 for episode 84\n",
      "Accuracy: 0.7889947295188904 for episode 85\n",
      "Accuracy: 0.7771551012992859 for episode 86\n",
      "Accuracy: 0.7889947295188904 for episode 87\n",
      "Accuracy: 0.7692904472351074 for episode 88\n",
      "Accuracy: 0.759606122970581 for episode 89\n",
      "Accuracy: 0.7889947295188904 for episode 90\n",
      "Accuracy: 0.9322266578674316 for episode 91\n",
      "Accuracy: 0.7839134335517883 for episode 92\n",
      "Accuracy: 0.7372422218322754 for episode 93\n",
      "Accuracy: 0.9441887140274048 for episode 94\n",
      "Accuracy: 0.7466604113578796 for episode 95\n",
      "Accuracy: 0.7744229435920715 for episode 96\n",
      "Accuracy: 0.9373398423194885 for episode 97\n",
      "Accuracy: 0.770458459854126 for episode 98\n",
      "Accuracy: 0.7349362373352051 for episode 99\n",
      "Accuracy: 0.9141677618026733 for episode 100\n",
      "Accuracy: 0.7889947295188904 for episode 101\n",
      "Accuracy: 0.770458459854126 for episode 102\n",
      "Accuracy: 0.7889947295188904 for episode 103\n",
      "Accuracy: 0.9202268123626709 for episode 104\n",
      "Accuracy: 0.7889947295188904 for episode 105\n",
      "Accuracy: 0.7889947295188904 for episode 106\n",
      "Accuracy: 0.9218910336494446 for episode 107\n",
      "Accuracy: 0.9252868294715881 for episode 108\n",
      "Accuracy: 0.770458459854126 for episode 109\n",
      "Accuracy: 0.9243929982185364 for episode 110\n",
      "Accuracy: 0.7584961652755737 for episode 111\n",
      "Accuracy: 0.9281994104385376 for episode 112\n",
      "Accuracy: 0.7535715699195862 for episode 113\n",
      "Accuracy: 0.7889947295188904 for episode 114\n",
      "Accuracy: 0.7760769128799438 for episode 115\n",
      "Accuracy: 0.7889947295188904 for episode 116\n",
      "Accuracy: 0.7348170280456543 for episode 117\n",
      "Accuracy: 0.944547712802887 for episode 118\n",
      "Accuracy: 0.7606016993522644 for episode 119\n",
      "Accuracy: 0.7889947295188904 for episode 120\n",
      "Accuracy: 0.7889947295188904 for episode 121\n",
      "Accuracy: 0.7562592625617981 for episode 122\n",
      "Accuracy: 0.7375838756561279 for episode 123\n",
      "Accuracy: 0.819848895072937 for episode 124\n",
      "Accuracy: 0.7771551012992859 for episode 125\n",
      "Accuracy: 0.770458459854126 for episode 126\n",
      "Accuracy: 0.7839134335517883 for episode 127\n",
      "Accuracy: 0.9238914251327515 for episode 128\n",
      "Accuracy: 0.7889947295188904 for episode 129\n",
      "Accuracy: 0.7818193435668945 for episode 130\n",
      "Accuracy: 0.770458459854126 for episode 131\n",
      "Accuracy: 0.9227997064590454 for episode 132\n",
      "Accuracy: 0.7376570105552673 for episode 133\n",
      "Accuracy: 0.7599714398384094 for episode 134\n",
      "Accuracy: 0.7551933526992798 for episode 135\n",
      "Accuracy: 0.8088371157646179 for episode 136\n",
      "Accuracy: 0.7692904472351074 for episode 137\n",
      "Accuracy: 0.7575494050979614 for episode 138\n",
      "Accuracy: 0.7348170280456543 for episode 139\n",
      "Accuracy: 0.9261704683303833 for episode 140\n",
      "Accuracy: 0.9281994104385376 for episode 141\n",
      "Accuracy: 0.9212459325790405 for episode 142\n",
      "Accuracy: 0.7889947295188904 for episode 143\n",
      "Accuracy: 0.7839134335517883 for episode 144\n",
      "Accuracy: 0.7889947295188904 for episode 145\n",
      "Accuracy: 0.9155168533325195 for episode 146\n",
      "Accuracy: 0.9353243112564087 for episode 147\n",
      "Accuracy: 0.9272683262825012 for episode 148\n",
      "Accuracy: 0.7889947295188904 for episode 149\n",
      "Accuracy: 0.9344345927238464 for episode 150\n",
      "Accuracy: 0.9281994104385376 for episode 151\n",
      "Accuracy: 0.7889947295188904 for episode 152\n",
      "Accuracy: 0.7692904472351074 for episode 153\n",
      "Accuracy: 0.7889947295188904 for episode 154\n",
      "Accuracy: 0.7889947295188904 for episode 155\n",
      "Accuracy: 0.7725493907928467 for episode 156\n",
      "Accuracy: 0.9322266578674316 for episode 157\n",
      "Accuracy: 0.7348170280456543 for episode 158\n",
      "Accuracy: 0.7551933526992798 for episode 159\n",
      "Accuracy: 0.9429131746292114 for episode 160\n",
      "Accuracy: 0.7518733143806458 for episode 161\n",
      "Accuracy: 0.7599714398384094 for episode 162\n",
      "Accuracy: 0.9227997064590454 for episode 163\n",
      "Accuracy: 0.925092875957489 for episode 164\n",
      "Accuracy: 0.7753796577453613 for episode 165\n",
      "Accuracy: 0.9322266578674316 for episode 166\n",
      "Accuracy: 0.770458459854126 for episode 167\n",
      "Accuracy: 0.925092875957489 for episode 168\n",
      "Accuracy: 0.927057147026062 for episode 169\n",
      "Accuracy: 0.9272683262825012 for episode 170\n",
      "Accuracy: 0.7839134335517883 for episode 171\n",
      "Accuracy: 0.7584961652755737 for episode 172\n",
      "Accuracy: 0.9281994104385376 for episode 173\n",
      "Accuracy: 0.798057496547699 for episode 174\n",
      "Accuracy: 0.7889947295188904 for episode 175\n",
      "Accuracy: 0.9319303035736084 for episode 176\n",
      "Accuracy: 0.7889947295188904 for episode 177\n",
      "Accuracy: 0.759606122970581 for episode 178\n",
      "Accuracy: 0.7889947295188904 for episode 179\n",
      "Accuracy: 0.7889947295188904 for episode 180\n",
      "Accuracy: 0.7562592625617981 for episode 181\n",
      "Accuracy: 0.7725493907928467 for episode 182\n",
      "Accuracy: 0.7744229435920715 for episode 183\n",
      "Accuracy: 0.7644068598747253 for episode 184\n",
      "Accuracy: 0.7686756253242493 for episode 185\n",
      "Accuracy: 0.7889947295188904 for episode 186\n",
      "Accuracy: 0.7889947295188904 for episode 187\n",
      "Accuracy: 0.7575494050979614 for episode 188\n",
      "Accuracy: 0.7753796577453613 for episode 189\n",
      "Accuracy: 0.7889947295188904 for episode 190\n",
      "Accuracy: 0.7889947295188904 for episode 191\n",
      "Accuracy: 0.7889947295188904 for episode 192\n",
      "Accuracy: 0.762614905834198 for episode 193\n",
      "Accuracy: 0.7889947295188904 for episode 194\n",
      "Accuracy: 0.7889947295188904 for episode 195\n",
      "Accuracy: 0.9373398423194885 for episode 196\n",
      "Accuracy: 0.770458459854126 for episode 197\n",
      "Accuracy: 0.9281994104385376 for episode 198\n",
      "Accuracy: 0.7889947295188904 for episode 199\n",
      "Accuracy: 0.798057496547699 for episode 200\n",
      "Accuracy: 0.9438520669937134 for episode 201\n",
      "Accuracy: 0.7889947295188904 for episode 202\n",
      "Accuracy: 0.7889947295188904 for episode 203\n",
      "Accuracy: 0.7692904472351074 for episode 204\n",
      "Accuracy: 0.7839134335517883 for episode 205\n",
      "Accuracy: 0.7889947295188904 for episode 206\n",
      "Accuracy: 0.819848895072937 for episode 207\n",
      "Accuracy: 0.7744229435920715 for episode 208\n",
      "Accuracy: 0.7466604113578796 for episode 209\n",
      "Accuracy: 0.7725493907928467 for episode 210\n",
      "Accuracy: 0.7889947295188904 for episode 211\n",
      "Accuracy: 0.7348170280456543 for episode 212\n",
      "Accuracy: 0.9202268123626709 for episode 213\n",
      "Accuracy: 0.7889947295188904 for episode 214\n",
      "Accuracy: 0.7889947295188904 for episode 215\n",
      "Accuracy: 0.9281994104385376 for episode 216\n",
      "Accuracy: 0.7376570105552673 for episode 217\n",
      "Accuracy: 0.759606122970581 for episode 218\n",
      "Accuracy: 0.7376570105552673 for episode 219\n",
      "Accuracy: 0.9238914251327515 for episode 220\n",
      "Accuracy: 0.7889947295188904 for episode 221\n",
      "Accuracy: 0.7889947295188904 for episode 222\n",
      "Accuracy: 0.7889947295188904 for episode 223\n",
      "Accuracy: 0.9291768670082092 for episode 224\n",
      "Accuracy: 0.7889947295188904 for episode 225\n",
      "Accuracy: 0.7889947295188904 for episode 226\n",
      "Accuracy: 0.7535715699195862 for episode 227\n",
      "Accuracy: 0.759606122970581 for episode 228\n",
      "Accuracy: 0.7889947295188904 for episode 229\n",
      "Accuracy: 0.7348170280456543 for episode 230\n",
      "Accuracy: 0.7889947295188904 for episode 231\n",
      "Accuracy: 0.9440740346908569 for episode 232\n",
      "Accuracy: 0.7734160423278809 for episode 233\n",
      "Accuracy: 0.9331715703010559 for episode 234\n",
      "Accuracy: 0.7889947295188904 for episode 235\n",
      "Accuracy: 0.7644068598747253 for episode 236\n",
      "Accuracy: 0.770458459854126 for episode 237\n",
      "Accuracy: 0.7889947295188904 for episode 238\n",
      "Accuracy: 0.7839134335517883 for episode 239\n",
      "Accuracy: 0.759606122970581 for episode 240\n",
      "Accuracy: 0.7372422218322754 for episode 241\n",
      "Accuracy: 0.9238914251327515 for episode 242\n",
      "Accuracy: 0.7889947295188904 for episode 243\n",
      "Accuracy: 0.923737645149231 for episode 244\n",
      "Accuracy: 0.7733031511306763 for episode 245\n",
      "Accuracy: 0.7889947295188904 for episode 246\n",
      "Accuracy: 0.7889947295188904 for episode 247\n",
      "Accuracy: 0.7348170280456543 for episode 248\n",
      "Accuracy: 0.7575494050979614 for episode 249\n",
      "Accuracy: 0.923686146736145 for episode 250\n",
      "Accuracy: 0.9342565536499023 for episode 251\n",
      "Accuracy: 0.7889947295188904 for episode 252\n",
      "Accuracy: 0.925092875957489 for episode 253\n",
      "Accuracy: 0.7889947295188904 for episode 254\n",
      "Accuracy: 0.7889947295188904 for episode 255\n",
      "Accuracy: 0.7839134335517883 for episode 256\n",
      "Accuracy: 0.7889947295188904 for episode 257\n",
      "Accuracy: 0.7584961652755737 for episode 258\n",
      "Accuracy: 0.7575494050979614 for episode 259\n",
      "Accuracy: 0.7889947295188904 for episode 260\n",
      "Accuracy: 0.7599714398384094 for episode 261\n",
      "Accuracy: 0.7348170280456543 for episode 262\n",
      "Accuracy: 0.7513466477394104 for episode 263\n",
      "Accuracy: 0.7753796577453613 for episode 264\n",
      "Accuracy: 0.7535715699195862 for episode 265\n",
      "Accuracy: 0.757368803024292 for episode 266\n",
      "Accuracy: 0.7725493907928467 for episode 267\n",
      "Accuracy: 0.9281994104385376 for episode 268\n",
      "Accuracy: 0.7404907941818237 for episode 269\n",
      "Accuracy: 0.9141677618026733 for episode 270\n",
      "Accuracy: 0.7348170280456543 for episode 271\n",
      "Accuracy: 0.7839134335517883 for episode 272\n",
      "Accuracy: 0.9281994104385376 for episode 273\n",
      "Accuracy: 0.7551933526992798 for episode 274\n",
      "Accuracy: 0.9227997064590454 for episode 275\n",
      "Accuracy: 0.7889947295188904 for episode 276\n",
      "Accuracy: 0.9238914251327515 for episode 277\n",
      "Accuracy: 0.9243929982185364 for episode 278\n",
      "Accuracy: 0.7575494050979614 for episode 279\n",
      "Accuracy: 0.7889947295188904 for episode 280\n",
      "Accuracy: 0.9369305968284607 for episode 281\n",
      "Accuracy: 0.7692904472351074 for episode 282\n",
      "Accuracy: 0.7725493907928467 for episode 283\n",
      "Accuracy: 0.7375838756561279 for episode 284\n",
      "Accuracy: 0.7372422218322754 for episode 285\n",
      "Accuracy: 0.7889947295188904 for episode 286\n",
      "Accuracy: 0.7599714398384094 for episode 287\n",
      "Accuracy: 0.7753796577453613 for episode 288\n",
      "Accuracy: 0.7889947295188904 for episode 289\n",
      "Accuracy: 0.9429131746292114 for episode 290\n",
      "Accuracy: 0.9552870988845825 for episode 291\n",
      "Accuracy: 0.9285473227500916 for episode 292\n",
      "Accuracy: 0.7839134335517883 for episode 293\n",
      "Accuracy: 0.7375838756561279 for episode 294\n",
      "Accuracy: 0.7753796577453613 for episode 295\n",
      "Accuracy: 0.7466604113578796 for episode 296\n",
      "Accuracy: 0.9201174378395081 for episode 297\n",
      "Accuracy: 0.923686146736145 for episode 298\n",
      "Accuracy: 0.7640172243118286 for episode 299\n",
      "Accuracy: 0.9325860738754272 for episode 300\n",
      "Accuracy: 0.759606122970581 for episode 301\n",
      "Accuracy: 0.7733031511306763 for episode 302\n",
      "Accuracy: 0.7692904472351074 for episode 303\n",
      "Accuracy: 0.7889947295188904 for episode 304\n",
      "Accuracy: 0.770458459854126 for episode 305\n",
      "Accuracy: 0.7772524356842041 for episode 306\n",
      "Accuracy: 0.7818193435668945 for episode 307\n",
      "Accuracy: 0.9318768382072449 for episode 308\n",
      "Accuracy: 0.7551933526992798 for episode 309\n",
      "Accuracy: 0.7889947295188904 for episode 310\n",
      "Accuracy: 0.9281994104385376 for episode 311\n",
      "Accuracy: 0.7535715699195862 for episode 312\n",
      "Accuracy: 0.770458459854126 for episode 313\n",
      "Accuracy: 0.7466604113578796 for episode 314\n",
      "Accuracy: 0.757368803024292 for episode 315\n",
      "Accuracy: 0.7692904472351074 for episode 316\n",
      "Accuracy: 0.9218910336494446 for episode 317\n",
      "Accuracy: 0.9141677618026733 for episode 318\n",
      "Accuracy: 0.7889947295188904 for episode 319\n",
      "Accuracy: 0.7744229435920715 for episode 320\n",
      "Accuracy: 0.7889947295188904 for episode 321\n",
      "Accuracy: 0.7889947295188904 for episode 322\n",
      "Accuracy: 0.9238914251327515 for episode 323\n",
      "Accuracy: 0.7348170280456543 for episode 324\n",
      "Accuracy: 0.9417205452919006 for episode 325\n",
      "Accuracy: 0.7518733143806458 for episode 326\n",
      "Accuracy: 0.759606122970581 for episode 327\n",
      "Accuracy: 0.8088371157646179 for episode 328\n",
      "Accuracy: 0.7889947295188904 for episode 329\n",
      "Accuracy: 0.7686992287635803 for episode 330\n",
      "Accuracy: 0.7889947295188904 for episode 331\n",
      "Accuracy: 0.9363871216773987 for episode 332\n",
      "Accuracy: 0.7889947295188904 for episode 333\n",
      "Accuracy: 0.7889947295188904 for episode 334\n",
      "Accuracy: 0.8088371157646179 for episode 335\n",
      "Accuracy: 0.931036651134491 for episode 336\n",
      "Accuracy: 0.7889947295188904 for episode 337\n",
      "Accuracy: 0.7889947295188904 for episode 338\n",
      "Accuracy: 0.9254611730575562 for episode 339\n",
      "Accuracy: 0.7889947295188904 for episode 340\n",
      "Accuracy: 0.7760769128799438 for episode 341\n",
      "Accuracy: 0.9243929982185364 for episode 342\n",
      "Accuracy: 0.7644068598747253 for episode 343\n",
      "Accuracy: 0.7551933526992798 for episode 344\n",
      "Accuracy: 0.757368803024292 for episode 345\n",
      "Accuracy: 0.7644068598747253 for episode 346\n",
      "Accuracy: 0.7889947295188904 for episode 347\n",
      "Accuracy: 0.9363871216773987 for episode 348\n",
      "Accuracy: 0.7889947295188904 for episode 349\n",
      "Accuracy: 0.7644068598747253 for episode 350\n",
      "Accuracy: 0.7889947295188904 for episode 351\n",
      "Accuracy: 0.7372422218322754 for episode 352\n",
      "Accuracy: 0.7348170280456543 for episode 353\n",
      "Accuracy: 0.7889947295188904 for episode 354\n",
      "Accuracy: 0.7889947295188904 for episode 355\n",
      "Accuracy: 0.7889947295188904 for episode 356\n",
      "Accuracy: 0.9238914251327515 for episode 357\n",
      "Accuracy: 0.7889947295188904 for episode 358\n",
      "Accuracy: 0.7348170280456543 for episode 359\n",
      "Accuracy: 0.7686992287635803 for episode 360\n",
      "Accuracy: 0.7575494050979614 for episode 361\n",
      "Accuracy: 0.770458459854126 for episode 362\n",
      "Accuracy: 0.923737645149231 for episode 363\n",
      "Accuracy: 0.7889947295188904 for episode 364\n",
      "Accuracy: 0.819848895072937 for episode 365\n",
      "Accuracy: 0.7404413819313049 for episode 366\n",
      "Accuracy: 0.9373398423194885 for episode 367\n",
      "Accuracy: 0.7744229435920715 for episode 368\n",
      "Accuracy: 0.7644068598747253 for episode 369\n",
      "Accuracy: 0.7889947295188904 for episode 370\n",
      "Accuracy: 0.770458459854126 for episode 371\n",
      "Accuracy: 0.7644068598747253 for episode 372\n",
      "Accuracy: 0.9304686188697815 for episode 373\n",
      "Accuracy: 0.7349362373352051 for episode 374\n",
      "Accuracy: 0.9318768382072449 for episode 375\n",
      "Accuracy: 0.7889947295188904 for episode 376\n",
      "Accuracy: 0.9141677618026733 for episode 377\n",
      "Accuracy: 0.9243929982185364 for episode 378\n",
      "Accuracy: 0.7889947295188904 for episode 379\n",
      "Accuracy: 0.7733031511306763 for episode 380\n",
      "Accuracy: 0.9325860738754272 for episode 381\n",
      "Accuracy: 0.7348170280456543 for episode 382\n",
      "Accuracy: 0.9285754561424255 for episode 383\n",
      "Accuracy: 0.7734160423278809 for episode 384\n",
      "Accuracy: 0.9438520669937134 for episode 385\n",
      "Accuracy: 0.7562592625617981 for episode 386\n",
      "Accuracy: 0.7889947295188904 for episode 387\n",
      "Accuracy: 0.7889947295188904 for episode 388\n",
      "Accuracy: 0.7889947295188904 for episode 389\n",
      "Accuracy: 0.9276752471923828 for episode 390\n",
      "Accuracy: 0.7349642515182495 for episode 391\n",
      "Accuracy: 0.7575494050979614 for episode 392\n",
      "Accuracy: 0.7404413819313049 for episode 393\n",
      "Accuracy: 0.7518733143806458 for episode 394\n",
      "Accuracy: 0.9226190447807312 for episode 395\n",
      "Accuracy: 0.925092875957489 for episode 396\n",
      "Accuracy: 0.7575494050979614 for episode 397\n",
      "Accuracy: 0.7772524356842041 for episode 398\n",
      "Accuracy: 0.7889947295188904 for episode 399\n",
      "Accuracy: 0.7889947295188904 for episode 400\n",
      "Accuracy: 0.7686992287635803 for episode 401\n",
      "Accuracy: 0.7753796577453613 for episode 402\n",
      "Accuracy: 0.9244470596313477 for episode 403\n",
      "Accuracy: 0.770458459854126 for episode 404\n",
      "Accuracy: 0.7644068598747253 for episode 405\n",
      "Accuracy: 0.759606122970581 for episode 406\n",
      "Accuracy: 0.9281994104385376 for episode 407\n",
      "Accuracy: 0.7518733143806458 for episode 408\n",
      "Accuracy: 0.8088371157646179 for episode 409\n",
      "Accuracy: 0.759606122970581 for episode 410\n",
      "Accuracy: 0.7889947295188904 for episode 411\n",
      "Accuracy: 0.7889947295188904 for episode 412\n",
      "Accuracy: 0.9342565536499023 for episode 413\n",
      "Accuracy: 0.7889947295188904 for episode 414\n",
      "Accuracy: 0.9276752471923828 for episode 415\n",
      "Accuracy: 0.770458459854126 for episode 416\n",
      "Accuracy: 0.7889947295188904 for episode 417\n",
      "Accuracy: 0.7889947295188904 for episode 418\n",
      "Accuracy: 0.7753796577453613 for episode 419\n",
      "Accuracy: 0.7599714398384094 for episode 420\n",
      "Accuracy: 0.7889947295188904 for episode 421\n",
      "Accuracy: 0.7889947295188904 for episode 422\n",
      "Accuracy: 0.7466604113578796 for episode 423\n",
      "Accuracy: 0.7889947295188904 for episode 424\n",
      "Accuracy: 0.7889947295188904 for episode 425\n",
      "Accuracy: 0.7772524356842041 for episode 426\n",
      "Accuracy: 0.9243929982185364 for episode 427\n",
      "Accuracy: 0.7889947295188904 for episode 428\n",
      "Accuracy: 0.7348170280456543 for episode 429\n",
      "Accuracy: 0.9233819842338562 for episode 430\n",
      "Accuracy: 0.7889947295188904 for episode 431\n",
      "Accuracy: 0.7889947295188904 for episode 432\n",
      "Accuracy: 0.7733031511306763 for episode 433\n",
      "Accuracy: 0.7349642515182495 for episode 434\n",
      "Accuracy: 0.7889947295188904 for episode 435\n",
      "Accuracy: 0.7889947295188904 for episode 436\n",
      "Accuracy: 0.7889947295188904 for episode 437\n",
      "Accuracy: 0.7889947295188904 for episode 438\n",
      "Accuracy: 0.759606122970581 for episode 439\n",
      "Accuracy: 0.7723342180252075 for episode 440\n",
      "Accuracy: 0.770458459854126 for episode 441\n",
      "Accuracy: 0.9281994104385376 for episode 442\n",
      "Accuracy: 0.9281994104385376 for episode 443\n",
      "Accuracy: 0.7372422218322754 for episode 444\n",
      "Accuracy: 0.7644068598747253 for episode 445\n",
      "Accuracy: 0.7889947295188904 for episode 446\n",
      "Accuracy: 0.7348170280456543 for episode 447\n",
      "Accuracy: 0.7889947295188904 for episode 448\n",
      "Accuracy: 0.7889947295188904 for episode 449\n",
      "Accuracy: 0.8088371157646179 for episode 450\n",
      "Accuracy: 0.7889947295188904 for episode 451\n",
      "Accuracy: 0.7640172243118286 for episode 452\n",
      "Accuracy: 0.7889947295188904 for episode 453\n",
      "Accuracy: 0.7518733143806458 for episode 454\n",
      "Accuracy: 0.925092875957489 for episode 455\n",
      "Accuracy: 0.7889947295188904 for episode 456\n",
      "Accuracy: 0.7889947295188904 for episode 457\n",
      "Accuracy: 0.9272683262825012 for episode 458\n",
      "Accuracy: 0.7889947295188904 for episode 459\n",
      "Accuracy: 0.9440740346908569 for episode 460\n",
      "Accuracy: 0.7513466477394104 for episode 461\n",
      "Accuracy: 0.7535715699195862 for episode 462\n",
      "Accuracy: 0.7644068598747253 for episode 463\n",
      "Accuracy: 0.7376570105552673 for episode 464\n",
      "Accuracy: 0.931036651134491 for episode 465\n",
      "Accuracy: 0.7562592625617981 for episode 466\n",
      "Accuracy: 0.9281994104385376 for episode 467\n",
      "Accuracy: 0.7644068598747253 for episode 468\n",
      "Accuracy: 0.7889947295188904 for episode 469\n",
      "Accuracy: 0.9276752471923828 for episode 470\n",
      "Accuracy: 0.819848895072937 for episode 471\n",
      "Accuracy: 0.7839134335517883 for episode 472\n",
      "Accuracy: 0.7575494050979614 for episode 473\n",
      "Accuracy: 0.7889947295188904 for episode 474\n",
      "Accuracy: 0.7889947295188904 for episode 475\n",
      "Accuracy: 0.9441887140274048 for episode 476\n",
      "Accuracy: 0.7889947295188904 for episode 477\n",
      "Accuracy: 0.9342565536499023 for episode 478\n",
      "Accuracy: 0.7404413819313049 for episode 479\n",
      "Accuracy: 0.7889947295188904 for episode 480\n",
      "Accuracy: 0.9285473227500916 for episode 481\n",
      "Accuracy: 0.7889947295188904 for episode 482\n",
      "Accuracy: 0.7889947295188904 for episode 483\n",
      "Accuracy: 0.7889947295188904 for episode 484\n",
      "Accuracy: 0.7889947295188904 for episode 485\n",
      "Accuracy: 0.9243929982185364 for episode 486\n",
      "Accuracy: 0.7889947295188904 for episode 487\n",
      "Accuracy: 0.9322266578674316 for episode 488\n",
      "Accuracy: 0.7753796577453613 for episode 489\n",
      "Accuracy: 0.7562592625617981 for episode 490\n",
      "Accuracy: 0.7889947295188904 for episode 491\n",
      "Accuracy: 0.7348170280456543 for episode 492\n",
      "Accuracy: 0.7889947295188904 for episode 493\n",
      "Accuracy: 0.7376570105552673 for episode 494\n",
      "Accuracy: 0.7889947295188904 for episode 495\n",
      "Accuracy: 0.9438520669937134 for episode 496\n",
      "Accuracy: 0.7889947295188904 for episode 497\n",
      "Accuracy: 0.7692904472351074 for episode 498\n",
      "Accuracy: 0.7686992287635803 for episode 499\n",
      "Accuracy: 0.759606122970581 for episode 500\n",
      "Accuracy: 0.7889947295188904 for episode 501\n",
      "Accuracy: 0.7725493907928467 for episode 502\n",
      "Accuracy: 0.7535715699195862 for episode 503\n",
      "Accuracy: 0.770458459854126 for episode 504\n",
      "Accuracy: 0.7889947295188904 for episode 505\n",
      "Accuracy: 0.9448685050010681 for episode 506\n",
      "Accuracy: 0.931036651134491 for episode 507\n",
      "Accuracy: 0.770458459854126 for episode 508\n",
      "Accuracy: 0.7372422218322754 for episode 509\n",
      "Accuracy: 0.819848895072937 for episode 510\n",
      "Accuracy: 0.7889947295188904 for episode 511\n",
      "Accuracy: 0.9417205452919006 for episode 512\n",
      "Accuracy: 0.7889947295188904 for episode 513\n",
      "Accuracy: 0.7692904472351074 for episode 514\n",
      "Accuracy: 0.944547712802887 for episode 515\n",
      "Accuracy: 0.7686992287635803 for episode 516\n",
      "Accuracy: 0.7518733143806458 for episode 517\n",
      "Accuracy: 0.9319303035736084 for episode 518\n",
      "Accuracy: 0.7692904472351074 for episode 519\n",
      "Accuracy: 0.7889947295188904 for episode 520\n",
      "Accuracy: 0.7686992287635803 for episode 521\n",
      "Accuracy: 0.7889947295188904 for episode 522\n",
      "Accuracy: 0.7349362373352051 for episode 523\n",
      "Accuracy: 0.7889947295188904 for episode 524\n",
      "Accuracy: 0.7889947295188904 for episode 525\n",
      "Accuracy: 0.7725493907928467 for episode 526\n",
      "Accuracy: 0.7889947295188904 for episode 527\n",
      "Accuracy: 0.7889947295188904 for episode 528\n",
      "Accuracy: 0.7733031511306763 for episode 529\n",
      "Accuracy: 0.7551933526992798 for episode 530\n",
      "Accuracy: 0.7349642515182495 for episode 531\n",
      "Accuracy: 0.7562592625617981 for episode 532\n",
      "Accuracy: 0.798057496547699 for episode 533\n",
      "Accuracy: 0.7372422218322754 for episode 534\n",
      "Accuracy: 0.770458459854126 for episode 535\n",
      "Accuracy: 0.7889947295188904 for episode 536\n",
      "Accuracy: 0.7551933526992798 for episode 537\n",
      "Accuracy: 0.7686992287635803 for episode 538\n",
      "Accuracy: 0.9272683262825012 for episode 539\n",
      "Accuracy: 0.918295681476593 for episode 540\n",
      "Accuracy: 0.7372422218322754 for episode 541\n",
      "Accuracy: 0.7889947295188904 for episode 542\n",
      "Accuracy: 0.9285473227500916 for episode 543\n",
      "Accuracy: 0.7375838756561279 for episode 544\n",
      "Accuracy: 0.7889947295188904 for episode 545\n",
      "Accuracy: 0.7686992287635803 for episode 546\n",
      "Accuracy: 0.7671456336975098 for episode 547\n",
      "Accuracy: 0.9243929982185364 for episode 548\n",
      "Accuracy: 0.7535715699195862 for episode 549\n",
      "Accuracy: 0.7889947295188904 for episode 550\n",
      "Accuracy: 0.9407078623771667 for episode 551\n",
      "Accuracy: 0.9441887140274048 for episode 552\n",
      "Accuracy: 0.7889947295188904 for episode 553\n",
      "Accuracy: 0.9440740346908569 for episode 554\n",
      "Accuracy: 0.7686756253242493 for episode 555\n",
      "Accuracy: 0.9285473227500916 for episode 556\n",
      "Accuracy: 0.759606122970581 for episode 557\n",
      "Accuracy: 0.7889947295188904 for episode 558\n",
      "Accuracy: 0.7889947295188904 for episode 559\n",
      "Accuracy: 0.7889947295188904 for episode 560\n",
      "Accuracy: 0.7889947295188904 for episode 561\n",
      "Accuracy: 0.7839134335517883 for episode 562\n",
      "Accuracy: 0.7889947295188904 for episode 563\n",
      "Accuracy: 0.7889947295188904 for episode 564\n",
      "Accuracy: 0.7889947295188904 for episode 565\n",
      "Accuracy: 0.9201174378395081 for episode 566\n",
      "Accuracy: 0.9417205452919006 for episode 567\n",
      "Accuracy: 0.7348170280456543 for episode 568\n",
      "Accuracy: 0.9373398423194885 for episode 569\n",
      "Accuracy: 0.7372422218322754 for episode 570\n",
      "Accuracy: 0.7889947295188904 for episode 571\n",
      "Accuracy: 0.770458459854126 for episode 572\n",
      "Accuracy: 0.7599714398384094 for episode 573\n",
      "Accuracy: 0.7349362373352051 for episode 574\n",
      "Accuracy: 0.7575494050979614 for episode 575\n",
      "Accuracy: 0.9238914251327515 for episode 576\n",
      "Accuracy: 0.7771551012992859 for episode 577\n",
      "Accuracy: 0.7889947295188904 for episode 578\n",
      "Accuracy: 0.770458459854126 for episode 579\n",
      "Accuracy: 0.9272683262825012 for episode 580\n",
      "Accuracy: 0.925092875957489 for episode 581\n",
      "Accuracy: 0.7889947295188904 for episode 582\n",
      "Accuracy: 0.9243929982185364 for episode 583\n",
      "Accuracy: 0.7889947295188904 for episode 584\n",
      "Accuracy: 0.7772524356842041 for episode 585\n",
      "Accuracy: 0.7744229435920715 for episode 586\n",
      "Accuracy: 0.7744229435920715 for episode 587\n",
      "Accuracy: 0.7889947295188904 for episode 588\n",
      "Accuracy: 0.7348170280456543 for episode 589\n",
      "Accuracy: 0.7753796577453613 for episode 590\n",
      "Accuracy: 0.7889947295188904 for episode 591\n",
      "Accuracy: 0.9141677618026733 for episode 592\n",
      "Accuracy: 0.8088371157646179 for episode 593\n",
      "Accuracy: 0.944547712802887 for episode 594\n",
      "Accuracy: 0.7889947295188904 for episode 595\n",
      "Accuracy: 0.7889947295188904 for episode 596\n",
      "Accuracy: 0.770458459854126 for episode 597\n",
      "Accuracy: 0.7584961652755737 for episode 598\n",
      "Accuracy: 0.7889947295188904 for episode 599\n",
      "Accuracy: 0.9276752471923828 for episode 600\n",
      "Accuracy: 0.7644068598747253 for episode 601\n",
      "Accuracy: 0.7889947295188904 for episode 602\n",
      "Accuracy: 0.7348170280456543 for episode 603\n",
      "Accuracy: 0.759606122970581 for episode 604\n",
      "Accuracy: 0.7372422218322754 for episode 605\n",
      "Accuracy: 0.7889947295188904 for episode 606\n",
      "Accuracy: 0.7348170280456543 for episode 607\n",
      "Accuracy: 0.7889947295188904 for episode 608\n",
      "Accuracy: 0.7723342180252075 for episode 609\n",
      "Accuracy: 0.7513466477394104 for episode 610\n",
      "Accuracy: 0.7535715699195862 for episode 611\n",
      "Accuracy: 0.7889947295188904 for episode 612\n",
      "Accuracy: 0.7733031511306763 for episode 613\n",
      "Accuracy: 0.9448685050010681 for episode 614\n",
      "Accuracy: 0.7466604113578796 for episode 615\n",
      "Accuracy: 0.9202268123626709 for episode 616\n",
      "Accuracy: 0.925092875957489 for episode 617\n",
      "Accuracy: 0.7640172243118286 for episode 618\n",
      "Accuracy: 0.770458459854126 for episode 619\n",
      "Accuracy: 0.9281994104385376 for episode 620\n",
      "Accuracy: 0.8088371157646179 for episode 621\n",
      "Accuracy: 0.7889947295188904 for episode 622\n",
      "Accuracy: 0.9290140271186829 for episode 623\n",
      "Accuracy: 0.7839134335517883 for episode 624\n",
      "Accuracy: 0.7671456336975098 for episode 625\n",
      "Accuracy: 0.9325972199440002 for episode 626\n",
      "Accuracy: 0.7889947295188904 for episode 627\n",
      "Accuracy: 0.736478328704834 for episode 628\n",
      "Accuracy: 0.7839134335517883 for episode 629\n",
      "Accuracy: 0.7889947295188904 for episode 630\n",
      "Accuracy: 0.9373398423194885 for episode 631\n",
      "Accuracy: 0.7839134335517883 for episode 632\n",
      "Accuracy: 0.925092875957489 for episode 633\n",
      "Accuracy: 0.7889947295188904 for episode 634\n",
      "Accuracy: 0.736478328704834 for episode 635\n",
      "Accuracy: 0.923686146736145 for episode 636\n",
      "Accuracy: 0.7889947295188904 for episode 637\n",
      "Accuracy: 0.7889947295188904 for episode 638\n",
      "Accuracy: 0.7753796577453613 for episode 639\n",
      "Accuracy: 0.7889947295188904 for episode 640\n",
      "Accuracy: 0.7562592625617981 for episode 641\n",
      "Accuracy: 0.9319303035736084 for episode 642\n",
      "Accuracy: 0.7535715699195862 for episode 643\n",
      "Accuracy: 0.7889947295188904 for episode 644\n",
      "Accuracy: 0.9281994104385376 for episode 645\n",
      "Accuracy: 0.7889947295188904 for episode 646\n",
      "Accuracy: 0.9281994104385376 for episode 647\n",
      "Accuracy: 0.7889947295188904 for episode 648\n",
      "Accuracy: 0.7889947295188904 for episode 649\n",
      "Accuracy: 0.770458459854126 for episode 650\n",
      "Accuracy: 0.7889947295188904 for episode 651\n",
      "Accuracy: 0.7376570105552673 for episode 652\n",
      "Accuracy: 0.7348170280456543 for episode 653\n",
      "Accuracy: 0.7889947295188904 for episode 654\n",
      "Accuracy: 0.9201174378395081 for episode 655\n",
      "Accuracy: 0.7889947295188904 for episode 656\n",
      "Accuracy: 0.7889947295188904 for episode 657\n",
      "Accuracy: 0.7372422218322754 for episode 658\n",
      "Accuracy: 0.7551933526992798 for episode 659\n",
      "Accuracy: 0.7686756253242493 for episode 660\n",
      "Accuracy: 0.7889947295188904 for episode 661\n",
      "Accuracy: 0.944547712802887 for episode 662\n",
      "Accuracy: 0.7535715699195862 for episode 663\n",
      "Accuracy: 0.925092875957489 for episode 664\n",
      "Accuracy: 0.9307712912559509 for episode 665\n",
      "Accuracy: 0.819848895072937 for episode 666\n",
      "Accuracy: 0.9318768382072449 for episode 667\n",
      "Accuracy: 0.7562592625617981 for episode 668\n",
      "Accuracy: 0.7704378962516785 for episode 669\n",
      "Accuracy: 0.7644068598747253 for episode 670\n",
      "Accuracy: 0.7889947295188904 for episode 671\n",
      "Accuracy: 0.7404413819313049 for episode 672\n",
      "Accuracy: 0.7889947295188904 for episode 673\n",
      "Accuracy: 0.9243929982185364 for episode 674\n",
      "Accuracy: 0.9290140271186829 for episode 675\n",
      "Accuracy: 0.7686756253242493 for episode 676\n",
      "Accuracy: 0.7772524356842041 for episode 677\n",
      "Accuracy: 0.9141677618026733 for episode 678\n",
      "Accuracy: 0.7889947295188904 for episode 679\n",
      "Accuracy: 0.759606122970581 for episode 680\n",
      "Accuracy: 0.9304686188697815 for episode 681\n",
      "Accuracy: 0.9324069023132324 for episode 682\n",
      "Accuracy: 0.7889947295188904 for episode 683\n",
      "Accuracy: 0.7575494050979614 for episode 684\n",
      "Accuracy: 0.925092875957489 for episode 685\n",
      "Accuracy: 0.9324069023132324 for episode 686\n",
      "Accuracy: 0.9318768382072449 for episode 687\n",
      "Accuracy: 0.944547712802887 for episode 688\n",
      "Accuracy: 0.770458459854126 for episode 689\n",
      "Accuracy: 0.9281994104385376 for episode 690\n",
      "Accuracy: 0.7889947295188904 for episode 691\n",
      "Accuracy: 0.7535715699195862 for episode 692\n",
      "Accuracy: 0.7744229435920715 for episode 693\n",
      "Accuracy: 0.770458459854126 for episode 694\n",
      "Accuracy: 0.798057496547699 for episode 695\n",
      "Accuracy: 0.928982675075531 for episode 696\n",
      "Accuracy: 0.7889947295188904 for episode 697\n",
      "Accuracy: 0.7744229435920715 for episode 698\n",
      "Accuracy: 0.7562592625617981 for episode 699\n",
      "Accuracy: 0.7889947295188904 for episode 700\n",
      "Accuracy: 0.925092875957489 for episode 701\n",
      "Accuracy: 0.9281994104385376 for episode 702\n",
      "Accuracy: 0.7348170280456543 for episode 703\n",
      "Accuracy: 0.7518733143806458 for episode 704\n",
      "Accuracy: 0.923686146736145 for episode 705\n",
      "Accuracy: 0.7734160423278809 for episode 706\n",
      "Accuracy: 0.9290140271186829 for episode 707\n",
      "Accuracy: 0.9281994104385376 for episode 708\n",
      "Accuracy: 0.9155168533325195 for episode 709\n",
      "Accuracy: 0.925092875957489 for episode 710\n",
      "Accuracy: 0.7889947295188904 for episode 711\n",
      "Accuracy: 0.7889947295188904 for episode 712\n",
      "Accuracy: 0.7584961652755737 for episode 713\n",
      "Accuracy: 0.7645118832588196 for episode 714\n",
      "Accuracy: 0.770458459854126 for episode 715\n",
      "Accuracy: 0.9155168533325195 for episode 716\n",
      "Accuracy: 0.7349642515182495 for episode 717\n",
      "Accuracy: 0.7889947295188904 for episode 718\n",
      "Accuracy: 0.925092875957489 for episode 719\n",
      "Accuracy: 0.7889947295188904 for episode 720\n",
      "Accuracy: 0.9235689640045166 for episode 721\n",
      "Accuracy: 0.770458459854126 for episode 722\n",
      "Accuracy: 0.7889947295188904 for episode 723\n",
      "Accuracy: 0.7744229435920715 for episode 724\n",
      "Accuracy: 0.770458459854126 for episode 725\n",
      "Accuracy: 0.7839134335517883 for episode 726\n",
      "Accuracy: 0.7644068598747253 for episode 727\n",
      "Accuracy: 0.7723342180252075 for episode 728\n",
      "Accuracy: 0.9438520669937134 for episode 729\n",
      "Accuracy: 0.9261704683303833 for episode 730\n",
      "Accuracy: 0.7744229435920715 for episode 731\n",
      "Accuracy: 0.7725493907928467 for episode 732\n",
      "Accuracy: 0.7513466477394104 for episode 733\n",
      "Accuracy: 0.9281994104385376 for episode 734\n",
      "Accuracy: 0.7584961652755737 for episode 735\n",
      "Accuracy: 0.7889947295188904 for episode 736\n",
      "Accuracy: 0.9285473227500916 for episode 737\n",
      "Accuracy: 0.770458459854126 for episode 738\n",
      "Accuracy: 0.7889947295188904 for episode 739\n",
      "Accuracy: 0.7376570105552673 for episode 740\n",
      "Accuracy: 0.9141677618026733 for episode 741\n",
      "Accuracy: 0.7889947295188904 for episode 742\n",
      "Accuracy: 0.7889947295188904 for episode 743\n",
      "Accuracy: 0.7889947295188904 for episode 744\n",
      "Accuracy: 0.7889947295188904 for episode 745\n",
      "Accuracy: 0.7889947295188904 for episode 746\n",
      "Accuracy: 0.9318768382072449 for episode 747\n",
      "Accuracy: 0.8088371157646179 for episode 748\n",
      "Accuracy: 0.7889947295188904 for episode 749\n",
      "Accuracy: 0.7889947295188904 for episode 750\n",
      "Accuracy: 0.798057496547699 for episode 751\n",
      "Accuracy: 0.7372422218322754 for episode 752\n",
      "Accuracy: 0.925092875957489 for episode 753\n",
      "Accuracy: 0.7513466477394104 for episode 754\n",
      "Accuracy: 0.7584961652755737 for episode 755\n",
      "Accuracy: 0.770458459854126 for episode 756\n",
      "Accuracy: 0.7889947295188904 for episode 757\n",
      "Accuracy: 0.7599714398384094 for episode 758\n",
      "Accuracy: 0.9226190447807312 for episode 759\n",
      "Accuracy: 0.7889947295188904 for episode 760\n",
      "Accuracy: 0.7889947295188904 for episode 761\n",
      "Accuracy: 0.7889947295188904 for episode 762\n",
      "Accuracy: 0.7889947295188904 for episode 763\n",
      "Accuracy: 0.7889947295188904 for episode 764\n",
      "Accuracy: 0.9285473227500916 for episode 765\n",
      "Accuracy: 0.7889947295188904 for episode 766\n",
      "Accuracy: 0.7889947295188904 for episode 767\n",
      "Accuracy: 0.7889947295188904 for episode 768\n",
      "Accuracy: 0.9438520669937134 for episode 769\n",
      "Accuracy: 0.7686992287635803 for episode 770\n",
      "Accuracy: 0.7889947295188904 for episode 771\n",
      "Accuracy: 0.9227997064590454 for episode 772\n",
      "Accuracy: 0.7744229435920715 for episode 773\n",
      "Accuracy: 0.759606122970581 for episode 774\n",
      "Accuracy: 0.7889947295188904 for episode 775\n",
      "Accuracy: 0.7686992287635803 for episode 776\n",
      "Accuracy: 0.9202268123626709 for episode 777\n",
      "Accuracy: 0.9325860738754272 for episode 778\n",
      "Accuracy: 0.9319303035736084 for episode 779\n",
      "Accuracy: 0.7466604113578796 for episode 780\n",
      "Accuracy: 0.9325860738754272 for episode 781\n",
      "Accuracy: 0.9202268123626709 for episode 782\n",
      "Accuracy: 0.7889947295188904 for episode 783\n",
      "Accuracy: 0.7889947295188904 for episode 784\n",
      "Accuracy: 0.7889947295188904 for episode 785\n",
      "Accuracy: 0.7889947295188904 for episode 786\n",
      "Accuracy: 0.7889947295188904 for episode 787\n",
      "Accuracy: 0.7606016993522644 for episode 788\n",
      "Accuracy: 0.7348170280456543 for episode 789\n",
      "Accuracy: 0.9238914251327515 for episode 790\n",
      "Accuracy: 0.757368803024292 for episode 791\n",
      "Accuracy: 0.7889947295188904 for episode 792\n",
      "Accuracy: 0.7889947295188904 for episode 793\n",
      "Accuracy: 0.7889947295188904 for episode 794\n",
      "Accuracy: 0.7372422218322754 for episode 795\n",
      "Accuracy: 0.927057147026062 for episode 796\n",
      "Accuracy: 0.7692904472351074 for episode 797\n",
      "Accuracy: 0.9322266578674316 for episode 798\n",
      "Accuracy: 0.7889947295188904 for episode 799\n",
      "Accuracy: 0.770458459854126 for episode 800\n",
      "Accuracy: 0.9318768382072449 for episode 801\n",
      "Accuracy: 0.9190993905067444 for episode 802\n",
      "Accuracy: 0.7640172243118286 for episode 803\n",
      "Accuracy: 0.7889947295188904 for episode 804\n",
      "Accuracy: 0.9243929982185364 for episode 805\n",
      "Accuracy: 0.9291768670082092 for episode 806\n",
      "Accuracy: 0.9243929982185364 for episode 807\n",
      "Accuracy: 0.7839134335517883 for episode 808\n",
      "Accuracy: 0.770458459854126 for episode 809\n",
      "Accuracy: 0.923686146736145 for episode 810\n",
      "Accuracy: 0.7889947295188904 for episode 811\n",
      "Accuracy: 0.7349362373352051 for episode 812\n",
      "Accuracy: 0.7889947295188904 for episode 813\n",
      "Accuracy: 0.7349362373352051 for episode 814\n",
      "Accuracy: 0.7889947295188904 for episode 815\n",
      "Accuracy: 0.7606016993522644 for episode 816\n",
      "Accuracy: 0.7889947295188904 for episode 817\n",
      "Accuracy: 0.9189534187316895 for episode 818\n",
      "Accuracy: 0.7375838756561279 for episode 819\n",
      "Accuracy: 0.7704378962516785 for episode 820\n",
      "Accuracy: 0.7889947295188904 for episode 821\n",
      "Accuracy: 0.7889947295188904 for episode 822\n",
      "Accuracy: 0.7348170280456543 for episode 823\n",
      "Accuracy: 0.7889947295188904 for episode 824\n",
      "Accuracy: 0.9202268123626709 for episode 825\n",
      "Accuracy: 0.7733031511306763 for episode 826\n",
      "Accuracy: 0.7889947295188904 for episode 827\n",
      "Accuracy: 0.8088371157646179 for episode 828\n",
      "Accuracy: 0.7575494050979614 for episode 829\n",
      "Accuracy: 0.759606122970581 for episode 830\n",
      "Accuracy: 0.7889947295188904 for episode 831\n",
      "Accuracy: 0.9218910336494446 for episode 832\n",
      "Accuracy: 0.9202268123626709 for episode 833\n",
      "Accuracy: 0.7889947295188904 for episode 834\n",
      "Accuracy: 0.9325860738754272 for episode 835\n",
      "Accuracy: 0.9440740346908569 for episode 836\n",
      "Accuracy: 0.9342565536499023 for episode 837\n",
      "Accuracy: 0.7889947295188904 for episode 838\n",
      "Accuracy: 0.9238914251327515 for episode 839\n",
      "Accuracy: 0.770458459854126 for episode 840\n",
      "Accuracy: 0.9189534187316895 for episode 841\n",
      "Accuracy: 0.7889947295188904 for episode 842\n",
      "Accuracy: 0.7686756253242493 for episode 843\n",
      "Accuracy: 0.925092875957489 for episode 844\n",
      "Accuracy: 0.770458459854126 for episode 845\n",
      "Accuracy: 0.7889947295188904 for episode 846\n",
      "Accuracy: 0.7671456336975098 for episode 847\n",
      "Accuracy: 0.7372422218322754 for episode 848\n",
      "Accuracy: 0.7535715699195862 for episode 849\n",
      "Accuracy: 0.7551933526992798 for episode 850\n",
      "Accuracy: 0.7889947295188904 for episode 851\n",
      "Accuracy: 0.9244470596313477 for episode 852\n",
      "Accuracy: 0.7889947295188904 for episode 853\n",
      "Accuracy: 0.9322266578674316 for episode 854\n",
      "Accuracy: 0.7348170280456543 for episode 855\n",
      "Accuracy: 0.7889947295188904 for episode 856\n",
      "Accuracy: 0.7535715699195862 for episode 857\n",
      "Accuracy: 0.9281994104385376 for episode 858\n",
      "Accuracy: 0.7671456336975098 for episode 859\n",
      "Accuracy: 0.7551933526992798 for episode 860\n",
      "Accuracy: 0.7513466477394104 for episode 861\n",
      "Accuracy: 0.944547712802887 for episode 862\n",
      "Accuracy: 0.7889947295188904 for episode 863\n",
      "Accuracy: 0.7584961652755737 for episode 864\n",
      "Accuracy: 0.7889947295188904 for episode 865\n",
      "Accuracy: 0.7744229435920715 for episode 866\n",
      "Accuracy: 0.9202268123626709 for episode 867\n",
      "Accuracy: 0.7575494050979614 for episode 868\n",
      "Accuracy: 0.8088371157646179 for episode 869\n",
      "Accuracy: 0.7349642515182495 for episode 870\n",
      "Accuracy: 0.7348170280456543 for episode 871\n",
      "Accuracy: 0.9238914251327515 for episode 872\n",
      "Accuracy: 0.798057496547699 for episode 873\n",
      "Accuracy: 0.757368803024292 for episode 874\n",
      "Accuracy: 0.9141677618026733 for episode 875\n",
      "Accuracy: 0.7692904472351074 for episode 876\n",
      "Accuracy: 0.7889947295188904 for episode 877\n",
      "Accuracy: 0.7348170280456543 for episode 878\n",
      "Accuracy: 0.7599714398384094 for episode 879\n",
      "Accuracy: 0.9141677618026733 for episode 880\n",
      "Accuracy: 0.9448685050010681 for episode 881\n",
      "Accuracy: 0.770458459854126 for episode 882\n",
      "Accuracy: 0.7889947295188904 for episode 883\n",
      "Accuracy: 0.9202268123626709 for episode 884\n",
      "Accuracy: 0.7733031511306763 for episode 885\n",
      "Accuracy: 0.9212055206298828 for episode 886\n",
      "Accuracy: 0.7372422218322754 for episode 887\n",
      "Accuracy: 0.7889947295188904 for episode 888\n",
      "Accuracy: 0.9238914251327515 for episode 889\n",
      "Accuracy: 0.9227997064590454 for episode 890\n",
      "Accuracy: 0.7889947295188904 for episode 891\n",
      "Accuracy: 0.7686756253242493 for episode 892\n",
      "Accuracy: 0.7889947295188904 for episode 893\n",
      "Accuracy: 0.9440740346908569 for episode 894\n",
      "Accuracy: 0.9440740346908569 for episode 895\n",
      "Accuracy: 0.7889947295188904 for episode 896\n",
      "Accuracy: 0.7644068598747253 for episode 897\n",
      "Accuracy: 0.760255753993988 for episode 898\n",
      "Accuracy: 0.7644068598747253 for episode 899\n",
      "Accuracy: 0.7575494050979614 for episode 900\n",
      "Accuracy: 0.759606122970581 for episode 901\n",
      "Accuracy: 0.7562592625617981 for episode 902\n",
      "Accuracy: 0.7671456336975098 for episode 903\n",
      "Accuracy: 0.7466604113578796 for episode 904\n",
      "Accuracy: 0.7889947295188904 for episode 905\n",
      "Accuracy: 0.7562592625617981 for episode 906\n",
      "Accuracy: 0.9227997064590454 for episode 907\n",
      "Accuracy: 0.925092875957489 for episode 908\n",
      "Accuracy: 0.7599714398384094 for episode 909\n",
      "Accuracy: 0.7535715699195862 for episode 910\n",
      "Accuracy: 0.7889947295188904 for episode 911\n",
      "Accuracy: 0.9324069023132324 for episode 912\n",
      "Accuracy: 0.8088371157646179 for episode 913\n",
      "Accuracy: 0.9373398423194885 for episode 914\n",
      "Accuracy: 0.7889947295188904 for episode 915\n",
      "Accuracy: 0.9325860738754272 for episode 916\n",
      "Accuracy: 0.7889947295188904 for episode 917\n",
      "Accuracy: 0.7372422218322754 for episode 918\n",
      "Accuracy: 0.7733031511306763 for episode 919\n",
      "Accuracy: 0.7372422218322754 for episode 920\n",
      "Accuracy: 0.7889947295188904 for episode 921\n",
      "Accuracy: 0.7404907941818237 for episode 922\n",
      "Accuracy: 0.759606122970581 for episode 923\n",
      "Accuracy: 0.819848895072937 for episode 924\n",
      "Accuracy: 0.7692904472351074 for episode 925\n",
      "Accuracy: 0.7692904472351074 for episode 926\n",
      "Accuracy: 0.7349642515182495 for episode 927\n",
      "Accuracy: 0.8088371157646179 for episode 928\n",
      "Accuracy: 0.7562592625617981 for episode 929\n",
      "Accuracy: 0.9356663823127747 for episode 930\n",
      "Accuracy: 0.9190993905067444 for episode 931\n",
      "Accuracy: 0.7725493907928467 for episode 932\n",
      "Accuracy: 0.7889947295188904 for episode 933\n",
      "Accuracy: 0.7889947295188904 for episode 934\n",
      "Accuracy: 0.9218910336494446 for episode 935\n",
      "Accuracy: 0.7889947295188904 for episode 936\n",
      "Accuracy: 0.9441887140274048 for episode 937\n",
      "Accuracy: 0.7686992287635803 for episode 938\n",
      "Accuracy: 0.770458459854126 for episode 939\n",
      "Accuracy: 0.7372422218322754 for episode 940\n",
      "Accuracy: 0.9441887140274048 for episode 941\n",
      "Accuracy: 0.759606122970581 for episode 942\n",
      "Accuracy: 0.7889947295188904 for episode 943\n",
      "Accuracy: 0.736478328704834 for episode 944\n",
      "Accuracy: 0.7348170280456543 for episode 945\n",
      "Accuracy: 0.9238914251327515 for episode 946\n",
      "Accuracy: 0.7889947295188904 for episode 947\n",
      "Accuracy: 0.770458459854126 for episode 948\n",
      "Accuracy: 0.819848895072937 for episode 949\n",
      "Accuracy: 0.928982675075531 for episode 950\n",
      "Accuracy: 0.757368803024292 for episode 951\n",
      "Accuracy: 0.7772524356842041 for episode 952\n",
      "Accuracy: 0.944547712802887 for episode 953\n",
      "Accuracy: 0.9281994104385376 for episode 954\n",
      "Accuracy: 0.819848895072937 for episode 955\n",
      "Accuracy: 0.7723342180252075 for episode 956\n",
      "Accuracy: 0.7404413819313049 for episode 957\n",
      "Accuracy: 0.9281994104385376 for episode 958\n",
      "Accuracy: 0.9155168533325195 for episode 959\n",
      "Accuracy: 0.770458459854126 for episode 960\n",
      "Accuracy: 0.7349362373352051 for episode 961\n",
      "Accuracy: 0.770458459854126 for episode 962\n",
      "Accuracy: 0.9356663823127747 for episode 963\n",
      "Accuracy: 0.7889947295188904 for episode 964\n",
      "Accuracy: 0.7744229435920715 for episode 965\n",
      "Accuracy: 0.7372422218322754 for episode 966\n",
      "Accuracy: 0.7889947295188904 for episode 967\n",
      "Accuracy: 0.925092875957489 for episode 968\n",
      "Accuracy: 0.7725493907928467 for episode 969\n",
      "Accuracy: 0.7725493907928467 for episode 970\n",
      "Accuracy: 0.9438520669937134 for episode 971\n",
      "Accuracy: 0.757368803024292 for episode 972\n",
      "Accuracy: 0.9202268123626709 for episode 973\n",
      "Accuracy: 0.762614905834198 for episode 974\n",
      "Accuracy: 0.7515074014663696 for episode 975\n",
      "Accuracy: 0.7818193435668945 for episode 976\n",
      "Accuracy: 0.7889947295188904 for episode 977\n",
      "Accuracy: 0.7348170280456543 for episode 978\n",
      "Accuracy: 0.7889947295188904 for episode 979\n",
      "Accuracy: 0.7839134335517883 for episode 980\n",
      "Accuracy: 0.7606016993522644 for episode 981\n",
      "Accuracy: 0.9238914251327515 for episode 982\n",
      "Accuracy: 0.7404413819313049 for episode 983\n",
      "Accuracy: 0.9370210766792297 for episode 984\n",
      "Accuracy: 0.9238914251327515 for episode 985\n",
      "Accuracy: 0.7889947295188904 for episode 986\n",
      "Accuracy: 0.7744229435920715 for episode 987\n",
      "Accuracy: 0.7644068598747253 for episode 988\n",
      "Accuracy: 0.7760769128799438 for episode 989\n",
      "Accuracy: 0.7889947295188904 for episode 990\n",
      "Accuracy: 0.7839134335517883 for episode 991\n",
      "Accuracy: 0.7889947295188904 for episode 992\n",
      "Accuracy: 0.9141677618026733 for episode 993\n",
      "Accuracy: 0.7889947295188904 for episode 994\n",
      "Accuracy: 0.7889947295188904 for episode 995\n",
      "Accuracy: 0.9202268123626709 for episode 996\n",
      "Accuracy: 0.7889947295188904 for episode 997\n",
      "Accuracy: 0.9243929982185364 for episode 998\n",
      "Accuracy: 0.7889947295188904 for episode 999\n",
      "Average Accuracy: 0.8142589330673218\n",
      "Max Accuracy: 0.9552870988845825\n"
     ]
    }
   ],
   "source": [
    "max_state = trainer.evaluate_accuracy(num_episodes=1000, num_steps=500, find_max=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 22])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surrogate model loaded from:  /Users/tawab/Desktop/columbia/Courses/Spring2024/HPML/Project/Analog_NAS/env/models/surrogate_model.json\n",
      "Codebook loaded from:  /Users/tawab/Desktop/columbia/Courses/Spring2024/HPML/Project/Analog_NAS/env/models/codebook.pth\n",
      "Decoder model loaded from:  /Users/tawab/Desktop/columbia/Courses/Spring2024/HPML/Project/Analog_NAS/env/models/decoder_model.pth\n",
      "Predictions:  [0.7889947]\n",
      "Predictions:  [0.7404414]\n",
      "Predictions:  [0.92011744]\n",
      "Predictions:  [0.9441887]\n",
      "Predictions:  [0.9441887]\n",
      "Predictions:  [0.9441887]\n",
      "Predictions:  [0.9238914]\n",
      "Predictions:  [0.9238914]\n",
      "Predictions:  [0.9238914]\n",
      "Predictions:  [0.9238914]\n",
      "Predictions:  [0.9238914]\n",
      "Environment check passed\n"
     ]
    }
   ],
   "source": [
    "decoder_config = {\n",
    "    \"out_dim\": 22,           # Output dimension\n",
    "    \"embed_dim\": 8,          # Embedding dimension\n",
    "    \"h_nodes\": 512,          # Number of hidden nodes\n",
    "    \"dropout\": 0.2,          # Dropout rate\n",
    "    \"scale\": 2,              # Scale factor\n",
    "    \"num_layers\": 5,         # Number of layers\n",
    "    \"load_path\": '/Users/tawab/Desktop/columbia/Courses/Spring2024/HPML/Project/Analog_NAS/env/models/decoder_model.pth', # Path to load model weights\n",
    "}\n",
    "\n",
    "env_config = {\n",
    "    \"embed_dim\": decoder_config['embed_dim'],    # Embedding dimension\n",
    "    \"num_embeddings\": 14,           # Number of embeddings\n",
    "    \"max_allowed_actions\": 200,      # Maximum allowed actions\n",
    "    \"consider_previous_actions\": True, # Consider previous actions\n",
    "    \"num_previous_actions\": 12,       # Number of previous actions to consider  \n",
    "    \"render_mode\": 'human',          # Render mode\n",
    "    \"render_data\": '/Users/tawab/Desktop/columbia/Courses/Spring2024/HPML/Project/Analog_NAS/env/render/architectures_trained_on.npy',  # Data for rendering\n",
    "    \"render_labels\": '/Users/tawab/Desktop/columbia/Courses/Spring2024/HPML/Project/Analog_NAS/env/render/labels.npy',   # Labels for rendering\n",
    "    \"render_log_dir\": 'trainingLogs',                  # Directory for logging data\n",
    "}\n",
    "\n",
    "model_config = {\n",
    "    \"model\": \"PPO\",                # Model type ('PPO', 'A2C', 'DQN', etc.)\n",
    "    \"policy\": 'MultiInputPolicy',  # Policy type\n",
    "    \"total_timesteps\": 1500000,       # Total number of timesteps\n",
    "    \"verbose\": 0,                  # Verbosity level\n",
    "    \"tensorboard_log\": env_config['render_log_dir'],  # Tensorboard log directory\n",
    "    \"n_steps\": 2048,               # Number of steps to run for each environment per update\n",
    "    \"progress_bar\": False,          # Whether to display a progress bar\n",
    "    \"n_epochs\": 10,                # Number of epochs\n",
    "    \"batch_size\": 64,              # Batch size\n",
    "}\n",
    "\n",
    "log_config = {\n",
    "    \"project\": 'PPO Training',                          # Project name in wandb\n",
    "    #\"entity\": 'trex-ai',                            # Entity name in wandb\n",
    "    \"sync_tensorboard\": True,                           # Whether to sync TensorBoard\n",
    "    \"save_code\": True,                                  # Whether to save code in wandb\n",
    "    \"model_save_path\": env_config['render_log_dir'],    # Path to save the model\n",
    "    \"gradient_save_freq\": 100,                          # Frequency to save gradients\n",
    "    \"verbose\": 2,                                       # Verbosity level\n",
    "}\n",
    "\n",
    "trainer = Trainer(surrogate_path=surrogate_model, \n",
    "                  codebook_path=codebook, \n",
    "                  decoder_config=decoder_config, \n",
    "                  env_config=env_config, \n",
    "                  model_config=model_config, \n",
    "                  log_config=log_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.load_model('/Users/tawab/Desktop/columbia/Courses/Spring2024/HPML/Project/Analog_NAS/models/ppo_multiInputPolicy_12_1500000.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0\n",
      "Predictions:  [0.7723342]\n",
      "reward: [0.7723342], action: [110], cum_reward: [0.7723342], max_reward: 0\n",
      "Predictions:  [0.7760769]\n",
      "reward: [0.00374269], action: [69], cum_reward: [0.7760769], max_reward: [0.7760769]\n",
      "Predictions:  [0.91561663]\n",
      "reward: [0.13953972], action: [39], cum_reward: [0.91561663], max_reward: [0.91561663]\n",
      "Predictions:  [0.91561663]\n",
      "reward: [0.], action: [57], cum_reward: [0.91561663], max_reward: [0.91561663]\n",
      "Predictions:  [0.91561663]\n",
      "reward: [0.], action: [39], cum_reward: [0.91561663], max_reward: [0.91561663]\n",
      "Predictions:  [0.7626149]\n",
      "reward: [-0.15300173], action: [11], cum_reward: [0.7626149], max_reward: [0.7626149]\n",
      "Predictions:  [0.77330315]\n",
      "reward: [0.01068825], action: [84], cum_reward: [0.77330315], max_reward: [0.77330315]\n",
      "Predictions:  [0.7725494]\n",
      "reward: [-0.00075376], action: [59], cum_reward: [0.7725494], max_reward: [0.7725494]\n",
      "Predictions:  [0.7725494]\n",
      "reward: [0.], action: [12], cum_reward: [0.7725494], max_reward: [0.7725494]\n",
      "Predictions:  [0.7725494]\n",
      "reward: [0.], action: [65], cum_reward: [0.7725494], max_reward: [0.7725494]\n",
      "Predictions:  [0.7725494]\n",
      "reward: [0.], action: [81], cum_reward: [0.7725494], max_reward: [0.7725494]\n",
      "reward: [0.], action: [112], cum_reward: [0.7725494], max_reward: [0.7725494]\n",
      "Episode 0,11: cum reward: [0.7725494], max reward: [0.7725494] action: [110] last action: [112]\n",
      "[[False False False False False False False False]]\n",
      "Predictions:  [0.9281994]\n",
      "Episode 0: Accuracy: 0.9281994104385376\n",
      "Episode 1\n",
      "Predictions:  [0.7606017]\n",
      "reward: [0.7606017], action: [86], cum_reward: [0.7606017], max_reward: 0\n",
      "Predictions:  [0.9238914]\n",
      "reward: [0.16328973], action: [84], cum_reward: [0.9238914], max_reward: [0.9238914]\n",
      "reward: [0.], action: [112], cum_reward: [0.9238914], max_reward: [0.9238914]\n",
      "Episode 1,2: cum reward: [0.9238914], max reward: [0.9238914] action: [86] last action: [112]\n",
      "[[False False False False False False False False]]\n",
      "Predictions:  [0.7889947]\n",
      "Episode 1: Accuracy: 0.7889947295188904\n",
      "Episode 2\n",
      "Predictions:  [0.9325861]\n",
      "reward: [0.9325861], action: [69], cum_reward: [0.9325861], max_reward: 0\n",
      "Predictions:  [0.9319303]\n",
      "reward: [-0.00065577], action: [39], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.9319303]\n",
      "reward: [0.], action: [95], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.9324069]\n",
      "reward: [0.0004766], action: [97], cum_reward: [0.9324069], max_reward: [0.9324069]\n",
      "Predictions:  [0.9324069]\n",
      "reward: [0.], action: [12], cum_reward: [0.9324069], max_reward: [0.9324069]\n",
      "Predictions:  [0.9324069]\n",
      "reward: [0.], action: [65], cum_reward: [0.9324069], max_reward: [0.9324069]\n",
      "Predictions:  [0.9319303]\n",
      "reward: [-0.0004766], action: [59], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.9319303]\n",
      "reward: [0.], action: [95], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.9319303]\n",
      "reward: [0.], action: [12], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.9319303]\n",
      "reward: [0.], action: [59], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.9319303]\n",
      "reward: [0.], action: [12], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.9319303]\n",
      "reward: [0.], action: [95], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.9319303]\n",
      "reward: [0.], action: [81], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.9319303]\n",
      "reward: [0.], action: [95], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.9319303]\n",
      "reward: [0.], action: [81], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.9319303]\n",
      "reward: [0.], action: [12], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.9319303]\n",
      "reward: [0.], action: [81], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.9319303]\n",
      "reward: [0.], action: [12], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.9319303]\n",
      "reward: [0.], action: [95], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.9319303]\n",
      "reward: [0.], action: [59], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.9319303]\n",
      "reward: [0.], action: [39], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.9319303]\n",
      "reward: [0.], action: [65], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.9319303]\n",
      "reward: [0.], action: [81], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.9319303]\n",
      "reward: [0.], action: [59], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.9319303]\n",
      "reward: [0.], action: [59], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.9319303]\n",
      "reward: [0.], action: [59], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.9319303]\n",
      "reward: [0.], action: [12], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.9319303]\n",
      "reward: [0.], action: [59], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.9319303]\n",
      "reward: [0.], action: [12], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.9319303]\n",
      "reward: [0.], action: [95], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.9319303]\n",
      "reward: [0.], action: [81], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.9319303]\n",
      "reward: [0.], action: [39], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.9319303]\n",
      "reward: [0.], action: [81], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.9319303]\n",
      "reward: [0.], action: [81], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.9319303]\n",
      "reward: [0.], action: [59], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.9319303]\n",
      "reward: [0.], action: [59], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.9319303]\n",
      "reward: [0.], action: [59], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.9319303]\n",
      "reward: [0.], action: [12], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.9319303]\n",
      "reward: [0.], action: [81], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.9319303]\n",
      "reward: [0.], action: [12], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.9319303]\n",
      "reward: [0.], action: [95], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.9319303]\n",
      "reward: [0.], action: [81], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.9319303]\n",
      "reward: [0.], action: [39], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.9319303]\n",
      "reward: [0.], action: [81], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.9319303]\n",
      "reward: [0.], action: [81], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.9319303]\n",
      "reward: [0.], action: [59], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.9319303]\n",
      "reward: [0.], action: [59], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.9319303]\n",
      "reward: [0.], action: [95], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.9319303]\n",
      "reward: [0.], action: [12], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.9319303]\n",
      "reward: [0.], action: [59], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.9319303]\n",
      "reward: [0.], action: [12], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.9319303]\n",
      "reward: [0.], action: [95], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.9319303]\n",
      "reward: [0.], action: [81], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.9319303]\n",
      "reward: [0.], action: [59], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.9319303]\n",
      "reward: [0.], action: [81], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.9319303]\n",
      "reward: [0.], action: [95], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.9319303]\n",
      "reward: [0.], action: [81], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.9319303]\n",
      "reward: [0.], action: [59], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.9319303]\n",
      "reward: [0.], action: [95], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.9319303]\n",
      "reward: [0.], action: [12], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.9319303]\n",
      "reward: [0.], action: [65], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.9319303]\n",
      "reward: [0.], action: [12], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.9319303]\n",
      "reward: [0.], action: [95], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.9319303]\n",
      "reward: [0.], action: [39], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.9319303]\n",
      "reward: [0.], action: [39], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.9319303]\n",
      "reward: [0.], action: [81], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.9319303]\n",
      "reward: [0.], action: [95], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.9319303]\n",
      "reward: [0.], action: [59], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.9319303]\n",
      "reward: [0.], action: [81], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.9319303]\n",
      "reward: [0.], action: [12], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.9319303]\n",
      "reward: [0.], action: [12], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.9319303]\n",
      "reward: [0.], action: [59], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.9319303]\n",
      "reward: [0.], action: [59], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.9319303]\n",
      "reward: [0.], action: [59], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.9319303]\n",
      "reward: [0.], action: [81], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.9319303]\n",
      "reward: [0.], action: [81], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.9319303]\n",
      "reward: [0.], action: [81], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.9319303]\n",
      "reward: [0.], action: [81], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.9319303]\n",
      "reward: [0.], action: [12], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.9319303]\n",
      "reward: [0.], action: [59], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.9319303]\n",
      "reward: [0.], action: [12], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.9319303]\n",
      "reward: [0.], action: [39], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.9319303]\n",
      "reward: [0.], action: [59], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.9319303]\n",
      "reward: [0.], action: [39], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.9319303]\n",
      "reward: [0.], action: [81], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.9319303]\n",
      "reward: [0.], action: [81], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.9319303]\n",
      "reward: [0.], action: [81], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.9319303]\n",
      "reward: [0.], action: [81], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.9319303]\n",
      "reward: [0.], action: [95], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.9319303]\n",
      "reward: [0.], action: [12], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.9319303]\n",
      "reward: [0.], action: [12], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.9319303]\n",
      "reward: [0.], action: [12], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.9319303]\n",
      "reward: [0.], action: [95], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.9319303]\n",
      "reward: [0.], action: [59], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.9319303]\n",
      "reward: [0.], action: [59], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.9319303]\n",
      "reward: [0.], action: [81], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.9319303]\n",
      "reward: [0.], action: [81], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.9319303]\n",
      "reward: [0.], action: [81], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.9319303]\n",
      "reward: [0.], action: [59], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.9319303]\n",
      "reward: [0.], action: [59], cum_reward: [0.9319303], max_reward: [0.9319303]\n",
      "Predictions:  [0.94385207]\n",
      "reward: [0.01192176], action: [0], cum_reward: [0.94385207], max_reward: [0.94385207]\n",
      "Predictions:  [0.94385207]\n",
      "reward: [0.], action: [81], cum_reward: [0.94385207], max_reward: [0.94385207]\n",
      "Predictions:  [0.94385207]\n",
      "reward: [0.], action: [12], cum_reward: [0.94385207], max_reward: [0.94385207]\n",
      "Predictions:  [0.94385207]\n",
      "reward: [0.], action: [59], cum_reward: [0.94385207], max_reward: [0.94385207]\n",
      "Predictions:  [0.94385207]\n",
      "reward: [0.], action: [81], cum_reward: [0.94385207], max_reward: [0.94385207]\n",
      "Predictions:  [0.94385207]\n",
      "reward: [0.], action: [39], cum_reward: [0.94385207], max_reward: [0.94385207]\n",
      "reward: [0.], action: [112], cum_reward: [0.94385207], max_reward: [0.94385207]\n",
      "Episode 2,106: cum reward: [0.94385207], max reward: [0.94385207] action: [69] last action: [112]\n",
      "[[False False False False False False False False]]\n",
      "Predictions:  [0.76929045]\n",
      "Episode 2: Accuracy: 0.7692904472351074\n",
      "Episode 3\n",
      "Predictions:  [0.93425655]\n",
      "reward: [0.93425655], action: [2], cum_reward: [0.93425655], max_reward: 0\n",
      "Predictions:  [0.9324069]\n",
      "reward: [-0.00184965], action: [95], cum_reward: [0.9324069], max_reward: [0.9324069]\n",
      "Predictions:  [0.93425655]\n",
      "reward: [0.00184965], action: [34], cum_reward: [0.93425655], max_reward: [0.93425655]\n",
      "Predictions:  [0.93425655]\n",
      "reward: [0.], action: [95], cum_reward: [0.93425655], max_reward: [0.93425655]\n",
      "Predictions:  [0.93425655]\n",
      "reward: [0.], action: [97], cum_reward: [0.93425655], max_reward: [0.93425655]\n",
      "Predictions:  [0.9325861]\n",
      "reward: [-0.00167048], action: [12], cum_reward: [0.9325861], max_reward: [0.9325861]\n",
      "Predictions:  [0.94070786]\n",
      "reward: [0.00812179], action: [11], cum_reward: [0.94070786], max_reward: [0.94070786]\n",
      "Predictions:  [0.94070786]\n",
      "reward: [0.], action: [95], cum_reward: [0.94070786], max_reward: [0.94070786]\n",
      "Predictions:  [0.94070786]\n",
      "reward: [0.], action: [39], cum_reward: [0.94070786], max_reward: [0.94070786]\n",
      "Predictions:  [0.94070786]\n",
      "reward: [0.], action: [95], cum_reward: [0.94070786], max_reward: [0.94070786]\n",
      "Predictions:  [0.94070786]\n",
      "reward: [0.], action: [39], cum_reward: [0.94070786], max_reward: [0.94070786]\n",
      "Predictions:  [0.94070786]\n",
      "reward: [0.], action: [12], cum_reward: [0.94070786], max_reward: [0.94070786]\n",
      "Predictions:  [0.94070786]\n",
      "reward: [0.], action: [81], cum_reward: [0.94070786], max_reward: [0.94070786]\n",
      "Predictions:  [0.94070786]\n",
      "reward: [0.], action: [95], cum_reward: [0.94070786], max_reward: [0.94070786]\n",
      "Predictions:  [0.94070786]\n",
      "reward: [0.], action: [59], cum_reward: [0.94070786], max_reward: [0.94070786]\n",
      "Predictions:  [0.94070786]\n",
      "reward: [0.], action: [59], cum_reward: [0.94070786], max_reward: [0.94070786]\n",
      "Predictions:  [0.94070786]\n",
      "reward: [0.], action: [12], cum_reward: [0.94070786], max_reward: [0.94070786]\n",
      "Predictions:  [0.94070786]\n",
      "reward: [0.], action: [12], cum_reward: [0.94070786], max_reward: [0.94070786]\n",
      "Predictions:  [0.94070786]\n",
      "reward: [0.], action: [95], cum_reward: [0.94070786], max_reward: [0.94070786]\n",
      "Predictions:  [0.94070786]\n",
      "reward: [0.], action: [95], cum_reward: [0.94070786], max_reward: [0.94070786]\n",
      "Predictions:  [0.94070786]\n",
      "reward: [0.], action: [59], cum_reward: [0.94070786], max_reward: [0.94070786]\n",
      "Predictions:  [0.94070786]\n",
      "reward: [0.], action: [65], cum_reward: [0.94070786], max_reward: [0.94070786]\n",
      "Predictions:  [0.94070786]\n",
      "reward: [0.], action: [12], cum_reward: [0.94070786], max_reward: [0.94070786]\n",
      "Predictions:  [0.94070786]\n",
      "reward: [0.], action: [81], cum_reward: [0.94070786], max_reward: [0.94070786]\n",
      "Predictions:  [0.94070786]\n",
      "reward: [0.], action: [59], cum_reward: [0.94070786], max_reward: [0.94070786]\n",
      "Predictions:  [0.94070786]\n",
      "reward: [0.], action: [59], cum_reward: [0.94070786], max_reward: [0.94070786]\n",
      "Predictions:  [0.92189103]\n",
      "reward: [-0.01881683], action: [97], cum_reward: [0.92189103], max_reward: [0.92189103]\n",
      "Predictions:  [0.92189103]\n",
      "reward: [0.], action: [39], cum_reward: [0.92189103], max_reward: [0.92189103]\n",
      "Predictions:  [0.94070786]\n",
      "reward: [0.01881683], action: [81], cum_reward: [0.94070786], max_reward: [0.94070786]\n",
      "Predictions:  [0.94070786]\n",
      "reward: [0.], action: [81], cum_reward: [0.94070786], max_reward: [0.94070786]\n",
      "Predictions:  [0.94070786]\n",
      "reward: [0.], action: [95], cum_reward: [0.94070786], max_reward: [0.94070786]\n",
      "Predictions:  [0.94070786]\n",
      "reward: [0.], action: [81], cum_reward: [0.94070786], max_reward: [0.94070786]\n",
      "Predictions:  [0.94070786]\n",
      "reward: [0.], action: [59], cum_reward: [0.94070786], max_reward: [0.94070786]\n",
      "Predictions:  [0.94070786]\n",
      "reward: [0.], action: [12], cum_reward: [0.94070786], max_reward: [0.94070786]\n",
      "Predictions:  [0.94070786]\n",
      "reward: [0.], action: [12], cum_reward: [0.94070786], max_reward: [0.94070786]\n",
      "Predictions:  [0.94070786]\n",
      "reward: [0.], action: [59], cum_reward: [0.94070786], max_reward: [0.94070786]\n",
      "Predictions:  [0.94070786]\n",
      "reward: [0.], action: [59], cum_reward: [0.94070786], max_reward: [0.94070786]\n",
      "Predictions:  [0.94070786]\n",
      "reward: [0.], action: [81], cum_reward: [0.94070786], max_reward: [0.94070786]\n",
      "Predictions:  [0.94070786]\n",
      "reward: [0.], action: [81], cum_reward: [0.94070786], max_reward: [0.94070786]\n",
      "Predictions:  [0.94070786]\n",
      "reward: [0.], action: [81], cum_reward: [0.94070786], max_reward: [0.94070786]\n",
      "Predictions:  [0.94070786]\n",
      "reward: [0.], action: [81], cum_reward: [0.94070786], max_reward: [0.94070786]\n",
      "Predictions:  [0.94070786]\n",
      "reward: [0.], action: [59], cum_reward: [0.94070786], max_reward: [0.94070786]\n",
      "Predictions:  [0.94070786]\n",
      "reward: [0.], action: [12], cum_reward: [0.94070786], max_reward: [0.94070786]\n",
      "Predictions:  [0.9281994]\n",
      "reward: [-0.01250845], action: [0], cum_reward: [0.9281994], max_reward: [0.9281994]\n",
      "Predictions:  [0.9281994]\n",
      "reward: [0.], action: [12], cum_reward: [0.9281994], max_reward: [0.9281994]\n",
      "Predictions:  [0.9281994]\n",
      "reward: [0.], action: [95], cum_reward: [0.9281994], max_reward: [0.9281994]\n",
      "Predictions:  [0.9281994]\n",
      "reward: [0.], action: [59], cum_reward: [0.9281994], max_reward: [0.9281994]\n",
      "Predictions:  [0.9281994]\n",
      "reward: [0.], action: [81], cum_reward: [0.9281994], max_reward: [0.9281994]\n",
      "Predictions:  [0.9281994]\n",
      "reward: [0.], action: [81], cum_reward: [0.9281994], max_reward: [0.9281994]\n",
      "Predictions:  [0.9281994]\n",
      "reward: [0.], action: [81], cum_reward: [0.9281994], max_reward: [0.9281994]\n",
      "Predictions:  [0.9281994]\n",
      "reward: [0.], action: [81], cum_reward: [0.9281994], max_reward: [0.9281994]\n",
      "Predictions:  [0.9281994]\n",
      "reward: [0.], action: [59], cum_reward: [0.9281994], max_reward: [0.9281994]\n",
      "Predictions:  [0.9281994]\n",
      "reward: [0.], action: [59], cum_reward: [0.9281994], max_reward: [0.9281994]\n",
      "Predictions:  [0.9281994]\n",
      "reward: [0.], action: [0], cum_reward: [0.9281994], max_reward: [0.9281994]\n",
      "Predictions:  [0.9281994]\n",
      "reward: [0.], action: [81], cum_reward: [0.9281994], max_reward: [0.9281994]\n",
      "Predictions:  [0.9281994]\n",
      "reward: [0.], action: [81], cum_reward: [0.9281994], max_reward: [0.9281994]\n",
      "Predictions:  [0.9281994]\n",
      "reward: [0.], action: [59], cum_reward: [0.9281994], max_reward: [0.9281994]\n",
      "reward: [0.], action: [112], cum_reward: [0.9281994], max_reward: [0.9281994]\n",
      "Episode 3,57: cum reward: [0.9281994], max reward: [0.9281994] action: [2] last action: [112]\n",
      "[[False False False False False False False False]]\n",
      "Predictions:  [0.93222666]\n",
      "Episode 3: Accuracy: 0.9322266578674316\n",
      "Episode 4\n",
      "Predictions:  [0.7375839]\n",
      "reward: [0.7375839], action: [0], cum_reward: [0.7375839], max_reward: 0\n",
      "Predictions:  [0.7640172]\n",
      "reward: [0.02643335], action: [2], cum_reward: [0.7640172], max_reward: [0.7640172]\n",
      "Predictions:  [0.924393]\n",
      "reward: [0.16037577], action: [69], cum_reward: [0.924393], max_reward: [0.924393]\n",
      "Predictions:  [0.7606017]\n",
      "reward: [-0.1637913], action: [39], cum_reward: [0.7606017], max_reward: [0.7606017]\n",
      "Predictions:  [0.7606017]\n",
      "reward: [0.], action: [95], cum_reward: [0.7606017], max_reward: [0.7606017]\n",
      "Predictions:  [0.92368615]\n",
      "reward: [0.16308445], action: [97], cum_reward: [0.92368615], max_reward: [0.92368615]\n",
      "Predictions:  [0.9441887]\n",
      "reward: [0.02050257], action: [12], cum_reward: [0.9441887], max_reward: [0.9441887]\n",
      "Predictions:  [0.9326378]\n",
      "reward: [-0.0115509], action: [11], cum_reward: [0.9326378], max_reward: [0.9326378]\n",
      "Predictions:  [0.9326378]\n",
      "reward: [0.], action: [95], cum_reward: [0.9326378], max_reward: [0.9326378]\n",
      "Predictions:  [0.9326378]\n",
      "reward: [0.], action: [59], cum_reward: [0.9326378], max_reward: [0.9326378]\n",
      "Predictions:  [0.9326378]\n",
      "reward: [0.], action: [95], cum_reward: [0.9326378], max_reward: [0.9326378]\n",
      "Predictions:  [0.9326378]\n",
      "reward: [0.], action: [39], cum_reward: [0.9326378], max_reward: [0.9326378]\n",
      "Predictions:  [0.9326378]\n",
      "reward: [0.], action: [12], cum_reward: [0.9326378], max_reward: [0.9326378]\n",
      "Predictions:  [0.9326378]\n",
      "reward: [0.], action: [12], cum_reward: [0.9326378], max_reward: [0.9326378]\n",
      "Predictions:  [0.9326378]\n",
      "reward: [0.], action: [95], cum_reward: [0.9326378], max_reward: [0.9326378]\n",
      "Predictions:  [0.9326378]\n",
      "reward: [0.], action: [59], cum_reward: [0.9326378], max_reward: [0.9326378]\n",
      "Predictions:  [0.9326378]\n",
      "reward: [0.], action: [12], cum_reward: [0.9326378], max_reward: [0.9326378]\n",
      "Predictions:  [0.9326378]\n",
      "reward: [0.], action: [97], cum_reward: [0.9326378], max_reward: [0.9326378]\n",
      "Predictions:  [0.9326378]\n",
      "reward: [0.], action: [12], cum_reward: [0.9326378], max_reward: [0.9326378]\n",
      "reward: [0.], action: [112], cum_reward: [0.9326378], max_reward: [0.9326378]\n",
      "Episode 4,19: cum reward: [0.9326378], max reward: [0.9326378] action: [0] last action: [112]\n",
      "[[False False False False False False False False]]\n",
      "Predictions:  [0.9552871]\n",
      "Episode 4: Accuracy: 0.9552870988845825\n",
      "Episode 5\n",
      "Predictions:  [0.7372422]\n",
      "reward: [0.7372422], action: [110], cum_reward: [0.7372422], max_reward: 0\n",
      "Predictions:  [0.7372422]\n",
      "reward: [0.], action: [34], cum_reward: [0.7372422], max_reward: [0.7372422]\n",
      "Predictions:  [0.76714563]\n",
      "reward: [0.02990341], action: [39], cum_reward: [0.76714563], max_reward: [0.76714563]\n",
      "Predictions:  [0.76714563]\n",
      "reward: [0.], action: [57], cum_reward: [0.76714563], max_reward: [0.76714563]\n",
      "Predictions:  [0.76714563]\n",
      "reward: [0.], action: [39], cum_reward: [0.76714563], max_reward: [0.76714563]\n",
      "Predictions:  [0.7704379]\n",
      "reward: [0.00329226], action: [11], cum_reward: [0.7704379], max_reward: [0.7704379]\n",
      "Predictions:  [0.7704379]\n",
      "reward: [0.], action: [59], cum_reward: [0.7704379], max_reward: [0.7704379]\n",
      "Predictions:  [0.7704379]\n",
      "reward: [0.], action: [103], cum_reward: [0.7704379], max_reward: [0.7704379]\n",
      "Predictions:  [0.7704379]\n",
      "reward: [0.], action: [39], cum_reward: [0.7704379], max_reward: [0.7704379]\n",
      "Predictions:  [0.7704379]\n",
      "reward: [0.], action: [65], cum_reward: [0.7704379], max_reward: [0.7704379]\n",
      "Predictions:  [0.7704379]\n",
      "reward: [0.], action: [12], cum_reward: [0.7704379], max_reward: [0.7704379]\n",
      "Predictions:  [0.7704379]\n",
      "reward: [0.], action: [81], cum_reward: [0.7704379], max_reward: [0.7704379]\n",
      "Predictions:  [0.7704379]\n",
      "reward: [0.], action: [12], cum_reward: [0.7704379], max_reward: [0.7704379]\n",
      "Predictions:  [0.7704379]\n",
      "reward: [0.], action: [59], cum_reward: [0.7704379], max_reward: [0.7704379]\n",
      "Predictions:  [0.7743999]\n",
      "reward: [0.00396198], action: [97], cum_reward: [0.7743999], max_reward: [0.7743999]\n",
      "Predictions:  [0.7743999]\n",
      "reward: [0.], action: [39], cum_reward: [0.7743999], max_reward: [0.7743999]\n",
      "Predictions:  [0.7704379]\n",
      "reward: [-0.00396198], action: [65], cum_reward: [0.7704379], max_reward: [0.7704379]\n",
      "Predictions:  [0.7704379]\n",
      "reward: [0.], action: [12], cum_reward: [0.7704379], max_reward: [0.7704379]\n",
      "Predictions:  [0.7704379]\n",
      "reward: [0.], action: [59], cum_reward: [0.7704379], max_reward: [0.7704379]\n",
      "Predictions:  [0.7704379]\n",
      "reward: [0.], action: [81], cum_reward: [0.7704379], max_reward: [0.7704379]\n",
      "Predictions:  [0.7704379]\n",
      "reward: [0.], action: [95], cum_reward: [0.7704379], max_reward: [0.7704379]\n",
      "Predictions:  [0.7704379]\n",
      "reward: [0.], action: [81], cum_reward: [0.7704379], max_reward: [0.7704379]\n",
      "Predictions:  [0.7704379]\n",
      "reward: [0.], action: [39], cum_reward: [0.7704379], max_reward: [0.7704379]\n",
      "Predictions:  [0.7704379]\n",
      "reward: [0.], action: [12], cum_reward: [0.7704379], max_reward: [0.7704379]\n",
      "Predictions:  [0.7704379]\n",
      "reward: [0.], action: [12], cum_reward: [0.7704379], max_reward: [0.7704379]\n",
      "Predictions:  [0.7704379]\n",
      "reward: [0.], action: [95], cum_reward: [0.7704379], max_reward: [0.7704379]\n",
      "Predictions:  [0.7704379]\n",
      "reward: [0.], action: [59], cum_reward: [0.7704379], max_reward: [0.7704379]\n",
      "Predictions:  [0.7704379]\n",
      "reward: [0.], action: [59], cum_reward: [0.7704379], max_reward: [0.7704379]\n",
      "Predictions:  [0.7704379]\n",
      "reward: [0.], action: [81], cum_reward: [0.7704379], max_reward: [0.7704379]\n",
      "Predictions:  [0.7704379]\n",
      "reward: [0.], action: [12], cum_reward: [0.7704379], max_reward: [0.7704379]\n",
      "Predictions:  [0.7704379]\n",
      "reward: [0.], action: [81], cum_reward: [0.7704379], max_reward: [0.7704379]\n",
      "Predictions:  [0.7704379]\n",
      "reward: [0.], action: [59], cum_reward: [0.7704379], max_reward: [0.7704379]\n",
      "Predictions:  [0.7704379]\n",
      "reward: [0.], action: [59], cum_reward: [0.7704379], max_reward: [0.7704379]\n",
      "Predictions:  [0.7743999]\n",
      "reward: [0.00396198], action: [97], cum_reward: [0.7743999], max_reward: [0.7743999]\n",
      "Predictions:  [0.7743999]\n",
      "reward: [0.], action: [39], cum_reward: [0.7743999], max_reward: [0.7743999]\n",
      "Predictions:  [0.7704379]\n",
      "reward: [-0.00396198], action: [81], cum_reward: [0.7704379], max_reward: [0.7704379]\n",
      "Predictions:  [0.7704379]\n",
      "reward: [0.], action: [81], cum_reward: [0.7704379], max_reward: [0.7704379]\n",
      "Predictions:  [0.7704379]\n",
      "reward: [0.], action: [59], cum_reward: [0.7704379], max_reward: [0.7704379]\n",
      "reward: [0.], action: [112], cum_reward: [0.7704379], max_reward: [0.7704379]\n",
      "Episode 5,38: cum reward: [0.7704379], max reward: [0.7704379] action: [110] last action: [112]\n",
      "[[False False False False False False False False]]\n",
      "Predictions:  [0.7889947]\n",
      "Episode 5: Accuracy: 0.7889947295188904\n",
      "Episode 6\n",
      "Predictions:  [0.9281994]\n",
      "reward: [0.9281994], action: [95], cum_reward: [0.9281994], max_reward: 0\n",
      "Predictions:  [0.9281994]\n",
      "reward: [0.], action: [39], cum_reward: [0.9281994], max_reward: [0.9281994]\n",
      "Predictions:  [0.9281994]\n",
      "reward: [0.], action: [95], cum_reward: [0.9281994], max_reward: [0.9281994]\n",
      "Predictions:  [0.9281994]\n",
      "reward: [0.], action: [97], cum_reward: [0.9281994], max_reward: [0.9281994]\n",
      "Predictions:  [0.9272683]\n",
      "reward: [-0.00093108], action: [12], cum_reward: [0.9272683], max_reward: [0.9272683]\n",
      "Predictions:  [0.93222666]\n",
      "reward: [0.00495833], action: [11], cum_reward: [0.93222666], max_reward: [0.93222666]\n",
      "Predictions:  [0.93222666]\n",
      "reward: [0.], action: [95], cum_reward: [0.93222666], max_reward: [0.93222666]\n",
      "Predictions:  [0.93222666]\n",
      "reward: [0.], action: [59], cum_reward: [0.93222666], max_reward: [0.93222666]\n",
      "Predictions:  [0.93222666]\n",
      "reward: [0.], action: [95], cum_reward: [0.93222666], max_reward: [0.93222666]\n",
      "Predictions:  [0.93222666]\n",
      "reward: [0.], action: [65], cum_reward: [0.93222666], max_reward: [0.93222666]\n",
      "Predictions:  [0.93222666]\n",
      "reward: [0.], action: [12], cum_reward: [0.93222666], max_reward: [0.93222666]\n",
      "Predictions:  [0.93222666]\n",
      "reward: [0.], action: [81], cum_reward: [0.93222666], max_reward: [0.93222666]\n",
      "Predictions:  [0.93222666]\n",
      "reward: [0.], action: [81], cum_reward: [0.93222666], max_reward: [0.93222666]\n",
      "Predictions:  [0.93222666]\n",
      "reward: [0.], action: [59], cum_reward: [0.93222666], max_reward: [0.93222666]\n",
      "Predictions:  [0.93222666]\n",
      "reward: [0.], action: [59], cum_reward: [0.93222666], max_reward: [0.93222666]\n",
      "Predictions:  [0.93222666]\n",
      "reward: [0.], action: [39], cum_reward: [0.93222666], max_reward: [0.93222666]\n",
      "Predictions:  [0.93222666]\n",
      "reward: [0.], action: [81], cum_reward: [0.93222666], max_reward: [0.93222666]\n",
      "Predictions:  [0.93222666]\n",
      "reward: [0.], action: [81], cum_reward: [0.93222666], max_reward: [0.93222666]\n",
      "Predictions:  [0.93222666]\n",
      "reward: [0.], action: [59], cum_reward: [0.93222666], max_reward: [0.93222666]\n",
      "Predictions:  [0.93222666]\n",
      "reward: [0.], action: [12], cum_reward: [0.93222666], max_reward: [0.93222666]\n",
      "Predictions:  [0.93222666]\n",
      "reward: [0.], action: [12], cum_reward: [0.93222666], max_reward: [0.93222666]\n",
      "Predictions:  [0.93222666]\n",
      "reward: [0.], action: [59], cum_reward: [0.93222666], max_reward: [0.93222666]\n",
      "Predictions:  [0.93222666]\n",
      "reward: [0.], action: [81], cum_reward: [0.93222666], max_reward: [0.93222666]\n",
      "Predictions:  [0.93222666]\n",
      "reward: [0.], action: [59], cum_reward: [0.93222666], max_reward: [0.93222666]\n",
      "Predictions:  [0.93222666]\n",
      "reward: [0.], action: [81], cum_reward: [0.93222666], max_reward: [0.93222666]\n",
      "Predictions:  [0.93222666]\n",
      "reward: [0.], action: [81], cum_reward: [0.93222666], max_reward: [0.93222666]\n",
      "Predictions:  [0.93222666]\n",
      "reward: [0.], action: [81], cum_reward: [0.93222666], max_reward: [0.93222666]\n",
      "Predictions:  [0.93222666]\n",
      "reward: [0.], action: [59], cum_reward: [0.93222666], max_reward: [0.93222666]\n",
      "Predictions:  [0.93222666]\n",
      "reward: [0.], action: [59], cum_reward: [0.93222666], max_reward: [0.93222666]\n",
      "Predictions:  [0.93222666]\n",
      "reward: [0.], action: [12], cum_reward: [0.93222666], max_reward: [0.93222666]\n",
      "Predictions:  [0.93222666]\n",
      "reward: [0.], action: [95], cum_reward: [0.93222666], max_reward: [0.93222666]\n",
      "Predictions:  [0.93222666]\n",
      "reward: [0.], action: [59], cum_reward: [0.93222666], max_reward: [0.93222666]\n",
      "Predictions:  [0.93222666]\n",
      "reward: [0.], action: [59], cum_reward: [0.93222666], max_reward: [0.93222666]\n",
      "reward: [0.], action: [112], cum_reward: [0.93222666], max_reward: [0.93222666]\n",
      "Episode 6,33: cum reward: [0.93222666], max reward: [0.93222666] action: [95] last action: [112]\n",
      "[[False False False False False False False False]]\n",
      "Predictions:  [0.737657]\n",
      "Episode 6: Accuracy: 0.7376570105552673\n",
      "Episode 7\n",
      "Predictions:  [0.9281994]\n",
      "reward: [0.9281994], action: [110], cum_reward: [0.9281994], max_reward: 0\n",
      "Predictions:  [0.9250929]\n",
      "reward: [-0.00310653], action: [34], cum_reward: [0.9250929], max_reward: [0.9250929]\n",
      "Predictions:  [0.9326378]\n",
      "reward: [0.00754493], action: [39], cum_reward: [0.9326378], max_reward: [0.9326378]\n",
      "Predictions:  [0.9238914]\n",
      "reward: [-0.00874639], action: [57], cum_reward: [0.9238914], max_reward: [0.9238914]\n",
      "Predictions:  [0.924393]\n",
      "reward: [0.00050157], action: [12], cum_reward: [0.924393], max_reward: [0.924393]\n",
      "Predictions:  [0.924393]\n",
      "reward: [0.], action: [92], cum_reward: [0.924393], max_reward: [0.924393]\n",
      "Predictions:  [0.924393]\n",
      "reward: [0.], action: [39], cum_reward: [0.924393], max_reward: [0.924393]\n",
      "Predictions:  [0.924393]\n",
      "reward: [0.], action: [39], cum_reward: [0.924393], max_reward: [0.924393]\n",
      "Predictions:  [0.924393]\n",
      "reward: [0.], action: [97], cum_reward: [0.924393], max_reward: [0.924393]\n",
      "Predictions:  [0.924393]\n",
      "reward: [0.], action: [39], cum_reward: [0.924393], max_reward: [0.924393]\n",
      "Predictions:  [0.924393]\n",
      "reward: [0.], action: [12], cum_reward: [0.924393], max_reward: [0.924393]\n",
      "Predictions:  [0.924393]\n",
      "reward: [0.], action: [81], cum_reward: [0.924393], max_reward: [0.924393]\n",
      "Predictions:  [0.924393]\n",
      "reward: [0.], action: [39], cum_reward: [0.924393], max_reward: [0.924393]\n",
      "Predictions:  [0.924393]\n",
      "reward: [0.], action: [95], cum_reward: [0.924393], max_reward: [0.924393]\n",
      "Predictions:  [0.924393]\n",
      "reward: [0.], action: [12], cum_reward: [0.924393], max_reward: [0.924393]\n",
      "Predictions:  [0.924393]\n",
      "reward: [0.], action: [12], cum_reward: [0.924393], max_reward: [0.924393]\n",
      "Predictions:  [0.924393]\n",
      "reward: [0.], action: [81], cum_reward: [0.924393], max_reward: [0.924393]\n",
      "Predictions:  [0.924393]\n",
      "reward: [0.], action: [95], cum_reward: [0.924393], max_reward: [0.924393]\n",
      "reward: [0.], action: [112], cum_reward: [0.924393], max_reward: [0.924393]\n",
      "Episode 7,18: cum reward: [0.924393], max reward: [0.924393] action: [110] last action: [112]\n",
      "[[False False False False False False False False]]\n",
      "Predictions:  [0.78391343]\n",
      "Episode 7: Accuracy: 0.7839134335517883\n",
      "Episode 8\n",
      "Predictions:  [0.9189534]\n",
      "reward: [0.9189534], action: [0], cum_reward: [0.9189534], max_reward: 0\n",
      "Predictions:  [0.9189534]\n",
      "reward: [0.], action: [0], cum_reward: [0.9189534], max_reward: [0.9189534]\n",
      "Predictions:  [0.9353243]\n",
      "reward: [0.01637089], action: [69], cum_reward: [0.9353243], max_reward: [0.9353243]\n",
      "Predictions:  [0.9281994]\n",
      "reward: [-0.0071249], action: [39], cum_reward: [0.9281994], max_reward: [0.9281994]\n",
      "Predictions:  [0.9281994]\n",
      "reward: [0.], action: [95], cum_reward: [0.9281994], max_reward: [0.9281994]\n",
      "Predictions:  [0.9281994]\n",
      "reward: [0.], action: [97], cum_reward: [0.9281994], max_reward: [0.9281994]\n",
      "Predictions:  [0.92617047]\n",
      "reward: [-0.00202894], action: [12], cum_reward: [0.92617047], max_reward: [0.92617047]\n",
      "Predictions:  [0.9285394]\n",
      "reward: [0.00236893], action: [11], cum_reward: [0.9285394], max_reward: [0.9285394]\n",
      "Predictions:  [0.9285394]\n",
      "reward: [0.], action: [95], cum_reward: [0.9285394], max_reward: [0.9285394]\n",
      "Predictions:  [0.9285394]\n",
      "reward: [0.], action: [59], cum_reward: [0.9285394], max_reward: [0.9285394]\n",
      "Predictions:  [0.9285394]\n",
      "reward: [0.], action: [95], cum_reward: [0.9285394], max_reward: [0.9285394]\n",
      "Predictions:  [0.9285394]\n",
      "reward: [0.], action: [39], cum_reward: [0.9285394], max_reward: [0.9285394]\n",
      "Predictions:  [0.9285394]\n",
      "reward: [0.], action: [12], cum_reward: [0.9285394], max_reward: [0.9285394]\n",
      "Predictions:  [0.9285394]\n",
      "reward: [0.], action: [12], cum_reward: [0.9285394], max_reward: [0.9285394]\n",
      "Predictions:  [0.9285394]\n",
      "reward: [0.], action: [95], cum_reward: [0.9285394], max_reward: [0.9285394]\n",
      "Predictions:  [0.9285394]\n",
      "reward: [0.], action: [59], cum_reward: [0.9285394], max_reward: [0.9285394]\n",
      "Predictions:  [0.9285394]\n",
      "reward: [0.], action: [12], cum_reward: [0.9285394], max_reward: [0.9285394]\n",
      "Predictions:  [0.9285394]\n",
      "reward: [0.], action: [97], cum_reward: [0.9285394], max_reward: [0.9285394]\n",
      "Predictions:  [0.9285394]\n",
      "reward: [0.], action: [12], cum_reward: [0.9285394], max_reward: [0.9285394]\n",
      "reward: [0.], action: [112], cum_reward: [0.9285394], max_reward: [0.9285394]\n",
      "Episode 8,19: cum reward: [0.9285394], max reward: [0.9285394] action: [0] last action: [112]\n",
      "[[False False False False False False False False]]\n",
      "Predictions:  [0.75625926]\n",
      "Episode 8: Accuracy: 0.7562592625617981\n",
      "Episode 9\n",
      "Predictions:  [0.9238914]\n",
      "reward: [0.9238914], action: [21], cum_reward: [0.9238914], max_reward: 0\n",
      "Predictions:  [0.7372422]\n",
      "reward: [-0.1866492], action: [110], cum_reward: [0.7372422], max_reward: [0.7372422]\n",
      "Predictions:  [0.7725494]\n",
      "reward: [0.03530717], action: [39], cum_reward: [0.7725494], max_reward: [0.7725494]\n",
      "reward: [0.], action: [112], cum_reward: [0.7725494], max_reward: [0.7725494]\n",
      "Episode 9,3: cum reward: [0.7725494], max reward: [0.7725494] action: [21] last action: [112]\n",
      "[[False False False False False False False False]]\n",
      "Predictions:  [0.924393]\n",
      "Episode 9: Accuracy: 0.9243929982185364\n",
      "Episode 10\n",
      "Predictions:  [0.9325972]\n",
      "reward: [0.9325972], action: [95], cum_reward: [0.9325972], max_reward: 0\n",
      "Predictions:  [0.9325972]\n",
      "reward: [0.], action: [84], cum_reward: [0.9325972], max_reward: [0.9325972]\n",
      "Predictions:  [0.9325972]\n",
      "reward: [0.], action: [39], cum_reward: [0.9325972], max_reward: [0.9325972]\n",
      "Predictions:  [0.9325861]\n",
      "reward: [-1.1146069e-05], action: [97], cum_reward: [0.9325861], max_reward: [0.9325861]\n",
      "Predictions:  [0.9325861]\n",
      "reward: [0.], action: [12], cum_reward: [0.9325861], max_reward: [0.9325861]\n",
      "Predictions:  [0.9325861]\n",
      "reward: [0.], action: [39], cum_reward: [0.9325861], max_reward: [0.9325861]\n",
      "Predictions:  [0.9325861]\n",
      "reward: [0.], action: [84], cum_reward: [0.9325861], max_reward: [0.9325861]\n",
      "Predictions:  [0.9325861]\n",
      "reward: [0.], action: [39], cum_reward: [0.9325861], max_reward: [0.9325861]\n",
      "Predictions:  [0.9325861]\n",
      "reward: [0.], action: [97], cum_reward: [0.9325861], max_reward: [0.9325861]\n",
      "Predictions:  [0.9325861]\n",
      "reward: [0.], action: [63], cum_reward: [0.9325861], max_reward: [0.9325861]\n",
      "Predictions:  [0.9325861]\n",
      "reward: [0.], action: [12], cum_reward: [0.9325861], max_reward: [0.9325861]\n",
      "Predictions:  [0.9325861]\n",
      "reward: [0.], action: [81], cum_reward: [0.9325861], max_reward: [0.9325861]\n",
      "Predictions:  [0.9325861]\n",
      "reward: [0.], action: [95], cum_reward: [0.9325861], max_reward: [0.9325861]\n",
      "Predictions:  [0.9325861]\n",
      "reward: [0.], action: [59], cum_reward: [0.9325861], max_reward: [0.9325861]\n",
      "Predictions:  [0.9325861]\n",
      "reward: [0.], action: [81], cum_reward: [0.9325861], max_reward: [0.9325861]\n",
      "Predictions:  [0.9325861]\n",
      "reward: [0.], action: [81], cum_reward: [0.9325861], max_reward: [0.9325861]\n",
      "Predictions:  [0.9325861]\n",
      "reward: [0.], action: [81], cum_reward: [0.9325861], max_reward: [0.9325861]\n",
      "Predictions:  [0.9325861]\n",
      "reward: [0.], action: [81], cum_reward: [0.9325861], max_reward: [0.9325861]\n",
      "Predictions:  [0.9325861]\n",
      "reward: [0.], action: [59], cum_reward: [0.9325861], max_reward: [0.9325861]\n",
      "Predictions:  [0.9325861]\n",
      "reward: [0.], action: [12], cum_reward: [0.9325861], max_reward: [0.9325861]\n",
      "Predictions:  [0.9325861]\n",
      "reward: [0.], action: [12], cum_reward: [0.9325861], max_reward: [0.9325861]\n",
      "Predictions:  [0.9325861]\n",
      "reward: [0.], action: [59], cum_reward: [0.9325861], max_reward: [0.9325861]\n",
      "Predictions:  [0.9325861]\n",
      "reward: [0.], action: [59], cum_reward: [0.9325861], max_reward: [0.9325861]\n",
      "Predictions:  [0.9325861]\n",
      "reward: [0.], action: [81], cum_reward: [0.9325861], max_reward: [0.9325861]\n",
      "Predictions:  [0.9325861]\n",
      "reward: [0.], action: [81], cum_reward: [0.9325861], max_reward: [0.9325861]\n",
      "Predictions:  [0.9325861]\n",
      "reward: [0.], action: [81], cum_reward: [0.9325861], max_reward: [0.9325861]\n",
      "Predictions:  [0.9325861]\n",
      "reward: [0.], action: [81], cum_reward: [0.9325861], max_reward: [0.9325861]\n",
      "Predictions:  [0.9325861]\n",
      "reward: [0.], action: [81], cum_reward: [0.9325861], max_reward: [0.9325861]\n",
      "Predictions:  [0.9325861]\n",
      "reward: [0.], action: [59], cum_reward: [0.9325861], max_reward: [0.9325861]\n",
      "Predictions:  [0.9325861]\n",
      "reward: [0.], action: [59], cum_reward: [0.9325861], max_reward: [0.9325861]\n",
      "Predictions:  [0.9325861]\n",
      "reward: [0.], action: [12], cum_reward: [0.9325861], max_reward: [0.9325861]\n",
      "Predictions:  [0.9325861]\n",
      "reward: [0.], action: [59], cum_reward: [0.9325861], max_reward: [0.9325861]\n",
      "Predictions:  [0.9325861]\n",
      "reward: [0.], action: [59], cum_reward: [0.9325861], max_reward: [0.9325861]\n",
      "Predictions:  [0.9325861]\n",
      "reward: [0.], action: [59], cum_reward: [0.9325861], max_reward: [0.9325861]\n",
      "Predictions:  [0.9325861]\n",
      "reward: [0.], action: [81], cum_reward: [0.9325861], max_reward: [0.9325861]\n",
      "Predictions:  [0.9325861]\n",
      "reward: [0.], action: [81], cum_reward: [0.9325861], max_reward: [0.9325861]\n",
      "Predictions:  [0.9325861]\n",
      "reward: [0.], action: [81], cum_reward: [0.9325861], max_reward: [0.9325861]\n",
      "Predictions:  [0.9325861]\n",
      "reward: [0.], action: [81], cum_reward: [0.9325861], max_reward: [0.9325861]\n",
      "Predictions:  [0.9325861]\n",
      "reward: [0.], action: [81], cum_reward: [0.9325861], max_reward: [0.9325861]\n",
      "reward: [0.], action: [112], cum_reward: [0.9325861], max_reward: [0.9325861]\n",
      "Episode 10,39: cum reward: [0.9325861], max reward: [0.9325861] action: [95] last action: [112]\n",
      "[[False False False False False False False False]]\n",
      "Predictions:  [0.7372422]\n",
      "Episode 10: Accuracy: 0.7372422218322754\n",
      "Episode 11\n",
      "Predictions:  [0.7889947]\n",
      "reward: [0.7889947], action: [78], cum_reward: [0.7889947], max_reward: 0\n",
      "Predictions:  [0.75625926]\n",
      "reward: [-0.03273547], action: [69], cum_reward: [0.75625926], max_reward: [0.75625926]\n",
      "Predictions:  [0.73496425]\n",
      "reward: [-0.02129501], action: [39], cum_reward: [0.73496425], max_reward: [0.73496425]\n",
      "Predictions:  [0.734817]\n",
      "reward: [-0.00014722], action: [97], cum_reward: [0.734817], max_reward: [0.734817]\n",
      "Predictions:  [0.7640172]\n",
      "reward: [0.0292002], action: [12], cum_reward: [0.7640172], max_reward: [0.7640172]\n",
      "Predictions:  [0.7640172]\n",
      "reward: [0.], action: [39], cum_reward: [0.7640172], max_reward: [0.7640172]\n",
      "Predictions:  [0.7725494]\n",
      "reward: [0.00853217], action: [59], cum_reward: [0.7725494], max_reward: [0.7725494]\n",
      "Predictions:  [0.7725494]\n",
      "reward: [0.], action: [59], cum_reward: [0.7725494], max_reward: [0.7725494]\n",
      "Predictions:  [0.7725494]\n",
      "reward: [0.], action: [97], cum_reward: [0.7725494], max_reward: [0.7725494]\n",
      "Predictions:  [0.7725494]\n",
      "reward: [0.], action: [39], cum_reward: [0.7725494], max_reward: [0.7725494]\n",
      "Predictions:  [0.7725494]\n",
      "reward: [0.], action: [95], cum_reward: [0.7725494], max_reward: [0.7725494]\n",
      "Predictions:  [0.77341604]\n",
      "reward: [0.00086665], action: [81], cum_reward: [0.77341604], max_reward: [0.77341604]\n",
      "Predictions:  [0.77341604]\n",
      "reward: [0.], action: [59], cum_reward: [0.77341604], max_reward: [0.77341604]\n",
      "Predictions:  [0.77341604]\n",
      "reward: [0.], action: [12], cum_reward: [0.77341604], max_reward: [0.77341604]\n",
      "Predictions:  [0.77341604]\n",
      "reward: [0.], action: [12], cum_reward: [0.77341604], max_reward: [0.77341604]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m max_state \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_accuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfind_max\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/columbia/Courses/Spring2024/HPML/Project/Analog_NAS/env/RLTrainer.py:174\u001b[0m, in \u001b[0;36mTrainer.evaluate_accuracy\u001b[0;34m(self, num_episodes, num_steps, find_max)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_steps):\n\u001b[1;32m    173\u001b[0m     action, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpredict(obs, deterministic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 174\u001b[0m     obs, rewards, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m     cum_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m rewards\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    177\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreward: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrewards\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, action: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maction\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, cum_reward: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcum_reward\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, max_reward: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_reward\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    178\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.11/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:206\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.11/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:58\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# Avoid circular imports\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[0;32m---> 58\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews[env_idx], terminated, truncated, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;66;03m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx] \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.11/site-packages/stable_baselines3/common/monitor.py:94\u001b[0m, in \u001b[0;36mMonitor.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneeds_reset:\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTried to step environment that needs reset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 94\u001b[0m observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewards\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mfloat\u001b[39m(reward))\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated:\n",
      "File \u001b[0;32m~/Desktop/columbia/Courses/Spring2024/HPML/Project/Analog_NAS/env/VQVAE_environment.py:194\u001b[0m, in \u001b[0;36mVQVAE_Env.step\u001b[0;34m(self, action_input)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;66;03m# Calculate the reward\u001b[39;00m\n\u001b[0;32m--> 194\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalculate_reward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# Check if the maximum number of actions has been reached\u001b[39;00m\n\u001b[1;32m    197\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_count \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_allowed_actions\n",
      "File \u001b[0;32m~/Desktop/columbia/Courses/Spring2024/HPML/Project/Analog_NAS/env/VQVAE_environment.py:255\u001b[0m, in \u001b[0;36mVQVAE_Env.calculate_reward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m decoded_state\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    254\u001b[0m     decoded_state \u001b[38;5;241m=\u001b[39m decoded_state\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 255\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msurrogate_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoded_state\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calculate the accuracy\u001b[39;00m\n\u001b[1;32m    258\u001b[0m reward \u001b[38;5;241m=\u001b[39m accuracy \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprevious_accuracy  \u001b[38;5;66;03m# Reward is the change in accuracy\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprevious_accuracy \u001b[38;5;241m=\u001b[39m accuracy  \u001b[38;5;66;03m# Update the previous accuracy\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/columbia/Courses/Spring2024/HPML/Project/Analog_NAS/env/SurrogateModel.py:80\u001b[0m, in \u001b[0;36mSurrogateModel.evaluate\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m---> 80\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;66;03m# Convert the input to DMatrix, which is the internal data structure used by XGBoost\u001b[39;00m\n\u001b[1;32m     83\u001b[0m     dtest \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mDMatrix(X, enable_categorical\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Desktop/columbia/Courses/Spring2024/HPML/Project/Analog_NAS/env/SurrogateModel.py:103\u001b[0m, in \u001b[0;36mSurrogateModel.clip_values\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]):\n\u001b[1;32m     99\u001b[0m     clamped_data[:, i] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mclamp(\n\u001b[1;32m    100\u001b[0m         rounded_data[:, i], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_values[i], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_values[i]\n\u001b[1;32m    101\u001b[0m     )\n\u001b[0;32m--> 103\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorrect_data_format\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclamped_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/Desktop/columbia/Courses/Spring2024/HPML/Project/Analog_NAS/env/SurrogateModel.py:123\u001b[0m, in \u001b[0;36mSurrogateModel.correct_data_format\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    116\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconvblock2\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    117\u001b[0m     df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconvblock2\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_mapping)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    118\u001b[0m )\n\u001b[1;32m    119\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconvblock3\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    120\u001b[0m     df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconvblock3\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_mapping)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    121\u001b[0m )\n\u001b[1;32m    122\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconvblock4\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 123\u001b[0m     \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconvblock4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue_mapping\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcategory\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m )\n\u001b[1;32m    125\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconvblock5\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    126\u001b[0m     df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconvblock5\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_mapping)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    127\u001b[0m )\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.11/site-packages/pandas/core/generic.py:6640\u001b[0m, in \u001b[0;36mNDFrame.astype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m   6634\u001b[0m     results \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   6635\u001b[0m         ser\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy, errors\u001b[38;5;241m=\u001b[39merrors) \u001b[38;5;28;01mfor\u001b[39;00m _, ser \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   6636\u001b[0m     ]\n\u001b[1;32m   6638\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6639\u001b[0m     \u001b[38;5;66;03m# else, only a single dtype is given\u001b[39;00m\n\u001b[0;32m-> 6640\u001b[0m     new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6641\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(new_data, axes\u001b[38;5;241m=\u001b[39mnew_data\u001b[38;5;241m.\u001b[39maxes)\n\u001b[1;32m   6642\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mastype\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.11/site-packages/pandas/core/internals/managers.py:430\u001b[0m, in \u001b[0;36mBaseBlockManager.astype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m using_copy_on_write():\n\u001b[1;32m    428\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 430\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mastype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43musing_cow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musing_copy_on_write\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.11/site-packages/pandas/core/internals/managers.py:363\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[0;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m         applied \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mapply(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 363\u001b[0m         applied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    364\u001b[0m     result_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(applied, result_blocks)\n\u001b[1;32m    366\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mfrom_blocks(result_blocks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.11/site-packages/pandas/core/internals/blocks.py:758\u001b[0m, in \u001b[0;36mBlock.astype\u001b[0;34m(self, dtype, copy, errors, using_cow, squeeze)\u001b[0m\n\u001b[1;32m    755\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan not squeeze with more than one column.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    756\u001b[0m     values \u001b[38;5;241m=\u001b[39m values[\u001b[38;5;241m0\u001b[39m, :]  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[0;32m--> 758\u001b[0m new_values \u001b[38;5;241m=\u001b[39m \u001b[43mastype_array_safe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    760\u001b[0m new_values \u001b[38;5;241m=\u001b[39m maybe_coerce_values(new_values)\n\u001b[1;32m    762\u001b[0m refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.11/site-packages/pandas/core/dtypes/astype.py:237\u001b[0m, in \u001b[0;36mastype_array_safe\u001b[0;34m(values, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    234\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtype\u001b[38;5;241m.\u001b[39mnumpy_dtype\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 237\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m \u001b[43mastype_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;66;03m# e.g. _astype_nansafe can fail on object-dtype of strings\u001b[39;00m\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;66;03m#  trying to convert to float\u001b[39;00m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.11/site-packages/pandas/core/dtypes/astype.py:182\u001b[0m, in \u001b[0;36mastype_array\u001b[0;34m(values, dtype, copy)\u001b[0m\n\u001b[1;32m    179\u001b[0m     values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 182\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43m_astype_nansafe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;66;03m# in pandas we don't store numpy str dtypes, so convert to object\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, np\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(values\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.11/site-packages/pandas/core/dtypes/astype.py:80\u001b[0m, in \u001b[0;36m_astype_nansafe\u001b[0;34m(arr, dtype, copy, skipna)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# dispatch on extension dtype if needed\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, ExtensionDtype):\n\u001b[0;32m---> 80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdtype\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstruct_array_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, np\u001b[38;5;241m.\u001b[39mdtype):  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype must be np.dtype or ExtensionDtype\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.11/site-packages/pandas/core/arrays/categorical.py:521\u001b[0m, in \u001b[0;36mCategorical._from_sequence\u001b[0;34m(cls, scalars, dtype, copy)\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_from_sequence\u001b[39m(\n\u001b[1;32m    519\u001b[0m     \u001b[38;5;28mcls\u001b[39m, scalars, \u001b[38;5;241m*\u001b[39m, dtype: Dtype \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, copy: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    520\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[0;32m--> 521\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mscalars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.11/site-packages/pandas/core/arrays/categorical.py:473\u001b[0m, in \u001b[0;36mCategorical.__init__\u001b[0;34m(self, values, categories, ordered, dtype, fastpath, copy)\u001b[0m\n\u001b[1;32m    466\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    467\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not ordered, please \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    468\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexplicitly specify the categories order \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    469\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mby passing in a categories argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    470\u001b[0m                 ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m    472\u001b[0m         \u001b[38;5;66;03m# we're inferring from values\u001b[39;00m\n\u001b[0;32m--> 473\u001b[0m         dtype \u001b[38;5;241m=\u001b[39m \u001b[43mCategoricalDtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcategories\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mordered\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(values\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype):\n\u001b[1;32m    476\u001b[0m     old_codes \u001b[38;5;241m=\u001b[39m extract_array(values)\u001b[38;5;241m.\u001b[39m_codes\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.11/site-packages/pandas/core/dtypes/dtypes.py:221\u001b[0m, in \u001b[0;36mCategoricalDtype.__init__\u001b[0;34m(self, categories, ordered)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, categories\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, ordered: Ordered \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 221\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_finalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcategories\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mordered\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfastpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.11/site-packages/pandas/core/dtypes/dtypes.py:378\u001b[0m, in \u001b[0;36mCategoricalDtype._finalize\u001b[0;34m(self, categories, ordered, fastpath)\u001b[0m\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidate_ordered(ordered)\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m categories \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 378\u001b[0m     categories \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_categories\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcategories\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfastpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfastpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_categories \u001b[38;5;241m=\u001b[39m categories\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ordered \u001b[38;5;241m=\u001b[39m ordered\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.11/site-packages/pandas/core/dtypes/dtypes.py:572\u001b[0m, in \u001b[0;36mCategoricalDtype.validate_categories\u001b[0;34m(categories, fastpath)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    569\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParameter \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategories\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be list-like, was \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(categories)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m     )\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(categories, ABCIndex):\n\u001b[0;32m--> 572\u001b[0m     categories \u001b[38;5;241m=\u001b[39m \u001b[43mIndex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcategories\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtupleize_cols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fastpath:\n\u001b[1;32m    575\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m categories\u001b[38;5;241m.\u001b[39mhasnans:\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.11/site-packages/pandas/core/indexes/base.py:680\u001b[0m, in \u001b[0;36mIndex._with_infer\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_with_infer\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    676\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    677\u001b[0m \u001b[38;5;124;03m    Constructor that uses the 1.0.x behavior inferring numeric dtypes\u001b[39;00m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;124;03m    for ndarray[object] inputs.\u001b[39;00m\n\u001b[1;32m    679\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 680\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m _dtype_obj \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result\u001b[38;5;241m.\u001b[39m_is_multi:\n\u001b[1;32m    683\u001b[0m         \u001b[38;5;66;03m# error: Argument 1 to \"maybe_convert_objects\" has incompatible type\u001b[39;00m\n\u001b[1;32m    684\u001b[0m         \u001b[38;5;66;03m# \"Union[ExtensionArray, ndarray[Any, Any]]\"; expected\u001b[39;00m\n\u001b[1;32m    685\u001b[0m         \u001b[38;5;66;03m# \"ndarray[Any, Any]\"\u001b[39;00m\n\u001b[1;32m    686\u001b[0m         values \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mmaybe_convert_objects(result\u001b[38;5;241m.\u001b[39m_values)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.11/site-packages/pandas/core/indexes/base.py:523\u001b[0m, in \u001b[0;36mIndex.__new__\u001b[0;34m(cls, data, dtype, copy, name, tupleize_cols)\u001b[0m\n\u001b[1;32m    516\u001b[0m         data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m    518\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miufcbmM\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    519\u001b[0m         \u001b[38;5;66;03m# GH#11836 we need to avoid having numpy coerce\u001b[39;00m\n\u001b[1;32m    520\u001b[0m         \u001b[38;5;66;03m# things that look like ints/floats to ints unless\u001b[39;00m\n\u001b[1;32m    521\u001b[0m         \u001b[38;5;66;03m# they are actually ints, e.g. '0' and 0.0\u001b[39;00m\n\u001b[1;32m    522\u001b[0m         \u001b[38;5;66;03m# should not be coerced\u001b[39;00m\n\u001b[0;32m--> 523\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43mcom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray_tuplesafe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_dtype_obj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_scalar(data):\n\u001b[1;32m    526\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_scalar_data_error(data)\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.11/site-packages/pandas/core/common.py:255\u001b[0m, in \u001b[0;36masarray_tuplesafe\u001b[0;34m(values, dtype)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;66;03m# Using try/except since it's more performant than checking is_list_like\u001b[39;00m\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;66;03m# over each element\u001b[39;00m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;66;03m# error: Argument 1 to \"construct_1d_object_array_from_listlike\"\u001b[39;00m\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;66;03m# has incompatible type \"Iterable[Any]\"; expected \"Sized\"\u001b[39;00m\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m construct_1d_object_array_from_listlike(values)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m--> 255\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(result\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    256\u001b[0m     result \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(values, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mobject\u001b[39m)\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# Avoid building an array of arrays:\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "max_state = trainer.evaluate_accuracy(num_episodes=5000, num_steps=500, find_max=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[117.6796,   7.0402,  13.8492,  12.9151,  11.1857,   7.0861,   4.1173,\n",
       "           4.1981,   5.0224,  11.5101,   4.4452,   4.2604,   9.8896,   3.9975,\n",
       "           3.9347,   7.7382,   3.2340,   3.2807,   6.5205,   0.0000,   0.0000,\n",
       "           3.6231]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_steps(env, model, num_steps):\n",
    "    obs = env.reset()\n",
    "    for _ in range(num_steps):\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        if done:\n",
    "            print('Early stopping')\n",
    "            break\n",
    "    return obs\n",
    "\n",
    "def evaluate_model(trainer, num_episodes, num_steps):\n",
    "\n",
    "    model = trainer.model\n",
    "    env = trainer.env\n",
    "    surrogate_model = trainer.surrogate_model\n",
    "    decoder_model = trainer.decoder_model\n",
    "\n",
    "    accuracy = []\n",
    "    for i in range(num_episodes):\n",
    "        rl_output = run_steps(env, model, num_steps)\n",
    "        rl_output_tensor = torch.tensor(rl_output, dtype=torch.float32)  # Add batch dimension\n",
    "        decoder_output = decoder_model(rl_output_tensor)\n",
    "        calculated_accuracy = surrogate_model.evaluate(decoder_output)\n",
    "        print(f\"Accuracy: {calculated_accuracy} for episode {i}\")\n",
    "        accuracy.append(calculated_accuracy)\n",
    "    \n",
    "    print(f\"Average accuracy: {sum(accuracy)/num_episodes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.load_model('/Users/tawab/Desktop/columbia/Courses/Spring2024/HPML/Project/Analog_NAS/trainingLogs/models/rklmkht6/model.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.798057496547699 for episode 0\n",
      "Accuracy: 0.7889947295188904 for episode 1\n",
      "Accuracy: 0.9471520185470581 for episode 2\n",
      "Accuracy: 0.7839134335517883 for episode 3\n",
      "Accuracy: 0.9190993905067444 for episode 4\n",
      "Accuracy: 0.7772524356842041 for episode 5\n",
      "Accuracy: 0.7889947295188904 for episode 6\n",
      "Accuracy: 0.7889947295188904 for episode 7\n",
      "Accuracy: 0.9243929982185364 for episode 8\n",
      "Accuracy: 0.7725493907928467 for episode 9\n",
      "Accuracy: 0.9243929982185364 for episode 10\n",
      "Accuracy: 0.7349642515182495 for episode 11\n",
      "Accuracy: 0.7551933526992798 for episode 12\n",
      "Accuracy: 0.819848895072937 for episode 13\n",
      "Accuracy: 0.9325972199440002 for episode 14\n",
      "Accuracy: 0.7681959271430969 for episode 15\n",
      "Accuracy: 0.759606122970581 for episode 16\n",
      "Accuracy: 0.9228904843330383 for episode 17\n",
      "Accuracy: 0.736478328704834 for episode 18\n",
      "Accuracy: 0.7753796577453613 for episode 19\n",
      "Accuracy: 0.9325860738754272 for episode 20\n",
      "Accuracy: 0.7889947295188904 for episode 21\n",
      "Accuracy: 0.7725493907928467 for episode 22\n",
      "Accuracy: 0.7348170280456543 for episode 23\n",
      "Accuracy: 0.8088371157646179 for episode 24\n",
      "Accuracy: 0.7513466477394104 for episode 25\n",
      "Accuracy: 0.9285473227500916 for episode 26\n",
      "Accuracy: 0.7704378962516785 for episode 27\n",
      "Accuracy: 0.798057496547699 for episode 28\n",
      "Accuracy: 0.9243929982185364 for episode 29\n",
      "Accuracy: 0.9324069023132324 for episode 30\n",
      "Accuracy: 0.9202268123626709 for episode 31\n",
      "Accuracy: 0.7839134335517883 for episode 32\n",
      "Accuracy: 0.7889947295188904 for episode 33\n",
      "Accuracy: 0.9272683262825012 for episode 34\n",
      "Accuracy: 0.8088371157646179 for episode 35\n",
      "Accuracy: 0.7349642515182495 for episode 36\n",
      "Accuracy: 0.7889947295188904 for episode 37\n",
      "Accuracy: 0.7889947295188904 for episode 38\n",
      "Accuracy: 0.757368803024292 for episode 39\n",
      "Accuracy: 0.7889947295188904 for episode 40\n",
      "Accuracy: 0.9156166315078735 for episode 41\n",
      "Accuracy: 0.9281994104385376 for episode 42\n",
      "Accuracy: 0.7753796577453613 for episode 43\n",
      "Accuracy: 0.7606016993522644 for episode 44\n",
      "Accuracy: 0.7889947295188904 for episode 45\n",
      "Accuracy: 0.7889947295188904 for episode 46\n",
      "Accuracy: 0.7725493907928467 for episode 47\n",
      "Accuracy: 0.7372422218322754 for episode 48\n",
      "Accuracy: 0.7535715699195862 for episode 49\n",
      "Accuracy: 0.9429131746292114 for episode 50\n",
      "Accuracy: 0.7692904472351074 for episode 51\n",
      "Accuracy: 0.7686992287635803 for episode 52\n",
      "Accuracy: 0.7889947295188904 for episode 53\n",
      "Accuracy: 0.9319303035736084 for episode 54\n",
      "Accuracy: 0.7404413819313049 for episode 55\n",
      "Accuracy: 0.7889947295188904 for episode 56\n",
      "Accuracy: 0.9441887140274048 for episode 57\n",
      "Accuracy: 0.944547712802887 for episode 58\n",
      "Accuracy: 0.770458459854126 for episode 59\n",
      "Accuracy: 0.7644068598747253 for episode 60\n",
      "Accuracy: 0.759606122970581 for episode 61\n",
      "Accuracy: 0.9342565536499023 for episode 62\n",
      "Accuracy: 0.7889947295188904 for episode 63\n",
      "Accuracy: 0.7686992287635803 for episode 64\n",
      "Accuracy: 0.9227997064590454 for episode 65\n",
      "Accuracy: 0.9319303035736084 for episode 66\n",
      "Accuracy: 0.7733031511306763 for episode 67\n",
      "Accuracy: 0.8088371157646179 for episode 68\n",
      "Accuracy: 0.7551933526992798 for episode 69\n",
      "Accuracy: 0.9285473227500916 for episode 70\n",
      "Accuracy: 0.7889947295188904 for episode 71\n",
      "Accuracy: 0.7644068598747253 for episode 72\n",
      "Accuracy: 0.9233819842338562 for episode 73\n",
      "Accuracy: 0.9227997064590454 for episode 74\n",
      "Accuracy: 0.7889947295188904 for episode 75\n",
      "Accuracy: 0.7644068598747253 for episode 76\n",
      "Accuracy: 0.7889947295188904 for episode 77\n",
      "Accuracy: 0.7889947295188904 for episode 78\n",
      "Accuracy: 0.7889947295188904 for episode 79\n",
      "Accuracy: 0.7889947295188904 for episode 80\n",
      "Accuracy: 0.9438520669937134 for episode 81\n",
      "Accuracy: 0.7692904472351074 for episode 82\n",
      "Accuracy: 0.9342565536499023 for episode 83\n",
      "Accuracy: 0.7839134335517883 for episode 84\n",
      "Accuracy: 0.9285473227500916 for episode 85\n",
      "Accuracy: 0.9369305968284607 for episode 86\n",
      "Accuracy: 0.9235689640045166 for episode 87\n",
      "Accuracy: 0.9238914251327515 for episode 88\n",
      "Accuracy: 0.9141677618026733 for episode 89\n",
      "Accuracy: 0.7348170280456543 for episode 90\n",
      "Accuracy: 0.7889947295188904 for episode 91\n",
      "Accuracy: 0.7889947295188904 for episode 92\n",
      "Accuracy: 0.7753796577453613 for episode 93\n",
      "Accuracy: 0.7733031511306763 for episode 94\n",
      "Accuracy: 0.7376570105552673 for episode 95\n",
      "Accuracy: 0.7889947295188904 for episode 96\n",
      "Accuracy: 0.7839134335517883 for episode 97\n",
      "Accuracy: 0.7575494050979614 for episode 98\n",
      "Accuracy: 0.7562592625617981 for episode 99\n",
      "Average Accuracy: 0.822519063949585\n"
     ]
    }
   ],
   "source": [
    "trainer.evaluate_accuracy(num_episodes=100, num_steps=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.load_model('/Users/tawab/Desktop/columbia/Courses/Spring2024/HPML/Project/Analog_NAS/trainingLogs/models/uenl6yuv/model.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9238914251327515 for episode 0\n",
      "Accuracy: 0.7889947295188904 for episode 1\n",
      "Accuracy: 0.7889947295188904 for episode 2\n",
      "Accuracy: 0.7818193435668945 for episode 3\n",
      "Accuracy: 0.7889947295188904 for episode 4\n",
      "Accuracy: 0.9326378107070923 for episode 5\n",
      "Accuracy: 0.7889947295188904 for episode 6\n",
      "Accuracy: 0.9325860738754272 for episode 7\n",
      "Accuracy: 0.7889947295188904 for episode 8\n",
      "Accuracy: 0.9238914251327515 for episode 9\n",
      "Accuracy: 0.759606122970581 for episode 10\n",
      "Accuracy: 0.819848895072937 for episode 11\n",
      "Accuracy: 0.7575494050979614 for episode 12\n",
      "Accuracy: 0.9322266578674316 for episode 13\n",
      "Accuracy: 0.9290140271186829 for episode 14\n",
      "Accuracy: 0.7889947295188904 for episode 15\n",
      "Accuracy: 0.7818193435668945 for episode 16\n",
      "Accuracy: 0.7348170280456543 for episode 17\n",
      "Accuracy: 0.7734160423278809 for episode 18\n",
      "Accuracy: 0.7889947295188904 for episode 19\n",
      "Accuracy: 0.7704378962516785 for episode 20\n",
      "Accuracy: 0.7744229435920715 for episode 21\n",
      "Accuracy: 0.7889947295188904 for episode 22\n",
      "Accuracy: 0.925092875957489 for episode 23\n",
      "Accuracy: 0.9238914251327515 for episode 24\n",
      "Accuracy: 0.7889947295188904 for episode 25\n",
      "Accuracy: 0.770458459854126 for episode 26\n",
      "Accuracy: 0.9285473227500916 for episode 27\n",
      "Accuracy: 0.9189534187316895 for episode 28\n",
      "Accuracy: 0.7889947295188904 for episode 29\n",
      "Accuracy: 0.757368803024292 for episode 30\n",
      "Accuracy: 0.9324069023132324 for episode 31\n",
      "Accuracy: 0.7535715699195862 for episode 32\n",
      "Accuracy: 0.9141677618026733 for episode 33\n",
      "Accuracy: 0.7889947295188904 for episode 34\n",
      "Accuracy: 0.7889947295188904 for episode 35\n",
      "Accuracy: 0.7551933526992798 for episode 36\n",
      "Accuracy: 0.7839134335517883 for episode 37\n",
      "Accuracy: 0.9373398423194885 for episode 38\n",
      "Accuracy: 0.7599714398384094 for episode 39\n",
      "Accuracy: 0.759606122970581 for episode 40\n",
      "Accuracy: 0.7839134335517883 for episode 41\n",
      "Accuracy: 0.7535715699195862 for episode 42\n",
      "Accuracy: 0.798057496547699 for episode 43\n",
      "Accuracy: 0.9238914251327515 for episode 44\n",
      "Accuracy: 0.9342565536499023 for episode 45\n",
      "Accuracy: 0.7889947295188904 for episode 46\n",
      "Accuracy: 0.9190993905067444 for episode 47\n",
      "Accuracy: 0.9290140271186829 for episode 48\n",
      "Accuracy: 0.7686756253242493 for episode 49\n",
      "Accuracy: 0.7889947295188904 for episode 50\n",
      "Accuracy: 0.7692904472351074 for episode 51\n",
      "Accuracy: 0.925092875957489 for episode 52\n",
      "Accuracy: 0.770458459854126 for episode 53\n",
      "Accuracy: 0.7733031511306763 for episode 54\n",
      "Accuracy: 0.944547712802887 for episode 55\n",
      "Accuracy: 0.7466604113578796 for episode 56\n",
      "Accuracy: 0.7889947295188904 for episode 57\n",
      "Accuracy: 0.7889947295188904 for episode 58\n",
      "Accuracy: 0.759606122970581 for episode 59\n",
      "Accuracy: 0.944547712802887 for episode 60\n",
      "Accuracy: 0.9141677618026733 for episode 61\n",
      "Accuracy: 0.7889947295188904 for episode 62\n",
      "Accuracy: 0.7704378962516785 for episode 63\n",
      "Accuracy: 0.7889947295188904 for episode 64\n",
      "Accuracy: 0.9238914251327515 for episode 65\n",
      "Accuracy: 0.7889947295188904 for episode 66\n",
      "Accuracy: 0.7889947295188904 for episode 67\n",
      "Accuracy: 0.9353243112564087 for episode 68\n",
      "Accuracy: 0.7575494050979614 for episode 69\n",
      "Accuracy: 0.7599714398384094 for episode 70\n",
      "Accuracy: 0.9318768382072449 for episode 71\n",
      "Accuracy: 0.7889947295188904 for episode 72\n",
      "Accuracy: 0.7889947295188904 for episode 73\n",
      "Accuracy: 0.7686756253242493 for episode 74\n",
      "Accuracy: 0.9326378107070923 for episode 75\n",
      "Accuracy: 0.7753796577453613 for episode 76\n",
      "Accuracy: 0.7889947295188904 for episode 77\n",
      "Accuracy: 0.9281994104385376 for episode 78\n",
      "Accuracy: 0.7733031511306763 for episode 79\n",
      "Accuracy: 0.7889947295188904 for episode 80\n",
      "Accuracy: 0.759606122970581 for episode 81\n",
      "Accuracy: 0.9202268123626709 for episode 82\n",
      "Accuracy: 0.9322266578674316 for episode 83\n",
      "Accuracy: 0.9369305968284607 for episode 84\n",
      "Accuracy: 0.7889947295188904 for episode 85\n",
      "Accuracy: 0.9155168533325195 for episode 86\n",
      "Accuracy: 0.7889947295188904 for episode 87\n",
      "Accuracy: 0.7466604113578796 for episode 88\n",
      "Accuracy: 0.9238914251327515 for episode 89\n",
      "Accuracy: 0.7348170280456543 for episode 90\n",
      "Accuracy: 0.7692904472351074 for episode 91\n",
      "Accuracy: 0.9238914251327515 for episode 92\n",
      "Accuracy: 0.7686756253242493 for episode 93\n",
      "Accuracy: 0.7889947295188904 for episode 94\n",
      "Accuracy: 0.9441887140274048 for episode 95\n",
      "Accuracy: 0.7771551012992859 for episode 96\n",
      "Accuracy: 0.7889947295188904 for episode 97\n",
      "Accuracy: 0.7733031511306763 for episode 98\n",
      "Accuracy: 0.759606122970581 for episode 99\n",
      "Average Accuracy: 0.8264170289039612\n"
     ]
    }
   ],
   "source": [
    "trainer.evaluate_accuracy(num_episodes=100, num_steps=500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
